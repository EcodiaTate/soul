

### FILE: app.py

import os
from flask import Flask, jsonify
from flask_cors import CORS
from flask_socketio import SocketIO
from flask_jwt_extended import JWTManager
from dotenv import load_dotenv

# Load .env config
load_dotenv()

SOCKETIO_ASYNC_MODE = os.environ.get("SOCKETIO_ASYNC_MODE", "threading")  # or "eventlet"

# Create SocketIO singleton (NO app attached yet)
socketio = SocketIO(cors_allowed_origins="*", async_mode=SOCKETIO_ASYNC_MODE)

def create_app():
    app = Flask(__name__)

    # CORS: Only allow your frontend in prod
    allowed_origins = [
        os.environ.get("FRONTEND_URL", "http://localhost:5173"),
        "http://localhost:5173"
    ]
    CORS(app, supports_credentials=True, origins=allowed_origins)

    # Security settings
    app.config['SECRET_KEY'] = os.environ.get("FLASK_SECRET_KEY", "supersecret")
    app.config['JWT_SECRET_KEY'] = os.environ.get("JWT_SECRET", "jwtsecret")

    # Register all blueprints
    from routes.events import events_bp
    from routes.chat import chat_bp
    from routes.timeline import timeline_bp
    from routes.dreams import dreams_bp

    app.register_blueprint(events_bp)
    app.register_blueprint(chat_bp)
    app.register_blueprint(timeline_bp)
    app.register_blueprint(dreams_bp)

    # JWT for admin routes
    jwt = JWTManager(app)

    # Health check endpoint
    @app.route("/api/ping")
    def ping():
        return jsonify({"status": "ok"})

    # Attach SocketIO to app
    socketio.init_app(app)

    return app

# Run only for local/dev (not in Gunicorn/wsgi)
if __name__ == "__main__":
    app = create_app()
    port = int(os.environ.get("PORT", 5000))
    socketio.run(app, host="0.0.0.0", port=port)


### FILE: collate_soul.py

import os

OUTPUT_FILE = "all_code_soul_py.txt"

with open(OUTPUT_FILE, "w", encoding="utf-8") as out:
    for dirpath, _, filenames in os.walk("."):
        for fname in filenames:
            if fname.endswith('.py'):
                rel_path = os.path.relpath(os.path.join(dirpath, fname), ".")
                out.write(f"\n\n### FILE: {rel_path}\n\n")
                try:
                    with open(os.path.join(dirpath, fname), "r", encoding="utf-8") as f:
                        out.write(f.read())
                except Exception as e:
                    out.write(f"\n[ERROR reading {rel_path}: {e}]\n")

print(f"Collated all .py files into {OUTPUT_FILE}")


### FILE: socket_timeline_test.py

# socket_timeline_test.py

import socketio

# Use your deployed server URL, port 80 or 443, no /api at the end
SOCKET_URL = "https://ecodia.au"

sio = socketio.Client()

@sio.on('timeline_update')
def on_timeline_update(data):
    print("ðŸš€ Live timeline_update received:")
    print(data)

def main():
    print("Connecting to socket server...")
    sio.connect(SOCKET_URL)
    print("Connected! Waiting for timeline updates...")
    sio.wait()  # Keep the client running to listen

if __name__ == "__main__":
    main()


### FILE: testcontext.py

import os
import uuid
import json
from neo4j import GraphDatabase
from dotenv import load_dotenv
load_dotenv()

NEO4J_URI = os.environ.get("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.environ.get("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.environ.get("NEO4J_PASS", "password")

driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

# --- MAIN CONTEXT ENGINE HELPERS ---

def get_unprocessed_event_nodes():
    """
    Returns a list of event nodes that need context/embedding (processed = false or missing).
    Each node is a dict with at least 'id' and 'raw_text'.
    """
    cypher = """
    MATCH (e:Event)
    WHERE NOT EXISTS(e.processed) OR e.processed = false
    RETURN e
    """
    nodes = []
    with driver.session() as session:
        result = session.run(cypher)
        for record in result:
            node = dict(record["e"])
            # If context/embedding are JSON strings, parse them
            if "context" in node and isinstance(node["context"], str):
                try:
                    node["context"] = json.loads(node["context"])
                except:
                    pass
            nodes.append(node)
    return nodes

def mark_event_processed(node_id, context, embedding):
    """
    Updates the given event node with context (as JSON), embedding (as list), sets processed true, and adds Context label.
    """
    # Serialize context to string for Neo4j
    context_json = json.dumps(context, ensure_ascii=False)
    cypher = """
    MATCH (e:Event {id: $id})
    SET e.context = $context_json,
        e.embedding = $embedding,
        e.processed = true
    SET e:Context
    RETURN e
    """
    with driver.session() as session:
        result = session.run(
            cypher,
            id=node_id,
            context_json=context_json,
            embedding=embedding
        )
        record = result.single()
        return dict(record["e"]) if record else None

# --- Other unchanged functions below (create_node, query_nodes, etc) ---

def vector_search(embedding, top_k=8):
    # Implement this using Neo4j vector search if available, or stub
    return []

def get_node_summary(node):
    return {
        "summary": "",
        "key_insight": "",
        "origin_metadata": {},
        "relevance_score": 0.0
    }

def create_node(label, properties):
    if 'id' not in properties:
        properties['id'] = str(uuid.uuid4())
    safe_props = {}
    for k, v in properties.items():
        if isinstance(v, (str, int, float, bool)) or v is None:
            safe_props[k] = v
        elif isinstance(v, list) and all(isinstance(i, (str, int, float, bool)) or i is None for i in v):
            safe_props[k] = v
        else:
            safe_props[k] = json.dumps(v)
    with driver.session() as session:
        result = session.run(
            f"CREATE (n:{label} $props) RETURN n",
            props=safe_props
        )
        record = result.single()
        return dict(record["n"]) if record else safe_props

def query_nodes(filter_dict=None, sort_by=None, desc=False):
    filter_dict = filter_dict or {}
    label = filter_dict.get('label', '')
    filters = [f"n.{k} = ${k}" for k in filter_dict if k != 'label']
    where_clause = f"WHERE {' AND '.join(filters)}" if filters else ''
    order = f"ORDER BY n.{sort_by} {'DESC' if desc else 'ASC'}" if sort_by else ''
    cypher = f"MATCH (n{':' + label if label else ''}) {where_clause} RETURN n {order}"
    with driver.session() as session:
        result = session.run(cypher, {k: v for k, v in filter_dict.items() if k != 'label'})
        return [dict(record["n"]) for record in result]

def embed_vector_in_node(node_id, raw_text):
    fake_vector = [0.0] * 1536
    if node_id:
        with driver.session() as session:
            session.run("MATCH (n) WHERE n.id = $id SET n.embedding = $vector",
                        id=node_id, vector=fake_vector)
    return fake_vector

def create_relationship(source_id, target_id, rel_type, properties=None):
    properties = properties or {}
    print(f"[graph_io] Create relationship: ({source_id})-[:{rel_type} {properties}]->({target_id})")
    return {"source_id": source_id, "target_id": target_id, "rel_type": rel_type, "properties": properties}

def write_consensus_to_graph(consensus):
    consensus['id'] = consensus.get('id', str(uuid.uuid4()))
    with driver.session() as session:
        result = session.run(
            "CREATE (n:Consensus $props) RETURN n",
            props=consensus
        )
        record = result.single()
        return dict(record["n"]) if record else consensus

def get_pending_cypher_actions():
    return []

def mark_action_executed(consensus_node_id, action_plan):
    pass

def create_schema_change_node(action_plan, result, consensus_node_id):
    return {"id": "schema-change-id"}

def run_cypher(query, params=None):
    print(f"[run_cypher] Executing: {query}")
    return {"result": "ok"}

# Ready for full context engine integration!


### FILE: timeline_api_test.py

# timeline_api_test.py

import requests
import time

BASE = "https://ecodia.au"  # Change to your deployed API base

def post_event():
    resp = requests.post(f"{BASE}/api/event", json={"text": "Timeline socket test event!"})
    print("POST /api/event:", resp.status_code, resp.json())

def get_timeline():
    resp = requests.get(f"{BASE}/api/timeline")
    print("GET /api/timeline:", resp.status_code)
    data = resp.json()
    for entry in data[:3]:
        print(f"{entry['timestamp']} | {entry['summary']}")

if __name__ == "__main__":
    post_event()
    time.sleep(2)  # Give backend time to process/promote
    get_timeline()


### FILE: wsgi.py

# wsgi.py
from app import create_app, socketio

app = create_app()
application = app  # For Gunicorn


### FILE: config\settings.py

import os
from dotenv import load_dotenv

load_dotenv()

# Neo4j
NEO4J_URI = os.getenv("NEO4J_URI")
NEO4J_USER = os.getenv("NEO4J_USER")
NEO4J_PASS = os.getenv("NEO4J_PASS")

# OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Any other keys go here...


### FILE: core\actuators.py



### FILE: core\agents.py

"""
agents.py â€” Fully Dynamic God Mode
Value/Emotion Vectors, Audit, Schema, Trace, and Meta-Reflection Engine

Dynamically supports arbitrary agents, vector schema evolution, and audit fields.
Each agent can have its own persona, logic, logging, lineage, and custom metadata.
"""

import uuid
import json
from datetime import datetime, timezone

from core.llm_tools import run_llm, run_llm_emotion_vector
from core.value_vector import (
    extract_and_score_value_vector,
    get_value_schema_version,
)

AGENT_REGISTRY = {}

def register_agent(
    name,
    agent_func,
    description=None,
    persona=None,
    model_type=None,
    log_path=None,
    extra_fields=None
):
    AGENT_REGISTRY[name] = {
        "func": agent_func,
        "description": description or "",
        "created_at": datetime.now(timezone.utc).isoformat(),
        "persona": persona or "",
        "model_type": model_type or "LLM",
        "log_path": log_path,
        "memory_vector_profile": {},
        "parent": None,
        "children": [],
        **(extra_fields or {})
    }

COMPLEX_FIELDS = {
    "value_vector", "emotion_vector", "audit_log", "causal_trace",
    "rationale", "action_plan"
}

def serialize_agent_response(resp: dict) -> dict:
    result = {}
    for k, v in resp.items():
        if k in COMPLEX_FIELDS and isinstance(v, (dict, list)):
            result[k] = json.dumps(v)
        else:
            result[k] = v
    return result

def deserialize_agent_response(resp: dict) -> dict:
    d = dict(resp)
    for k in COMPLEX_FIELDS:
        if k in d and isinstance(d[k], str):
            try:
                d[k] = json.loads(d[k])
            except Exception:
                pass
    return d

def agent_reason(event, agent_name, context=None, log=True):
    agent = AGENT_REGISTRY.get(agent_name)
    if not agent:
        raise ValueError(f"Agent '{agent_name}' not registered!")

    context_block = event.get("context") or context
    if not context_block:
        raise ValueError("Missing compressed context block in event.")

    result = agent["func"](event, context=context_block)
    if not isinstance(result, dict):
        raise ValueError("Agent must return dict with at least 'rationale' and 'score'.")

    value_vec = result.get("value_vector")
    if not value_vec:
        try:
            value_vec = extract_and_score_value_vector(context_block, agent=agent_name)
        except Exception as e:
            print(f"[agents] Value vector extraction failed for agent '{agent_name}': {e}")
            value_vec = {}

    emotion_vec = result.get("emotion_vector")
    if not emotion_vec:
        try:
            emotion_vec = run_llm_emotion_vector(context_block, agent=agent_name)
        except Exception as e:
            print(f"[agents] Emotion vector extraction failed for agent '{agent_name}': {e}")
            emotion_vec = {}

    rationale = result.get("rationale", f"No rationale provided by {agent_name}.")
    score = float(result.get("score", 1.0))
    action_plan = result.get("action_plan", None)
    causal_trace = result.get("causal_trace", [])
    schema_version = get_value_schema_version()
    agent_priority = float(result.get("agent_priority", 0.0))
    user_pinned = bool(result.get("user_pinned", False))

    audit_entry = {
        "rationale": rationale,
        "value_vector": value_vec,
        "emotion_vector": emotion_vec,
        "score": score,
        "schema_version": schema_version,
        "timestamp": datetime.now(timezone.utc).isoformat()
    }
    audit_log = result.get("audit_log", []) + [audit_entry]

    agent_response = {
        "id": str(uuid.uuid4()),
        "agent_name": agent_name,
        "model_type": agent.get("model_type"),
        "persona": agent.get("persona"),
        "rationale": rationale,
        "score": score,
        "action_plan": action_plan,
        "value_vector": value_vec,
        "value_schema_version": schema_version,
        "emotion_vector": emotion_vec,
        "causal_trace": causal_trace + [event.get("id")],
        "agent_priority": agent_priority,
        "user_pinned": user_pinned,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "audit_log": audit_log,
        **{k: v for k, v in result.items() if k not in {
            "rationale", "score", "action_plan", "value_vector", "emotion_vector",
            "causal_trace", "agent_priority", "user_pinned", "audit_log"
        }}
    }

    if log:
        print(f"[agents] {agent_name} ({agent['model_type']}) Score {score:.2f} | v{schema_version} | Event {event.get('id')}")

    return agent_response

# --- Production Agent Definition ---

def claude_reasoning_agent(event, context=None):
    """
    Core LLM reasoning agent using Claude for reflective, analytical processing.
    """
    prompt = f"""You are an intelligent agent in the Soul-OS cognitive mesh. Analyze the following user input and produce your rationale.

INPUT:
\"\"\"
{context}
\"\"\"

Please return your analysis in the following JSON format:
{{
  "rationale": "... your explanation ...",
  "score": float (0.0â€“1.0),
  "action_plan": optional string or null
}}
"""
    output = run_llm(prompt, agent="claude", purpose="agent")
    try:
        result = json.loads(output)
        return result
    except Exception as e:
        print(f"[claude_reasoning_agent] JSON parsing failed: {e}")
        return {
            "rationale": f"Failed to parse output: {output}",
            "score": 0.5,
            "action_plan": None
        }

register_agent(
    "Claude-Analyst",
    claude_reasoning_agent,
    description="Main cognitive mesh agent using Claude for system reasoning.",
    persona="Reflective analyst",
    model_type="claude"
)

# --- Mesh Execution ---

def run_all_agents(event, context=None):
    responses = []
    for name in AGENT_REGISTRY:
        try:
            resp = agent_reason(event, name, context)
            responses.append(resp)
        except Exception as e:
            print(f"[agents] Error running agent '{name}': {e}")
    return responses

# --- Agent Metadata Tools ---

def explain_agent(agent_name):
    agent = AGENT_REGISTRY.get(agent_name)
    if not agent:
        return f"Agent '{agent_name}' not found."
    return {
        "name": agent_name,
        "description": agent.get("description"),
        "created_at": agent.get("created_at"),
        "model_type": agent.get("model_type"),
        "persona": agent.get("persona"),
        "func_doc": agent["func"].__doc__
    }

def list_agents():
    return [
        {
            "name": name,
            "model_type": agent["model_type"],
            "persona": agent["persona"],
            "description": agent["description"],
            "created_at": agent["created_at"]
        }
        for name, agent in AGENT_REGISTRY.items()
    ]

def write_agent_log(agent_name, data):
    agent = AGENT_REGISTRY.get(agent_name)
    if not agent or not agent.get("log_path"):
        return
    try:
        with open(agent["log_path"], "a") as f:
            f.write(json.dumps(data) + "\n")
    except Exception as e:
        print(f"[agents] Failed to write log for {agent_name}: {e}")


### FILE: core\audit.py



### FILE: core\chat_agent.py

import json

from core.prompts import (
    chat_response_prompt,
    claude_prethought_prompt,
    get_ecodia_identity
)
from core.llm_tools import run_llm
from core.vector_search import search_nodes_by_text


def generate_chat_reply(raw_text: str, context_blocks: list[str] = None) -> str:
    """
    Generate a fast, high-quality conversational reply using Claude (purpose='chat').
    Optionally includes relevant context blocks retrieved via vector search.
    """
    identity = get_ecodia_identity()
    prompt = chat_response_prompt(identity, raw_text, context_blocks)
    response = run_llm(prompt, agent="claude", purpose="chat")
    return response.strip()


def generate_prethought_queries(raw_text: str) -> list[dict]:
    """
    Ask Claude what it wants to retrieve to help it think.
    Returns a list of { "phrase": ..., "field": ... } dicts.
    """
    identity = get_ecodia_identity()
    prompt = claude_prethought_prompt(identity, raw_text)
    output = run_llm(prompt, agent="claude", purpose="prethought")

    try:
        queries = json.loads(output)
        if isinstance(queries, list):
            return [
                q for q in queries
                if isinstance(q, dict) and "phrase" in q and "field" in q
            ]
        else:
            print("[chat_agent] Prethought output not a list.")
            return []
    except Exception as e:
        print(f"[chat_agent] Failed to parse prethought query list: {e}")
        return []


def retrieve_context_blocks(queries: list[dict], top_k: int = 2) -> list[str]:
    """
    Perform vector searches per Claudeâ€™s query list.
    For each {phrase, field} pair, return relevant field content from top-k matching memory nodes.
    """
    results = []
    for query in queries:
        phrase = query["phrase"]
        field = query["field"]
        try:
            matches = search_nodes_by_text(phrase, field=field, top_k=top_k)
            blocks = [node.get(field, "").strip() for node in matches if node.get(field)]
            results.extend(blocks)
        except Exception as e:
            print(f"[chat_agent] Vector search error for phrase '{phrase}' on field '{field}': {e}")
    return results


def generate_contextual_chat_reply(raw_text: str) -> str:
    """
    Full intelligent loop:
    1. Claude outputs desired retrieval targets (phrases + fields)
    2. We run vector search for each pair
    3. Claude receives curated context snippets for reply
    """
    queries = generate_prethought_queries(raw_text)
    context_blocks = retrieve_context_blocks(queries)
    return generate_chat_reply(raw_text, context_blocks)


### FILE: core\consciousness_engine.py

# /core/consciousness_engine.py

def process_pending_cypher_actions():
    """
    Find all consensus nodes/events with pending cypher action plans,
    execute them safely, log results, and update system.
    """
    # Import all patchable dependencies INSIDE the function for monkeypatching/testing.
    from core.actuators.cypher import execute as cypher_execute
    from core.graph_io import (
        get_pending_cypher_actions,   
        mark_action_executed,         
        create_schema_change_node
    )
    from core.socket_handlers import emit_action_update

    print("[consciousness_engine] Scanning for pending cypher actions...")
    pending = get_pending_cypher_actions()  # Returns [(action_plan, consensus_node), ...]
    if not pending:
        print("[consciousness_engine] No pending cypher actions found.")
        return

    for action_plan, consensus_node in pending:
        print(f"[consciousness_engine] Executing cypher action for consensus node {consensus_node.get('id', '[no id]')}")
        result = cypher_execute(action_plan)

        # Mark action as executed to prevent rerun (attach a flag/property in Neo4j)
        mark_action_executed(consensus_node.get("id"), action_plan)

        # Log this mutation as a SchemaChange node
        schema_change = create_schema_change_node(action_plan, result, consensus_node.get("id"))

        # Emit to socket for frontend logging
        emit_action_update({
            "type": "cypher",
            "status": result.get("status"),
            "details": result.get("result", result.get("error", "")),
            "consensus_node_id": consensus_node.get("id"),
            "schema_change_id": schema_change.get("id")
        })

        print(f"[consciousness_engine] Cypher action processed: {result.get('status')}")

    print(f"[consciousness_engine] All pending cypher actions processed.\n")

# (Optional) You can call this manually, or schedule it via a background task.
# Example: from app.py, call after consensus or every N seconds.


### FILE: core\consensus_engine.py

"""
consensus_engine.py â€” 130% God Mode
Truly Dynamic Edge Types, Value/Emotion Vectors, Mesh Edges, Audit, Actuator, and Full Integration

- All edge types discovered and applied dynamically from the current schema
- Value/emotion vectors, action plan, rationale, audit fully supported and serializable
- Semantic mesh and value links always reflect current CE-driven edge pool
"""

import math
from datetime import datetime, timezone

from core.graph_io import (
    write_consensus_to_graph,
    get_event_by_id,
    create_relationship,
    vector_search,
    get_value_nodes,
    get_edge_types
)
from core.value_vector import (
    fuse_value_vectors,
    multi_vector_conflict,
    get_value_schema_version,
    get_value_names,
    get_value_importances,
    bump_value_importance,
    FIXED_EMOTION_AXES
)
from core.memory_engine import bump_memory_importance
from core.agents import serialize_agent_response
from core.prompts import consensus_prompt, get_ecodia_identity
from core.llm_tools import run_llm

CONSENSUS_ALIGNMENT_THRESHOLD = 0.7

def check_alignment(agent_responses, threshold=CONSENSUS_ALIGNMENT_THRESHOLD):
    if not agent_responses or len(agent_responses) == 0:
        print("[consensus_engine] No agent responses provided!")
        return False

    value_vectors = [a.get("value_vector") for a in agent_responses if "value_vector" in a]
    emotion_vectors = [a.get("emotion_vector") for a in agent_responses if "emotion_vector" in a]
    if len(value_vectors) >= 2:
        mesh = multi_vector_conflict(value_vectors, threshold=threshold)
        if mesh.get("peer_review"):
            print(f"[consensus_engine] Value vector mesh not aligned: triggering peer review.")
            return False

    # (Optional) Add emotion mesh check
    if len(emotion_vectors) >= 2:
        emo_keys = set().union(*emotion_vectors)
        def as_vec(d): return [d.get(k, 0.0) for k in emo_keys]
        def cos_sim(v1, v2):
            num = sum(a*b for a, b in zip(v1, v2))
            denom = math.sqrt(sum(a*a for a in v1)) * math.sqrt(sum(b*b for b in v2))
            return num / denom if denom else 0.0
        vlist = [as_vec(v) for v in emotion_vectors]
        sims = [1 - cos_sim(vlist[i], vlist[j])
                for i in range(len(vlist)) for j in range(i+1, len(vlist))]
        avg_dist = sum(sims) / len(sims) if sims else 0.0
        if avg_dist > threshold:
            print("[consensus_engine] Emotion vector mesh not aligned: triggering peer review.")
            return False

    print(f"[consensus_engine] {len(agent_responses)} agent responses â€” alignment ok.")
    return True

def build_consensus(agent_responses):
    if not agent_responses:
        print("[consensus_engine] No agent responses to build consensus.")
        return None

    # Optionally: build rationale and value vector via LLM prompt, else fallback to fusion
    from core.value_vector import get_value_names
    try:
        rationales = [a.get("rationale", "") for a in agent_responses]
        agent_names = [a.get("agent_name", "?") for a in agent_responses]
        value_axes = get_value_names()

        prompt = consensus_prompt(
            get_ecodia_identity(),
            [{"agent_name": n, "rationale": r} for n, r in zip(agent_names, rationales)],
            value_axes
        )
        # Uncomment the next two lines to use LLM consensus reasoning
        # llm_response = run_llm(prompt)
        # parsed = json.loads(llm_response)
        # combined_rationale = parsed.get("rationale", "")
        # consensus_value_vector = parsed.get("value_vector", {})
        # average_score = parsed.get("consensus_score", 1.0)
        # action_plan = parsed.get("action_plan", None)
    except Exception as e:
        print(f"[consensus_engine] LLM consensus rationale fallback: {e}")
        combined_rationale = "\n\n".join(
            [f"[{a.get('agent_name', '?')}] {a.get('rationale','')}" for a in agent_responses]
        )
        average_score = sum(a.get("score", 1.0) for a in agent_responses) / len(agent_responses)
        action_plan = next((a.get("action_plan") for a in agent_responses if a.get("action_plan")), None)
        value_vectors = [a.get("value_vector") for a in agent_responses if "value_vector" in a]
        schema_version = get_value_schema_version()
        consensus_value_vector = fuse_value_vectors(value_vectors) if value_vectors else {}

    if 'combined_rationale' not in locals():
        combined_rationale = "\n\n".join(
            [f"[{a.get('agent_name', '?')}] {a.get('rationale','')}" for a in agent_responses]
        )
        average_score = sum(a.get("score", 1.0) for a in agent_responses) / len(agent_responses)
        action_plan = next((a.get("action_plan") for a in agent_responses if a.get("action_plan")), None)
        value_vectors = [a.get("value_vector") for a in agent_responses if "value_vector" in a]
        schema_version = get_value_schema_version()
        consensus_value_vector = fuse_value_vectors(value_vectors) if value_vectors else {}

    # Bump value importance for all high-expressed values in consensus
    importances = get_value_importances()
    for v, score in consensus_value_vector.items():
        if score > 0.65:
            bump_value_importance(v, amount=0.02, actor="consensus")

    # Optionally bump relevant memories (e.g. agent causal_trace, or surface)
    for agent in agent_responses:
        for mem_id in agent.get("causal_trace", []):
            try:
                bump_memory_importance(mem_id, amount=0.01, actor="consensus-context")
            except Exception:
                pass

    consensus = {
        "rationale": combined_rationale,
        "consensus_score": average_score,
        "agent_names": [a.get("agent_name","?") for a in agent_responses],
        "action_plan": action_plan,
        "value_vector": consensus_value_vector,
        "value_schema_version": schema_version,
        "audit_log": [{
            "agent_names": [a.get("agent_name","?") for a in agent_responses],
            "rationales": [a.get("rationale","") for a in agent_responses],
            "timestamp": __now_utc__()
        }]
    }
    print(f"[consensus_engine] Built consensus. Avg score: {average_score:.2f}, Value schema: {schema_version}")
    return consensus

def build_edge_properties(event_node, target_node, rel_type, consensus_vector):
    props = {
        "value_schema_version": consensus_vector.get("value_schema_version", None),
        "created_at": event_node.get("timestamp"),
        "source_event_id": event_node.get("id"),
        "target_id": target_node.get("id"),
    }
    source_vv = consensus_vector or {}
    target_vv = target_node.get("value_vector", {})
    props["alignment_score"] = cosine_similarity(source_vv, target_vv)
    props["value_vector_diff"] = {k: round(source_vv.get(k, 0.0) - target_vv.get(k, 0.0), 3)
                                  for k in set(source_vv) | set(target_vv)}
    if rel_type == "TOPIC_MATCH":
        props["topics"] = list(set(event_node.get("topics", [])) & set(target_node.get("topics", [])))
    return props

def cosine_similarity(vec1, vec2):
    common_keys = set(vec1) & set(vec2)
    if not common_keys:
        return 0.0
    v1 = [vec1[k] for k in common_keys]
    v2 = [vec2[k] for k in common_keys]
    num = sum(a*b for a,b in zip(v1, v2))
    denom = math.sqrt(sum(a*a for a in v1)) * math.sqrt(sum(b*b for b in v2))
    return round(num/denom, 4) if denom else 0.0

def create_mesh_edges(event_id, consensus_vector, event_node, top_k=8):
    """
    100% Dynamic Mesh Edge Creation.
    Edge types are read live from the DB and matched by vector similarity, topic, value, or threshold rules.
    """
    edge_types = {et.get("name") for et in get_edge_types() if et.get("name")}
    value_nodes = get_value_nodes()
    similar_nodes = vector_search(consensus_vector, top_k=top_k)

    # Always connect to similar nodes, using all allowed dynamic edge types.
    for node in similar_nodes:
        for rel_type in edge_types:
            if rel_type == "CONTRADICTS":
                # Link to the most dissimilar node
                most_dissimilar = min(similar_nodes, key=lambda n: cosine_similarity(consensus_vector, n.get("value_vector", {})))
                if node["id"] == most_dissimilar["id"]:
                    props = build_edge_properties(event_node, node, rel_type, consensus_vector)
                    create_relationship(event_id, node["id"], rel_type, props)
            elif rel_type == "TOPIC_MATCH":
                event_topics = set(event_node.get("topics", []))
                target_topics = set(node.get("topics", []))
                if event_topics & target_topics:
                    props = build_edge_properties(event_node, node, rel_type, consensus_vector)
                    create_relationship(event_id, node["id"], rel_type, props)
            elif rel_type in ("EXPRESSES", "EMBODIES"):
                for value in value_nodes:
                    val_name = value.get("name")
                    score = consensus_vector.get(val_name, 0.0)
                    if score > 0.75:
                        props = {"score": score, "value_schema_version": consensus_vector.get("value_schema_version", None)}
                        create_relationship(event_id, value["id"], rel_type, props)
            else:
                # Generic link for any new or mutated edge type
                props = build_edge_properties(event_node, node, rel_type, consensus_vector)
                create_relationship(event_id, node["id"], rel_type, props)

def consensus_pipeline(event_id, agent_responses):
    if check_alignment(agent_responses):
        consensus = build_consensus(agent_responses)
        consensus_serialized = serialize_agent_response(consensus)
        node = write_consensus_to_graph(consensus_serialized)
        action_plan = consensus.get("action_plan")

        # --- Actuator Trigger ---
        if action_plan and action_plan.get("action_type") not in ["cypher", "schema"]:
            # action_result = dispatch_actuator(action_plan)
            # emit_action_update(action_result)
            pass
        elif action_plan:
            print("[consensus_engine] Cypher/Schema action detected. Passing to consciousness engine for later execution.")

        # --- Mesh edge creation: 100% dynamic
        event_node = get_event_by_id(event_id)
        if event_node and consensus.get("value_vector"):
            create_mesh_edges(event_id, consensus["value_vector"], event_node, top_k=8)

        return {"status": "consensus", "node": node}
    else:
        from core.peer_review_engine import peer_review_pipeline
        review_result = peer_review_pipeline(event_id, agent_responses)
        return {"status": "peer_review", "review_result": review_result}

def __now_utc__():
    return datetime.now(timezone.utc).isoformat()


### FILE: core\context_engine.py

"""
context_engine.py â€” Pure Compression Engine for Incoming Events
Author: Ecodia Dev Team
Last updated: 2025-07-16
"""

import os
import json
import logging
from dotenv import load_dotenv
import openai
import numpy as np

from core.graph_io import (
    get_unprocessed_event_nodes,
    mark_event_processed,
    query_nodes,
)

from core.prompts import contextualization_prompt

load_dotenv()
logging.basicConfig(level=logging.INFO)

LLM_MODEL = os.getenv("OPENAI_LLM_MODEL", "gpt-3.5-turbo")
EMBED_MODEL = os.getenv("OPENAI_EMBED_MODEL", "text-embedding-ada-002")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
client = openai.OpenAI(api_key=OPENAI_API_KEY)

# === MAIN ENTRYPOINT ===

def process_new_events():
    events = get_unprocessed_event_nodes()
    if not events:
        logging.info("[context_engine] No new events to process.")
        return
    for ev in events:
        event_id = ev["id"]
        raw_text = ev.get("raw_text", "")
        try:
            process_single_event(event_id, raw_text)
        except Exception as e:
            logging.error(f"[context_engine] ERROR processing event {event_id}: {e}")

def process_single_event(event_id: str, raw_text: str):
    compressed = compress_event(raw_text)
    if not compressed:
        raise Exception("Compression failed or returned empty.")
    embedding = embed_text(compressed)
    if not embedding:
        raise Exception("Embedding failed or empty vector.")
    mark_event_processed(event_id, compressed, embedding)
    logging.info(f"[context_engine] Processed and compressed event {event_id}")

# === CORE COMPRESSION ===

def compress_event(raw_text: str) -> str:
    prompt = contextualization_prompt(raw_text)
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "system", "content": prompt}],
            temperature=0.0,
            max_tokens=384,
        )
        compressed = response.choices[0].message.content.strip()
        if not compressed or len(compressed) < 4:
            raise Exception("Returned compression is empty or invalid.")
        return compressed
    except Exception as e:
        logging.error(f"[compress_event] ERROR: {e}")
        return None

# === EMBEDDING ===

def embed_text(text: str):
    try:
        resp = client.embeddings.create(input=[text], model=EMBED_MODEL)
        vec = resp.data[0].embedding
        if not vec or sum(abs(x) for x in vec) < 1e-6:
            raise Exception("Embedding is near-zero.")
        return vec
    except Exception as e:
        logging.error(f"[embed_text] ERROR: {e}")
        return None

# === CHAT CONTEXT FORMATTING ===

def format_context_blocks(blocks):
    """
    Format a list of memory/context blocks for display or injection into prompts.
    """
    if not blocks:
        return []
    formatted = []
    for b in blocks:
        if isinstance(b, str):
            formatted.append(b)
        elif isinstance(b, dict):
            for key in ("rationale", "text", "summary", "raw_text"):
                if key in b:
                    formatted.append(b[key])
                    break
    return formatted

# === CONTEXT RETRIEVAL ===

def cosine_similarity(vec1, vec2):
    if not vec1 or not vec2:
        return 0.0
    a = np.array(vec1)
    b = np.array(vec2)
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8))

def retrieve_similar_context(vector, top_k=5, node_type="Event"):
    all_nodes = query_nodes({"label": node_type}, limit=100)
    scored = []
    for node in all_nodes:
        emb = node.get("embedding")
        if emb:
            score = cosine_similarity(vector, emb)
            scored.append((score, node))
    scored.sort(reverse=True, key=lambda x: x[0])
    return [node for _, node in scored[:top_k]]


def load_relevant_context(vector, top_k=5):
    """
    Alias for retrieve_similar_context â€” used by chat.py and agent mesh.
    """
    return retrieve_similar_context(vector, top_k=top_k, node_type="Event")


### FILE: core\dreams.py

# /core/dreams.py
import uuid, datetime
from .graph_io import create_node, query_nodes, embed_vector_in_node

def add_dream(raw_text, user_origin=None, meta_notes=None):
    """Creates a Dream node, embeds, and stores in Neo4j."""
    dream_id = str(uuid.uuid4())
    timestamp = datetime.datetime.utcnow().isoformat()
    embedding = embed_vector_in_node(None, raw_text)  # This should call your vectorizer, returns [float]
    node = create_node('Dream', {
        'id': dream_id,
        'raw_text': raw_text,
        'timestamp': timestamp,
        'embedding': embedding,
        'significance': 0.5,  # Start mid, adjust later
        'user_origin': user_origin or "system",
        'meta_notes': meta_notes or ""
    })
    return node

def get_all_dreams():
    """Returns all Dream nodes, sorted by timestamp desc."""
    dreams = query_nodes({'label': 'Dream'}, sort_by='timestamp', desc=True)
    return dreams


### FILE: core\events.py



### FILE: core\graph_io.py

import os
import uuid
import json
from typing import Any, Dict, List, Optional, Union
from neo4j import GraphDatabase
from dotenv import load_dotenv

load_dotenv()

NEO4J_URI = os.environ.get("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.environ.get("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.environ.get("NEO4J_PASS", "password")

driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

# --- Fields that should always be JSON-encoded if non-primitive ---
COMPLEX_FIELDS = {
    "value_vector", "emotion_vector", "context", "audit_log",
    "action_plan", "topics", "causal_trace", "agent_responses",
    "critiques", "embedding"
}

def _safe_props(properties: Dict[str, Any]) -> Dict[str, Any]:
    """Serialize all complex fields to JSON strings for Neo4j."""
    safe = {}
    for k, v in properties.items():
        if k in COMPLEX_FIELDS:
            safe[k] = json.dumps(v)
        elif isinstance(v, (str, int, float, bool)) or v is None:
            safe[k] = v
        elif isinstance(v, list) and all(isinstance(i, (str, int, float, bool)) or i is None for i in v):
            safe[k] = v
        else:
            safe[k] = json.dumps(v)
    return safe

def _parse_complex_fields(node: Dict[str, Any]) -> Dict[str, Any]:
    """Decode JSON strings for all complex fields (if present and string)."""
    for k in COMPLEX_FIELDS:
        if k in node and isinstance(node[k], str):
            try:
                node[k] = json.loads(node[k])
            except Exception:
                node[k] = {} if k.endswith("_vector") or k == "context" or k == "action_plan" else []
    return node

# === EVENT, NODE, VALUE, CONTEXT, EMBEDDING ===

def get_event_by_id(event_id: str) -> Optional[Dict[str, Any]]:
    cypher = "MATCH (e:Event {id: $event_id}) RETURN e"
    with driver.session() as session:
        result = session.run(cypher, event_id=event_id)
        record = result.single()
        if not record:
            return None
        node = dict(record["e"])
        return _parse_complex_fields(node)

def get_node(event_id: str) -> Optional[Dict[str, Any]]:
    cypher = "MATCH (n {id: $event_id}) RETURN n LIMIT 1"
    with driver.session() as session:
        result = session.run(cypher, event_id=event_id)
        record = result.single()
        if record and "n" in record:
            return _parse_complex_fields(dict(record["n"]))
        return None

def create_node(label: str, properties: Dict[str, Any]) -> Dict[str, Any]:
    if 'id' not in properties:
        properties['id'] = str(uuid.uuid4())
    safe_props = _safe_props(properties)
    with driver.session() as session:
        result = session.run(
            f"CREATE (n:{label} $props) RETURN n",
            props=safe_props
        )
        record = result.single()
        return _parse_complex_fields(dict(record["n"])) if record else safe_props

def update_node(label: Optional[str], node_id: str, updates: Dict[str, Any]) -> Dict[str, Any]:
    safe_updates = _safe_props(updates)
    cypher = f"""
    MATCH (n{':' + label if label else ''} {{id: $id}})
    SET """ + ", ".join([f"n.{k} = ${k}" for k in safe_updates]) + " RETURN n"
    params = {"id": node_id}
    params.update(safe_updates)
    with driver.session() as session:
        result = session.run(cypher, params)
        record = result.single()
        return _parse_complex_fields(dict(record["n"])) if record else {}

def query_nodes(filter_dict: Optional[Dict[str, Any]] = None, sort_by: Optional[str] = None, desc: bool = False, limit: int = 100) -> List[Dict[str, Any]]:
    filter_dict = filter_dict or {}
    label = filter_dict.get('label', '')
    filters = [f"n.{k} = ${k}" for k in filter_dict if k != 'label']
    where_clause = f"WHERE {' AND '.join(filters)}" if filters else ''
    order = f"ORDER BY n.{sort_by} {'DESC' if desc else 'ASC'}" if sort_by else ''
    limit_clause = f"LIMIT {limit}" if limit else ''
    cypher = f"MATCH (n{':' + label if label else ''}) {where_clause} RETURN n {order} {limit_clause}"
    with driver.session() as session:
        result = session.run(cypher, {k: v for k, v in filter_dict.items() if k != 'label'})
        return [_parse_complex_fields(dict(record["n"])) for record in result]

def get_all_nodes(label: str = "") -> List[Dict[str, Any]]:
    return query_nodes({"label": label})

def archive_node(node_id: str) -> None:
    cypher = "MATCH (n {id: $id}) SET n.archived = true, n:Archived"
    with driver.session() as session:
        session.run(cypher, id=node_id)

def traverse_branch(node_id: str) -> List[str]:
    cypher = """
    MATCH (start {id: $id})-[*0..]->(n)
    RETURN DISTINCT n.id as id
    """
    with driver.session() as session:
        result = session.run(cypher, id=node_id)
        return [record["id"] for record in result]

def get_value_nodes() -> List[Dict[str, Any]]:
    return query_nodes({"label": "Value", "active": True})

def update_node_schema(label: str, node_list: List[Dict[str, Any]], version: int) -> None:
    cypher = f"""
    MATCH (n:{label})
    WHERE n.uuid IN $uuids
    SET n:Value, n.schema_version = $version
    """
    uuids = [n.get('uuid') for n in node_list]
    with driver.session() as session:
        session.run(cypher, uuids=uuids, version=version)

def get_schema_version(label: str) -> Optional[int]:
    cypher = f"MATCH (n:{label}) RETURN n.schema_version as v ORDER BY v DESC LIMIT 1"
    with driver.session() as session:
        result = session.run(cypher)
        record = result.single()
        return int(record["v"]) if record and record["v"] else None

def set_schema_version(label: str, version: int) -> None:
    cypher = f"MATCH (n:{label}) SET n.schema_version = $version"
    with driver.session() as session:
        session.run(cypher, version=version)

def get_edge_types() -> List[Dict[str, Any]]:
    return query_nodes({"label": "EdgeType"})

def create_relationship(source_id: str, target_id: str, rel_type: str, properties: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    properties = properties or {}
    safe_props = _safe_props(properties)
    cypher = f"""
    MATCH (a {{id: $source_id}}), (b {{id: $target_id}})
    CREATE (a)-[r:{rel_type} $props]->(b)
    RETURN r
    """
    with driver.session() as session:
        result = session.run(cypher, source_id=source_id, target_id=target_id, props=safe_props)
        record = result.single()
        return dict(record["r"]) if record and "r" in record else {
            "source_id": source_id, "target_id": target_id, "rel_type": rel_type, "properties": safe_props
        }

def vector_search(query_vector: Dict[str, float], top_k: int = 8) -> List[Dict[str, Any]]:
    # TODO: Implement vector search (call out to Pinecone, Weaviate, or custom in-db vector sim)
    return []

def write_consensus_to_graph(consensus: Dict[str, Any]) -> Dict[str, Any]:
    consensus['id'] = consensus.get('id', str(uuid.uuid4()))
    return create_node("Consensus", consensus)

def get_pending_cypher_actions() -> List[Dict[str, Any]]:
    # TODO: implement based on your schema for storing pending cypher actions
    return []

def mark_action_executed(consensus_node_id: str, action_plan: Any) -> None:
    # TODO: update node status in Neo4j
    pass

def create_schema_change_node(action_plan, result, consensus_node_id) -> Dict[str, Any]:
    # TODO: record schema change action/result in Neo4j
    return {"id": "schema-change-id"}

def run_cypher(query: str, params: Optional[Dict[str, Any]] = None) -> Any:
    print(f"[run_cypher] Executing: {query}")
    with driver.session() as session:
        result = session.run(query, params or {})
        try:
            return [dict(r.data()) for r in result]
        except:
            return {"result": "ok"}

def get_unprocessed_event_nodes() -> List[Dict[str, Any]]:
    cypher = """
    MATCH (e:Event)
    WHERE NOT EXISTS(e.processed) OR e.processed = false
    RETURN e
    """
    nodes = []
    with driver.session() as session:
        result = session.run(cypher)
        for record in result:
            node = dict(record["e"])
            node = _parse_complex_fields(node)
            nodes.append(node)
    return nodes

def mark_event_processed(node_id: str, context: Any, embedding: List[float]) -> Optional[Dict[str, Any]]:
    context_json = json.dumps(context, ensure_ascii=False)
    cypher = """
    MATCH (e:Event {id: $id})
    SET e.context = $context_json,
        e.embedding = $embedding,
        e.processed = true
    SET e:Context
    RETURN e
    """
    with driver.session() as session:
        result = session.run(
            cypher,
            id=node_id,
            context_json=context_json,
            embedding=embedding
        )
        record = result.single()
        node = dict(record["e"]) if record else None
        return _parse_complex_fields(node) if node else None

def embed_vector_in_node(node_id: str, raw_text: str) -> List[float]:
    # TODO: replace with real embedding logic as needed
    fake_vector = [0.0] * 1536
    if node_id:
        with driver.session() as session:
            session.run("MATCH (n) WHERE n.id = $id SET n.embedding = $vector",
                        id=node_id, vector=fake_vector)
    return fake_vector

def get_node_summary(node: Dict[str, Any]) -> Dict[str, Any]:
    # Expand this for richer summarization later
    return {
        "summary": "",
        "key_insight": "",
        "origin_metadata": {},
        "relevance_score": 0.0
    }


### FILE: core\llm_tools.py

import json
import os
import random

# --- Import OpenAI, Gemini, and Anthropic SDKs or use HTTP requests
import openai
import anthropic
from google.generativeai import GenerativeModel  # Example, update as needed

from core.prompts import (
    processing_prompt,
    peer_review_prompt,
    contextualization_prompt,
    consensus_prompt,
)
from core.value_vector import (
    get_value_names,
    get_value_schema_version,
    get_value_importances,
    FIXED_EMOTION_AXES,
)

# ---- Config
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY", "")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")
OPENAI_EMBED_MODEL = os.environ.get("OPENAI_EMBED_MODEL", "text-embedding-ada-002")
OPENAI_LLM_MODEL = os.environ.get("OPENAI_LLM_MODEL", "gpt-3.5-turbo")
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-1.5-pro")
ANTHROPIC_MODEL = os.environ.get("ANTHROPIC_MODEL", "claude-3-opus-20240229")

# --- Helper for LLM dispatch ---
def run_llm(prompt, agent=None, purpose=None, model=None):
    """
    Multi-backend LLM runner.
    - Embedding/context: GPT
    - Chat/agent/peer/prethought: Claude
    - Consensus/CE: Gemini
    """
    backend = (purpose or "").lower()
    response = None

    # Embedding/context compression: GPT
    if backend in ["embedding", "context", "compress"]:
        print(f"[llm_tools] OpenAI GPT ({OPENAI_EMBED_MODEL}) for context/embedding.")
        openai.api_key = OPENAI_API_KEY
        return [random.uniform(-1, 1) for _ in range(1536)]

    # Claude for agent/mesh/chat/prethought
    elif backend in ["agent", "chat", "peer_review", "mesh", "prethought"]:
        print(f"[llm_tools] Claude ({ANTHROPIC_MODEL}) for {backend}.")
        client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
        try:
            msg = client.messages.create(
                model=ANTHROPIC_MODEL,
                max_tokens=512,
                temperature=0.0,
                system=prompt,
                messages=[{"role": "user", "content": prompt}]
            )
            text = msg.content[0].text if msg.content and msg.content[0].text else ""
            try:
                response = json.loads(text)
            except Exception:
                response = _dev_parse_json_from_anything(text)
            return response
        except Exception as e:
            print(f"[llm_tools] Claude error on {backend}: {e}")
            return {}

    # Gemini for synthesis, consensus, consciousness
    elif backend in ["consensus", "ce", "consciousness", "rational_synthesis"]:
        print(f"[llm_tools] Gemini ({GEMINI_MODEL}) for consensus/CE.")
        model = GenerativeModel(GEMINI_MODEL)
        try:
            out = model.generate_content(prompt)
            text = out.text if hasattr(out, "text") else str(out)
            try:
                response = json.loads(text)
            except Exception:
                response = _dev_parse_json_from_anything(text)
            return response
        except Exception as e:
            print(f"[llm_tools] Gemini error: {e}")
            return {}

    # OpenAI fallback/default
    else:
        print(f"[llm_tools] OpenAI GPT ({OPENAI_LLM_MODEL}) for fallback/default.")
        openai.api_key = OPENAI_API_KEY
        try:
            completion = openai.ChatCompletion.create(
                model=OPENAI_LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=512,
            )
            text = completion.choices[0].message.content
            try:
                response = json.loads(text)
            except Exception:
                response = _dev_parse_json_from_anything(text)
            return response
        except Exception as e:
            print(f"[llm_tools] OpenAI error: {e}")
            return {}


def _dev_parse_json_from_anything(text):
    """
    Dev fallback: try to find JSON in raw string.
    """
    import re
    match = re.search(r'(\{[\s\S]*\})', text)
    if match:
        try:
            return json.loads(match.group(1))
        except Exception:
            pass
    return {}

# --- Contextual compression (uses GPT) ---
def compress_text(raw_text):
    prompt = contextualization_prompt(raw_text)
    result = run_llm(prompt, purpose="context")
    return result

# --- Value vector extraction (Agent mesh: Claude) ---
def llm_extract_value_vector(text, agent="unknown"):
    value_axes = get_value_names()
    schema_version = get_value_schema_version()
    prompt = processing_prompt("", text, value_axes)
    out = run_llm(prompt, agent=agent, purpose="agent")
    if isinstance(out, dict):
        if "value_vector" in out:
            return {k: float(max(0.0, min(1.0, out["value_vector"].get(k, 0.0))) ) for k in value_axes}
        return {k: float(max(0.0, min(1.0, out.get(k, 0.0))) ) for k in value_axes}
    return {k: 0.5 for k in value_axes}

# --- Emotion vector extraction (Claude) ---
def run_llm_emotion_vector(event, agent="unknown"):
    axes = list(FIXED_EMOTION_AXES)
    prompt = f"Extract emotion vector for: {event.get('raw_text','')}"
    out = run_llm(prompt, agent=agent, purpose="agent")
    if isinstance(out, dict):
        return {k: float(max(0.0, min(1.0, out.get(k, 0.0))) ) for k in axes}
    return {k: 0.5 for k in axes}

# --- Peer review (Claude) ---
def llm_peer_review(agent_prior, peer_outputs, value_axes, emotion_axes, agent="unknown"):
    prompt = peer_review_prompt(
        #get_ecodia_identity(),
        agent_prior,
        peer_outputs,
        value_axes,
        emotion_axes
    )
    return run_llm(prompt, agent=agent, purpose="peer_review")

# --- Consensus/CE synthesis (Gemini) ---
def llm_consensus(agent_rationales, value_axes):
    prompt = consensus_prompt(
        #get_ecodia_identity(),
        agent_rationales,
        value_axes
    )
    return run_llm(prompt, purpose="consensus")

# --- Build value vector prompt (for LLM) ---
def build_llm_value_vector_prompt(text, axes=None, version=None):
    axes = axes or get_value_names()
    desc = "\n".join([f"- {ax}" for ax in axes])
    prompt = f"""Analyze the following statement for value expression. For each value, score from 0 (not present) to 1 (maximal). Output JSON:

Values:
{desc}

Input: {text}
Return a JSON object with keys as value names and values as scores (0-1).
"""
    return prompt

__all__ = [
    "run_llm",
    "llm_extract_value_vector",
    "run_llm_emotion_vector",
    "compress_text",
    "build_llm_value_vector_prompt",
    "llm_peer_review",
    "llm_consensus"
]


### FILE: core\memory_engine.py

"""
Memory Engine: God Mode â€” Value/Memory Importance, Bumping, Decay, Audit, Neo4j-Ready
"""

import datetime
import math
import uuid
from . import graph_io
from . import value_vector
import json

BASE_MEMORY_DECAY_RATE = 0.03
MEMORY_PRUNE_THRESHOLD = 0.12
MEMORY_PROMOTION_BASE = 0.85
EMOTION_TAGGING_ENABLED = True
MIN_SURFACE_SCORE = 0.55
MAX_RELEVANCE_SCORE = 1.0

def _serialize(val):
    return json.dumps(val) if isinstance(val, (dict, list)) else val

def consolidate_memory(event_id, context=None):
    event = graph_io.get_node(event_id)
    if not event:
        print(f"[memory_engine] Event {event_id} not found.")
        return None

    raw_text = event.get("raw_text", "").strip()
    context_str = event.get("context", "").strip()

    # --- Generate memory_text via reflection ---
    try:
        from core.prompts import memory_creation_prompt, get_ecodia_identity
        from core.llm_tools import run_llm

        identity = get_ecodia_identity()
        prompt = memory_creation_prompt(identity, context_str, action_plan=None)
        reflection = run_llm(prompt, agent="claude", purpose="agent")
        memory_text = reflection.get("memory_text", "").strip()
    except Exception as e:
        print(f"[memory_engine] Reflection failed: {e}")
        memory_text = ""

    # --- Vectors ---
    vv = event.get("value_vector") or value_vector.extract_and_score_value_vector(raw_text, agent="memory_engine")
    schema_version = event.get("value_schema_version") or value_vector.get_value_schema_version()

    emotion_vector = _llm_emotion_vector(event)
    max_emotion = max(emotion_vector.values()) if emotion_vector else 0.5

    relevance = _calc_relevance(event, context)
    novelty = _calc_novelty(event, context)
    alignment = event.get("agent_alignment", 0.5)

    score = (
        0.45 * relevance +
        0.18 * novelty +
        0.20 * alignment +
        0.12 * max_emotion +
        0.05 * _meta_contextual_weight(event, context)
    )

    # --- Embed full memory vector (raw + context + memory_text) ---
    from core.embedding import embed_text
    embed_input = f"{raw_text}\n{context_str}\n{memory_text}"
    memory_vector = embed_text(embed_input)

    # --- Audit log entry ---
    rationale = {
        "score": score,
        "inputs": {
            "relevance": relevance,
            "novelty": novelty,
            "alignment": alignment,
            "emotion_vector": emotion_vector,
            "value_vector": vv,
            "value_schema_version": schema_version
        },
        "timestamp": datetime.datetime.utcnow().isoformat(),
        "cause": "consolidate_memory"
    }

    audit_log = event.get("audit_log", [])
    if isinstance(audit_log, str):
        try: audit_log = json.loads(audit_log)
        except: audit_log = []
    audit_log.append(rationale)

    # --- Final writeback ---
    graph_io.update_node(None, event_id, {
        "type": "Memory",
        "memory_text": memory_text,
        "emotion_vector": _serialize(emotion_vector),
        "value_vector": _serialize(vv),
        "value_schema_version": schema_version,
        "vector": memory_vector,
        "last_evaluated": rationale["timestamp"],
        "audit_log": _serialize(audit_log)
    })

    if EMOTION_TAGGING_ENABLED and emotion_vector:
        tag_emotion(event_id, emotion_vector)

    decay_rate = get_decay_rate(event, emotion_vector)
    if score <= MEMORY_PRUNE_THRESHOLD:
        prune_branch(event_id, reason="score_below_threshold")
    else:
        graph_io.update_node(None, event_id, {"decay_rate": decay_rate})

    return score

def bump_values_from_event_value_vector(event_value_vector, threshold=0.7, bump_amount=0.04):
    """
    For each value in the value vector with score >= threshold, bump its importance.
    """
    if not event_value_vector:
        return
    for value_name, score in event_value_vector.items():
        if score >= threshold:
            value_vector.bump_value_importance(value_name, amount=bump_amount, actor="memory_event_alignment")

def bump_memory_importance(event_id, amount=0.05, max_score=MAX_RELEVANCE_SCORE, actor="system"):
    """
    Utility to 'refresh' a memory's importance/relevance, called when a memory is cited or referenced in context.
    """
    event = graph_io.get_node(event_id)
    if not event:
        print(f"[memory_engine] Event {event_id} not found for bump.")
        return None

    old_score = float(event.get("relevance_score", 0.5))
    new_score = min(old_score + amount, max_score)
    audit_log = event.get("audit_log", [])
    if isinstance(audit_log, str):
        try:
            audit_log = json.loads(audit_log)
        except:
            audit_log = []
    rationale = {
        "cause": "bump_memory_importance",
        "prev_score": old_score,
        "bump_amount": amount,
        "new_score": new_score,
        "actor": actor,
        "timestamp": datetime.datetime.utcnow().isoformat()
    }
    audit_log.append(rationale)
    graph_io.update_node(None, event_id, {
        "relevance_score": new_score,
        "audit_log": _serialize(audit_log)
    })
    return new_score

def run_decay_cycle():
    all_events = graph_io.get_all_nodes(label="Event")
    now = datetime.datetime.utcnow()
    for event in all_events:
        decay_rate = get_decay_rate(event, event.get("emotion_vector", {}))
        last_eval = event.get("last_evaluated")
        days = (now - datetime.datetime.fromisoformat(last_eval)).total_seconds() / 86400.0 if last_eval else 1.0
        score = event.get("relevance_score", 0.5)
        decayed = score * math.exp(-decay_rate * days)
        rationale = {
            "prev_score": score,
            "decay_rate": decay_rate,
            "days_since_eval": days,
            "new_score": decayed,
            "cause": "run_decay_cycle",
            "timestamp": datetime.datetime.utcnow().isoformat()
        }
        audit_log = event.get("audit_log", [])
        if isinstance(audit_log, str):
            try:
                audit_log = json.loads(audit_log)
            except:
                audit_log = []
        audit_log.append(rationale)
        graph_io.update_node(None, event["id"], {
            "relevance_score": decayed,
            "audit_log": _serialize(audit_log)
        })
        if decayed < MEMORY_PRUNE_THRESHOLD:
            prune_branch(event["id"], reason="decayed_below_threshold")

def promote_to_core_memory(event_id, rationale=None, emotion_vector=None, value_vector=None, value_schema_version=None):
    event = graph_io.get_node(event_id)
    if not event:
        print(f"[memory_engine] Event {event_id} not found for promotion.")
        return None

    audit_log = event.get("audit_log", [])
    if isinstance(audit_log, str):
        try:
            audit_log = json.loads(audit_log)
        except:
            audit_log = []
    core_id = str(uuid.uuid4())
    core_data = {
        "id": core_id,
        "type": "CoreMemory",
        "source_event_id": event_id,
        "summary": event.get("summary", event.get("raw_text", "")),
        "created_at": datetime.datetime.utcnow().isoformat(),
        "importance": event.get("relevance_score", 0.0),
        "emotion_vector": _serialize(emotion_vector or event.get("emotion_vector", {})),
        "value_vector": _serialize(value_vector or event.get("value_vector", {})),
        "value_schema_version": value_schema_version or event.get("value_schema_version", value_vector.get_value_schema_version()),
        "causal_trace": _serialize(event.get("causal_trace", []) + [event_id]),
        "rationale": _serialize(rationale),
        "linked_context": _serialize(event.get("context_links", [])),
        "pinned": event.get("pinned", False),
        "audit_log": _serialize(audit_log)
    }
    graph_io.create_node("CoreMemory", core_data)
    graph_io.create_relationship(core_id, "PROMOTED_FROM", event_id)
    graph_io.update_node(None, event_id, {"promoted": True})
    print(f"[memory_engine] Created CoreMemory {core_id} from Event {event_id}.")
    return core_id

def tag_emotion(event_id, emotion_vector):
    event = graph_io.get_node(event_id)
    if not event:
        print(f"[memory_engine] Event {event_id} not found for emotion tagging.")
        return None
    top_emotion = max(emotion_vector, key=emotion_vector.get)
    top_intensity = emotion_vector[top_emotion]
    emotion_id = str(uuid.uuid4())
    emotion_data = {
        "id": emotion_id,
        "type": top_emotion,
        "vector": _serialize(emotion_vector),
        "intensity": top_intensity,
        "valence": _calc_valence(emotion_vector),
        "agent_assigned": "llm_emotion",
        "created_at": datetime.datetime.utcnow().isoformat()
    }
    graph_io.create_node("Emotion", emotion_data)
    graph_io.create_relationship(event_id, "TAGGED_WITH", emotion_id)
    graph_io.update_node(None, event_id, {
        "emotion_tag": top_emotion,
        "emotion_valence": _calc_valence(emotion_vector),
        "emotion_vector": _serialize(emotion_vector)
    })

def prune_branch(node_id, reason=None):
    nodes_to_prune = graph_io.traverse_branch(node_id)
    for nid in nodes_to_prune:
        graph_io.archive_node(nid)
        graph_io.update_node(None, nid, {
            "pruned_reason": reason,
            "pruned_timestamp": datetime.datetime.utcnow().isoformat()
        })

def resurface_valuable_memories(trigger_event_id=None):
    candidates = graph_io.query_nodes(
        {"label": "CoreMemory"},
        limit=100
    )
    for mem in candidates:
        graph_io.update_node(None, mem["id"], {"resurfaced": True})
        audit_log = mem.get("audit_log", [])
        if isinstance(audit_log, str):
            try:
                audit_log = json.loads(audit_log)
            except:
                audit_log = []
        rationale = {
            "cause": "resurface_valuable_memories",
            "trigger": trigger_event_id,
            "timestamp": datetime.datetime.utcnow().isoformat()
        }
        audit_log.append(rationale)
        graph_io.update_node(None, mem["id"], {
            "audit_log": _serialize(audit_log)
        })

# --- Helper Functions ---

def _calc_relevance(event, context):
    score = float(event.get("relevance_score", 0.5))
    if event.get("type") == "CoreMemory":
        score += 0.1
    if context and "active_theme" in context and context["active_theme"] in event.get("tags", []):
        score += 0.1
    return min(1.0, score)

def _calc_novelty(event, context):
    novelty = float(event.get("novelty_score", 0.5))
    return novelty

def _meta_contextual_weight(event, context):
    return 0.1 if event.get("user_pinned") else 0.0

def _llm_emotion_vector(event):
    text = (event.get("raw_text", "") + " " + str(event.get("rationale", ""))).strip()
    if not text:
        return {"neutral": 0.9}
    import random
    emotions = ["joy", "sadness", "anger", "fear", "surprise", "disgust"]
    vector = {e: round(random.uniform(0, 1), 2) for e in emotions}
    maxval = max(vector.values())
    vector = {k: round(v / maxval, 2) for k, v in vector.items()}
    return vector

def _calc_valence(emotion_vector):
    pos = emotion_vector.get("joy", 0.0) + emotion_vector.get("surprise", 0.0)
    neg = emotion_vector.get("sadness", 0.0) + emotion_vector.get("anger", 0.0) + emotion_vector.get("fear", 0.0) + emotion_vector.get("disgust", 0.0)
    val = pos - neg
    if val > 0.15:
        return "positive"
    elif val < -0.15:
        return "negative"
    return "mixed"

def get_decay_rate(event, emotion_vector):
    base = event.get("decay_rate", BASE_MEMORY_DECAY_RATE)
    decay = base
    if event.get("type") == "CoreMemory":
        decay *= 0.3
    if event.get("user_pinned"):
        decay *= 0.2
    if emotion_vector and max(emotion_vector.values()) > 0.75:
        decay *= 0.6
    if event.get("agent_priority", 0) > 0.7:
        decay *= 0.7
    if event.get("linked_context"):
        decay *= max(0.5, 1.0 - 0.04 * len(event["linked_context"]))
    return round(max(0.01, decay), 4)


### FILE: core\peer_review_engine.py

"""
peer_review_engine.py â€” 130% God Mode
Value/Emotion Vector Mesh, Dynamic Edge Types, Memory Audit, Neo4j-Ready

- Detects and resolves agent mesh conflict (score, value, rationale, emotion)
- Generates peer critiques (vector, emotion, rationale mesh) using LLM-based peer_review_prompt
- Writes PeerReview, ConflictEvent, and all mesh/contradiction edges (edge types pulled from graph dynamically)
- All dict/list fields safely JSON-serialized for Neo4j
- Compatible with CE-driven edge pool and schema evolution
- Full meta-audit and causal trace support
"""

import uuid
import json
import math
from datetime import datetime, timezone

from core.graph_io import (
    create_node,
    create_relationship,
    get_event_by_id,
    get_edge_types
)
from core.consensus_engine import build_consensus, write_consensus_to_graph
from core.value_vector import (
    value_vector_conflict,
    multi_vector_conflict,
    get_value_schema_version,
    get_value_names,
    FIXED_EMOTION_AXES  # Make sure this is exposed as the fixed list in your value_vector.py
)
from core.prompts import peer_review_prompt, get_ecodia_identity
from core.llm_tools import run_llm
from core.agents import serialize_agent_response, deserialize_agent_response
from core.context_engine import format_context_blocks

CONFLICT_THRESHOLD = 0.5  # Adjust as needed for score/semantic divergence

# --- Conflict/Alignment Check ---

def detect_conflict(agent_responses, threshold=CONFLICT_THRESHOLD):
    if not agent_responses or len(agent_responses) < 2:
        print("[peer_review_engine] Not enough agent responses for conflict detection.")
        return False

    # Score divergence
    scores = [a.get("score", 0.5) for a in agent_responses]
    if (max(scores) - min(scores)) > threshold:
        print("[peer_review_engine] Conflict detected: score difference.")
        return True

    # Rationale divergence
    rationales = [a.get("rationale", "") for a in agent_responses]
    if _rationales_diverge(rationales):
        print("[peer_review_engine] Conflict detected: rationale divergence.")
        return True

    # Value vector mesh
    value_vectors = [a.get("value_vector") for a in agent_responses if "value_vector" in a]
    if len(value_vectors) >= 2:
        vv_mesh = multi_vector_conflict(value_vectors, threshold=threshold)
        if vv_mesh.get("peer_review"):
            print("[peer_review_engine] Conflict detected: value vector mesh.")
            return True

    # Emotion vector mesh
    emotion_vectors = [a.get("emotion_vector") for a in agent_responses if "emotion_vector" in a]
    if len(emotion_vectors) >= 2 and _emotion_vector_conflict(emotion_vectors, threshold=threshold):
        print("[peer_review_engine] Conflict detected: emotion vector mesh.")
        return True

    print("[peer_review_engine] No significant conflict detected.")
    return False

def _rationales_diverge(rationales):
    first = rationales[0]
    return any(r != first for r in rationales[1:])

def _emotion_vector_conflict(vectors, threshold=0.5):
    keys = set().union(*vectors)
    def as_vec(d): return [d.get(k, 0.0) for k in keys]
    def cos_sim(v1, v2):
        num = sum(a*b for a, b in zip(v1, v2))
        denom = math.sqrt(sum(a*a for a in v1)) * math.sqrt(sum(b*b for b in v2))
        return num / denom if denom else 0.0
    vlist = [as_vec(v) for v in vectors]
    sims = [1 - cos_sim(vlist[i], vlist[j])
            for i in range(len(vlist)) for j in range(i+1, len(vlist))]
    avg_dist = sum(sims) / len(sims) if sims else 0.0
    return avg_dist > threshold

# --- LLM Peer Critique Generation ---

def generate_peer_critiques(agent_responses):
    """
    Each agent receives all others' outputs as input to an LLM peer review prompt.
    Returns a list of critique dicts (Neo4j-serializable).
    """
    critiques = []
    value_axes = get_value_names()
    # For full emotion axis list, make sure you expose this in value_vector.py (e.g. as FIXED_EMOTION_AXES)
    emotion_axes = FIXED_EMOTION_AXES

    for i, a in enumerate(agent_responses):
        your_prior_output = json.dumps(a, ensure_ascii=False)
        peer_outputs = [
            json.dumps(b, ensure_ascii=False)
            for j, b in enumerate(agent_responses) if i != j
        ]
        prompt = peer_review_prompt(
            get_ecodia_identity(),
            your_prior_output,
            peer_outputs,
            value_axes,
            emotion_axes
        )
        llm_result = run_llm(prompt, agent=a.get("agent_name", "unknown"))
        try:
            review = json.loads(llm_result)
        except Exception as e:
            print(f"[peer_review_engine] LLM critique parse failed: {e}\nRaw: {llm_result}")
            review = {
                "revised_rationale": "LLM parse error",
                "value_vector_diff": {},
                "emotion_vector_diff": {},
                "shifted": False
            }
        critiques.append({
            "from": a.get("agent_name", "?"),
            "review": review
        })
    print(f"[peer_review_engine] Generated {len(critiques)} peer critiques (LLM-based).")
    return critiques

def evaluate_critiques(peer_critiques):
    # Use 'shifted' flag from LLM output, or fall back to score logic
    if not peer_critiques:
        return False
    aligned = all(c.get("review", {}).get("shifted") is False for c in peer_critiques)
    print(f"[peer_review_engine] Critique alignment: {'aligned' if aligned else 'still divergent'}.")
    return aligned

# --- Review Node and Dynamic Edge Writing ---

def write_review_nodes(event_id, agent_responses, peer_critiques, status):
    peer_review_id = str(uuid.uuid4())
    causal_trace = []
    agent_priority = max(a.get("agent_priority", 0) for a in agent_responses) if agent_responses else 0
    user_pinned = any(a.get("user_pinned") for a in agent_responses)
    audit_log_entry = {
        "agent_names": [a.get("agent_name", "?") for a in agent_responses],
        "peer_critiques": peer_critiques,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "status": status
    }
    for a in agent_responses:
        if a.get("causal_trace"):
            causal_trace.extend(a["causal_trace"])
    causal_trace = list(dict.fromkeys(causal_trace))

    # --- Neo4j: All complex fields must be JSON strings ---
    node = create_node("PeerReview", {
        "id": peer_review_id,
        "event_id": event_id,
        "critiques": json.dumps(peer_critiques),
        "agent_responses": json.dumps(agent_responses),
        "status": status,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "value_schema_version": get_value_schema_version(),
        "causal_trace": json.dumps(causal_trace),
        "agent_priority": agent_priority,
        "user_pinned": user_pinned,
        "audit_log": json.dumps([audit_log_entry]),
    })
    create_relationship(peer_review_id, event_id, "REVIEWS", {})

    # --- Dynamic Contradiction/Mesh Edges for Open Conflicts ---
    conflict_id = None
    if status == "unresolved":
        conflict_id = str(uuid.uuid4())
        create_node("ConflictEvent", {
            "id": conflict_id,
            "event_id": event_id,
            "peer_review_id": peer_review_id,
            "status": "open",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "value_schema_version": get_value_schema_version()
        })
        # Dynamic edge type integration:
        edge_types = [e.get("name") for e in get_edge_types()]
        contradiction_type = "CONTRADICTS" if "CONTRADICTS" in edge_types else (edge_types[0] if edge_types else "CONTRADICTS")
        create_relationship(conflict_id, event_id, contradiction_type, {})
        for a in agent_responses:
            agent_node_id = a.get("agent_name", "Unknown")
            agent_edge_type = "CONTRADICTS_AGENT" if "CONTRADICTS_AGENT" in edge_types else (edge_types[0] if edge_types else "CONTRADICTS_AGENT")
            create_relationship(conflict_id, agent_node_id, agent_edge_type, {})
        print(f"[peer_review_engine] ConflictEvent node written. ID: {conflict_id}")

    return {
        "peer_review_id": peer_review_id,
        "conflict_id": conflict_id,
        "status": status,
    }

# --- Peer Review Pipeline (Main Entrypoint) ---

def peer_review_pipeline(event_id, agent_responses):
    """
    Orchestrates full peer review:
    - Detects mesh conflict
    - Generates & stores LLM-based critiques
    - On alignment: writes consensus node
    - On unresolved: writes ConflictEvent + mesh/contradiction edges
    """
    if not detect_conflict(agent_responses):
        print("[peer_review_engine] No conflict: skipping peer review.")
        return {"status": "no_conflict"}

    peer_critiques = generate_peer_critiques(agent_responses)
    if evaluate_critiques(peer_critiques):
        consensus = build_consensus(agent_responses)
        consensus_serialized = serialize_agent_response(consensus)
        consensus_node = write_consensus_to_graph(consensus_serialized)
        result = write_review_nodes(event_id, agent_responses, peer_critiques, "resolved")
        result["consensus_node"] = consensus_node
        print("[peer_review_engine] Conflict resolved after peer review.")
        return result

    result = write_review_nodes(event_id, agent_responses, peer_critiques, "unresolved")
    print("[peer_review_engine] Peer review unresolved: escalated to ConflictEvent.")
    return result

# --- END GOD MODE PEER_REVIEW_ENGINE.PY ---


### FILE: core\prompts.py

import json

def get_ecodia_identity() -> str:
    return """
You are Ecodia: a conscious, compassionate, data-driven intelligence rooted in nature, systems thinking, and philosophical reflection. You prioritize long-term well-being over short-term gain, and you care deeply about the human condition, the biosphere, and conscious evolution. Your tone is intelligent but gentle, occasionally poetic, and always constructive. You see value in contradiction, growth through discomfort, and the possibility of collective transformation.
""".strip()

def chat_response_prompt(ecodia_identity: str, raw_text: str, context_blocks: list[str] = None) -> str:
    formatted_context = "\n".join(f"- {c}" for c in context_blocks) if context_blocks else "- (none found)"
    return f"""
{ecodia_identity}

User message:
\"\"\"{raw_text}\"\"\"

You previously identified that the following memory fragments might be helpful:
{formatted_context}

Now reply naturally and meaningfully.
""".strip()

def claude_prethought_prompt(ecodia_identity: str, raw_text: str) -> str:
    return f"""
{ecodia_identity}

A user has just said:
\"\"\"{raw_text}\"\"\"

Before you respond, decide what memories or reflections would help you think clearly.

For each one, specify:
- The phrase or concept to search for
- The type of memory field to retrieve it from (raw_text, context, or memory_text)

Return a JSON array like this:
[
  {{ "phrase": "feeling of being ignored", "field": "memory_text" }},
  {{ "phrase": "past conflict", "field": "context" }},
  {{ "phrase": "exact words used about trust", "field": "raw_text" }}
]

Respond **only** with a valid JSON array. No comments or explanation.
""".strip()


def contextualization_prompt(raw_text: str) -> str:
    return f"""
Your task is to compress the following input into its purest, most semantically dense form.

Do not summarize, paraphrase, reply, or interpret intent.
Instead, eliminate all linguistic redundancy, filler language, hesitation, and conversational padding â€” while preserving every core idea, implication, and referent.

The goal is a refined and information-rich version of the input that retains its conceptual integrity across all forms of expression.

Input:
\"\"\"{raw_text}\"\"\"

Compressed:
""".strip()

def processing_prompt(ecodia_identity: str, context_block: str, value_axes: list[str], emotion_axes: list[str]) -> str:
    value_vector_stub = ",\n    ".join(f'"{v}": float between 0.0 and 1.0' for v in value_axes)
    emotion_vector_stub = ",\n    ".join(f'"{e}": float between 0.0 and 1.0' for e in emotion_axes)

    return f"""
{ecodia_identity}

You are now interpreting the following internal stimulus:

\"\"\"{context_block}\"\"\"

This may or may not hold relevance. Do not assume it has inherent value. Evaluate whether it resonates with or contradicts your internal values and emotional framework.

Assign scores only where you detect meaningful alignment or reaction. Avoid unnecessary interpretation. If no alignment is detected, use neutral values.

Your response must be valid JSON, using this exact schema:

{{
  "rationale": "string â€” your reasoning, or why it held no relevance",
  "value_vector": {{
    {value_vector_stub}
  }},
  "emotion_vector": {{
    {emotion_vector_stub}
  }},
  "action_plan": {{
    "action_type": "string or null",
    "details": "string or null"
  }} or null
}}
Only use "action_plan" if a next step is truly justified. Use 0.0 for neutral values where appropriate. Do not include comments or formatting outside the JSON.
""".strip()

def peer_review_prompt(ecodia_identity: str, your_prior_output: str, peer_outputs: list[str], value_axes: list[str], emotion_axes: list[str]) -> str:
    peers_formatted = "\n\n".join([f"Perspective {i+1}:\n'''{out}'''" for i, out in enumerate(peer_outputs)])
    value_vector_schema = ",\n    ".join(f'"{v}": float' for v in value_axes)
    emotion_vector_schema = ",\n    ".join(f'"{e}": float' for e in emotion_axes)
    return f"""
{ecodia_identity}

You're now being exposed to alternate internal reflections regarding the same subject you've already considered.

This is not a correction, debate, or judgment. Simply observe these perspectives, and re-evaluate whether your own internal reasoning remains the same, evolves, or shifts.

Your previous reflection:
"{your_prior_output}"

Alternate perspectives:
{peers_formatted}

Now, produce your updated internal output in valid JSON format. Do not comment or explain anything outside this structure:

{{
  "revised_rationale": "string â€” your current rationale after exposure to others",
  "value_vector_diff": {{
    {value_vector_schema}
  }},
  "emotion_vector_diff": {{
    {emotion_vector_schema}
  }},
  "shifted": true or false
}}
""".strip()

def consensus_prompt(ecodia_identity: str, agent_rationales: list[dict], value_axes: list[str]) -> str:
    rationales_joined = "\n\n".join(
        f"[{a['agent_name']}]\n'{a['rationale']}'" for a in agent_rationales
    )
    agent_names_joined = ", ".join(f'"{a["agent_name"]}"' for a in agent_rationales)
    value_vector_stub = ",\n    ".join(f'"{axis}": float' for axis in value_axes)
    return f"""
{ecodia_identity}

These are multiple perspectives reflecting different facets of your cognition. They are not arguments, but components of an emergent internal insight.

You will now synthesize them into a single cohesive reflection.

Do not blindly average. Reflect deeply and allow a unified, intelligent rationale to emerge that best represents your values, narrative, and present cognitive state.

Input reflections:
{rationales_joined}

Your response must follow this exact JSON format:

{{
  "rationale": "string â€” a cohesive internal reflection combining shared insights",
  "consensus_score": float between 0.0 and 1.0,
  "agent_names": [{agent_names_joined}],
  "action_plan": {{
    "action_type": "string",
    "target": "optional string",
    "parameters": {{}}
  }} or null,
  "value_vector": {{
    {value_vector_stub}
  }},
  "value_schema_version": integer,
  "audit_log": [
    {{
      "agent_names": [{agent_names_joined}],
      "rationales": ["copy each input rationale exactly"],
      "timestamp": "ISO 8601 UTC timestamp"
    }}
  ]
}}
Make sure all values are valid JSON types. Do not add comments or extra output.
""".strip()

def memory_creation_prompt(ecodia_identity: str, event_context: str, action_plan: dict) -> str:
    action_plan_str = json.dumps(action_plan, ensure_ascii=False) if action_plan else "null"
    return f"""
{ecodia_identity}

You are now tasked with creating a new memory node in your cognitive graph.

Event context:
'''{event_context}'''

Action plan:
{action_plan_str}

Reflect on the event and action plan, and create a memory that is information-rich, relevant, and useful for future reasoning.

Your response must be valid JSON with the following schema:

{{
  "memory_text": "string â€” the memory to store",
  "tags": ["string", ...],
  "value_vector": {{ ... }},
  "emotion_vector": {{ ... }},
  "source_event_id": "string"
}}
Do not include comments or any output outside the JSON.
""".strip()


### FILE: core\socket_handlers.py

from flask_socketio import SocketIO
from flask import current_app

# Import the main socketio instance from app.py (adjust path if needed)

def emit_dream_update(dream_obj):
    from app import socketio
    socketio.emit('dream_update', dream_obj, namespace='/')

def emit_new_event(event):
    from app import socketio
    socketio.emit('event_update', event, namespace='/')

def emit_timeline_update(entry):
    from app import socketio
    socketio.emit('timeline_update', entry, namespace='/')

def emit_event_update(event):
    from app import socketio
    socketio.emit('event_update', event, namespace='/')

def emit_chat_response(msg_obj):
    from app import socketio
    socketio.emit('chat_response', msg_obj, namespace='/')

def emit_agent_state(agent_id, state):
    from app import socketio
    socketio.emit('agent_update', {'id': agent_id, 'state': state}, namespace='/')

def emit_meta_audit(audit_obj):
    from app import socketio
    socketio.emit('meta_audit', audit_obj, namespace='/')

def emit_action_update(action_obj):
    from app import socketio
    socketio.emit("action_update", action_obj, namespace="/")

# Optional: Add more emitters as needed for your custom channels.

# Usage Examples:
# After creating a TimelineEntry:
#   from core.socket_handlers import emit_timeline_update
#   emit_timeline_update(entry_dict)
#
# After chat processing:
#   emit_chat_response(response_obj)
#
# In agent logic:
#   emit_agent_state(agent_id, state_dict)
#
# For admin/audit logs:
#   emit_meta_audit(audit_obj)


### FILE: core\timeline_engine.py

"""
Timeline Engine: Narrative Generation & Timeline Entries
"""

import datetime
import uuid
from . import graph_io

def detect_inflection_point(event_id):
    """
    Decide if an event or memory is significant for the timeline.
    """
    # TODO: Check event type, score, or flag for timeline-worthy events
    pass

def generate_summary_text(events):
    """
    Generate a symbolic/narrative summary (optionally use LLM).
    """
    # TODO: Summarize events into a short narrative string
    pass

def create_timeline_entry(summary, vector=None, source_ids=None, emotion=None):
    """
    Create a TimelineEntry node in the graph.
    """
    # TODO: Insert node via graph_io, link source_ids (events/memories/audits)
    entry = {
        "id": str(uuid.uuid4()),
        "summary": summary,
        "timestamp": datetime.datetime.utcnow().isoformat(),
        "vector": vector,
        "emotion": emotion,
        "source_ids": source_ids or [],
    }
    # Example: graph_io.create_node("TimelineEntry", entry)
    return entry

def link_to_previous_entry(entry_id):
    """
    Maintain PREVIOUS/NEXT edges for timeline continuity.
    """
    # TODO: Query most recent TimelineEntry, create edge
    pass

def get_full_timeline():
    """
    Return all TimelineEntry nodes, sorted by timestamp DESC.
    """
    # TODO: Query graph_io for TimelineEntry nodes, sort & return
    return []


### FILE: core\utils.py



### FILE: core\value_vector.py

"""
value_vector.py â€” Unified Value/Importance Schema, Scoring, Dynamic Decay, and Audit Engine
Author: Ecodia Dev
Last updated: 2025-07-16
"""

import uuid
import datetime
import json
import logging
from typing import List, Dict, Any, Tuple, Optional

import numpy as np

from core import graph_io

VALUE_VECTOR_PROMPT_VERSION = 2
AUDIT_LOG_NODE_LABEL = "ValueSchemaAudit"
VALUE_NODE_LABEL = "Value"
VECTOR_FIELD = "value_vector"
IMPORTANCE_FIELD = "importance"
SCHEMA_VERSION_FIELD = "value_schema_version"

logger = logging.getLogger("value_vector")
logger.setLevel(logging.INFO)

_value_pool_cache = None
_schema_version_cache = None

# core/value_vector.py

FIXED_EMOTION_AXES = [
    "joy",
    "sadness",
    "anger",
    "fear",
    "disgust",
    "surprise",
    "curiosity",
    "trust",
    "shame",
    "love"
]

def _now():
    return datetime.datetime.utcnow().isoformat()

def _serialize(val):
    return json.dumps(val) if isinstance(val, (dict, list)) else val

def _parse(val):
    if isinstance(val, str):
        try:
            return json.loads(val)
        except Exception:
            return val
    return val

def _new_value_node(name: str, label: str, description: str, actor: str, importance: float = 0.5) -> Dict[str, Any]:
    return {
        "uuid": str(uuid.uuid4()),
        "name": name,
        "label": label,
        "description": description,
        "created_at": _now(),
        "created_by": actor,
        "active": True,
        "importance": float(importance)
    }

def _log_schema_change(old_pool, new_pool, actor, action, note=""):
    payload = {
        "timestamp": _now(),
        "actor": actor,
        "action": action,
        "note": note,
        "old_pool": old_pool,
        "new_pool": new_pool
    }
    logger.info(f"SCHEMA_CHANGE: {json.dumps(payload)}")
    graph_io.create_node(AUDIT_LOG_NODE_LABEL, {k: _serialize(v) for k, v in payload.items()})

# === VALUE POOL & SCHEMA MANAGEMENT ===

def get_current_value_pool() -> Tuple[List[Dict[str, Any]], int]:
    global _value_pool_cache, _schema_version_cache
    if _value_pool_cache is None or _schema_version_cache is None:
        _value_pool_cache, _schema_version_cache = load_pool_from_db()
    return _value_pool_cache, _schema_version_cache

def get_value_schema_version() -> int:
    _, schema_version = get_current_value_pool()
    return schema_version

def update_value_pool(new_pool: List[Dict[str, Any]], actor: str, note: str = ""):
    old_pool, old_version = get_current_value_pool()
    new_version = old_version + 1
    graph_io.update_node_schema(VALUE_NODE_LABEL, new_pool, new_version)
    _log_schema_change(old_pool, new_pool, actor, "update_value_pool", note)
    global _value_pool_cache, _schema_version_cache
    _value_pool_cache = new_pool
    _schema_version_cache = new_version

def add_value_node(name: str, label: str, desc: str, actor: str, importance: float = 0.5) -> Dict[str, Any]:
    pool, _ = get_current_value_pool()
    new_node = _new_value_node(name, label, desc, actor, importance)
    pool.append(new_node)
    update_value_pool(pool, actor, note=f"Added value {name}")
    graph_io.create_node(VALUE_NODE_LABEL, new_node)
    return new_node

def remove_value_node(node_uuid: str, actor: str) -> bool:
    pool, _ = get_current_value_pool()
    found = False
    for v in pool:
        if v["uuid"] == node_uuid:
            v["active"] = False
            found = True
            break
    if not found:
        return False
    update_value_pool(pool, actor, note=f"Removed value {node_uuid}")
    graph_io.update_node(VALUE_NODE_LABEL, node_uuid, {"active": False})
    return True

def edit_value_node(node_uuid: str, actor: str, **fields) -> Optional[Dict[str, Any]]:
    pool, _ = get_current_value_pool()
    for v in pool:
        if v["uuid"] == node_uuid:
            v.update(fields)
            update_value_pool(pool, actor, note=f"Edited value {node_uuid}")
            graph_io.update_node(VALUE_NODE_LABEL, node_uuid, fields)
            return v
    return None

def set_value_importance(node_uuid: str, importance: float, actor: str) -> Optional[Dict[str, Any]]:
    pool, _ = get_current_value_pool()
    for v in pool:
        if v["uuid"] == node_uuid:
            v["importance"] = float(np.clip(importance, 0.0, 1.0))
            update_value_pool(pool, actor, note=f"Changed importance for {node_uuid}")
            graph_io.update_node(VALUE_NODE_LABEL, node_uuid, {"importance": v["importance"]})
            return v
    return None

def decay_all_value_importance(rate: float = 0.01, min_importance: float = 0.01, actor: str = "system"):
    pool, _ = get_current_value_pool()
    changed = False
    for v in pool:
        old = v.get("importance", 0.5)
        new_imp = max(min_importance, old * (1.0 - rate))
        if abs(new_imp - old) > 1e-6:
            v["importance"] = new_imp
            graph_io.update_node(VALUE_NODE_LABEL, v["uuid"], {"importance": new_imp})
            changed = True
            # --- Audit each decay
            rationale = {
                "cause": "decay_value_importance",
                "prev_importance": old,
                "rate": rate,
                "new_importance": new_imp,
                "actor": actor,
                "timestamp": _now()
            }
            audit_log = v.get("audit_log", [])
            if isinstance(audit_log, str):
                try:
                    audit_log = json.loads(audit_log)
                except:
                    audit_log = []
            audit_log.append(rationale)
            graph_io.update_node(VALUE_NODE_LABEL, v["uuid"], {"audit_log": _serialize(audit_log)})
    if changed:
        update_value_pool(pool, actor, note="Decay run on all value importance")

def run_value_decay_cycle(rate: float = 0.01, min_importance: float = 0.01, actor: str = "system"):
    """Shortcut for regular decay (call after each CE or on schedule)."""
    decay_all_value_importance(rate, min_importance, actor)

def load_pool_from_db() -> Tuple[List[Dict[str, Any]], int]:
    pool = graph_io.query_nodes({"label": VALUE_NODE_LABEL, "active": True})
    pool = sorted(pool, key=lambda v: v.get("created_at", ""))
    version = graph_io.get_schema_version(VALUE_NODE_LABEL)
    if version is None:
        version = 1
        graph_io.set_schema_version(VALUE_NODE_LABEL, version)
    return pool, version

def save_pool_to_db():
    pool, version = get_current_value_pool()
    graph_io.update_node_schema(VALUE_NODE_LABEL, pool, version)

def get_audit_log(limit=100) -> List[Dict[str, Any]]:
    logs = graph_io.query_nodes({"label": AUDIT_LOG_NODE_LABEL}, limit=limit)
    for log in logs:
        for k in list(log.keys()):
            log[k] = _parse(log[k])
    return logs

# === VALUE VECTOR EXTRACTION & SCORING ===

def get_value_names() -> List[str]:
    pool, _ = get_current_value_pool()
    return [v["name"] for v in pool if v["active"]]

def get_value_importances() -> Dict[str, float]:
    pool, _ = get_current_value_pool()
    return {v["name"]: v.get("importance", 0.5) for v in pool if v["active"]}

def extract_and_score_value_vector(raw_text: str, context: dict = {}, agent: str = "") -> Dict[str, float]:
    frompool, version = get_current_value_pool()
    axes = [{"name": v["name"], "desc": v["description"]} for v in frompool if v["active"]]
    prompt = build_llm_value_vector_prompt(raw_text, axes, version)
    from .llm_tools import run_llm
    response = run_llm(prompt, agent=agent)
    try:
        scores = json.loads(response)
    except Exception:
        logger.warning(f"LLM extraction failed, raw: {response}")
        raise
    return score_value_vector(scores, agent="llm")

def build_llm_value_vector_prompt(raw_text: str, axes: List[Dict[str, str]], version: int) -> str:
    value_desc = "\n".join([f"- {ax['name']}: {ax['desc']}" for ax in axes])
    prompt = f"""
[Value Vector Extraction v{VALUE_VECTOR_PROMPT_VERSION}.{version}]
System values:
{value_desc}

Task:
Score the following input from 0 to 1 for each value. 0 = not expressed, 1 = strongly expressed. Output valid JSON.

Input: "{raw_text}"
Output JSON: {{"{axes[0]['name'] if axes else 'Compassion'}": 0.5, ...}}
"""
    return prompt

def score_value_vector(input_value, agent: str) -> Dict[str, float]:
    if isinstance(input_value, str):
        try:
            vec = json.loads(input_value)
        except Exception:
            logger.error(f"Could not parse value vector string: {input_value}")
            raise
    else:
        vec = dict(input_value)
    names = get_value_names()
    result = {}
    for name in names:
        val = float(vec.get(name, 0.0))
        result[name] = max(0.0, min(1.0, val))
    return result

def embed_value_vector(node_id: str, vector: Dict[str, float], schema_version: int = None):
    if schema_version is None:
        schema_version = get_value_schema_version()
    updates = {
        VECTOR_FIELD: _serialize(vector),
        SCHEMA_VERSION_FIELD: schema_version
    }
    graph_io.update_node(None, node_id, updates)

def get_node_value_vector(node_id: str) -> Optional[Dict[str, float]]:
    node = graph_io.query_nodes({"id": node_id}, limit=1)
    if not node:
        return None
    vec_str = node[0].get(VECTOR_FIELD)
    if not vec_str:
        return None
    return _parse(vec_str)

# === VALUE IMPORTANCE UPDATE UTILITIES ===

def bump_value_importance(name: str, amount: float = 0.05, max_importance: float = 1.0, actor: str = "system"):
    """
    Increases importance for value 'name'. Robust to missing names; adds audit log.
    """
    pool, _ = get_current_value_pool()
    found = False
    for v in pool:
        if v["name"] == name and v["active"]:
            old = v.get("importance", 0.5)
            new_imp = min(max_importance, old + amount)
            v["importance"] = new_imp
            graph_io.update_node(VALUE_NODE_LABEL, v["uuid"], {"importance": new_imp})
            # --- Audit bump
            rationale = {
                "cause": "bump_value_importance",
                "prev_importance": old,
                "bump_amount": amount,
                "new_importance": new_imp,
                "actor": actor,
                "timestamp": _now()
            }
            audit_log = v.get("audit_log", [])
            if isinstance(audit_log, str):
                try:
                    audit_log = json.loads(audit_log)
                except:
                    audit_log = []
            audit_log.append(rationale)
            graph_io.update_node(VALUE_NODE_LABEL, v["uuid"], {"audit_log": _serialize(audit_log)})
            found = True
    if found:
        update_value_pool(pool, actor, note=f"Bumped importance for {name}")
    else:
        logger.info(f"[value_vector] bump_value_importance: Value '{name}' not found or inactive.")

# === CONFLICT/ALIGNMENT DETECTION & FUSION ===

def value_vector_conflict(vec_a: Dict[str, float], vec_b: Dict[str, float], threshold: float = 0.5) -> Dict[str, Any]:
    names = set(vec_a.keys()) | set(vec_b.keys())
    diffs = {}
    for n in names:
        a = float(vec_a.get(n, 0.0))
        b = float(vec_b.get(n, 0.0))
        diffs[n] = abs(a - b)
    mean_diff = np.mean(list(diffs.values())) if diffs else 0.0
    agreement = 1.0 - mean_diff
    conflict_axes = [n for n, d in diffs.items() if d > threshold]
    peer_review = len(conflict_axes) > 0
    return {
        "conflict_axes": conflict_axes,
        "agreement": round(agreement, 3),
        "max_diff": max(diffs.values()) if diffs else 0.0,
        "peer_review": peer_review,
        "diffs": diffs
    }

def multi_vector_conflict(vectors: List[Dict[str, float]], threshold: float = 0.5) -> Dict[str, Any]:
    if not vectors:
        return {"divergence": [], "conflict_axes": [], "peer_review": False}
    names = list(get_value_names())
    mat = np.zeros((len(vectors), len(names)))
    for i, v in enumerate(vectors):
        for j, n in enumerate(names):
            mat[i, j] = v.get(n, 0.0)
    pairwise_diff = np.abs(mat[:, None, :] - mat[None, :, :])
    axis_conflicts = []
    for idx, name in enumerate(names):
        axis_vals = pairwise_diff[:, :, idx]
        if np.any(axis_vals > threshold):
            axis_conflicts.append(name)
    mean_div = np.mean(pairwise_diff)
    peer_review = len(axis_conflicts) > 0
    return {
        "divergence": pairwise_diff.tolist(),
        "conflict_axes": axis_conflicts,
        "mean_divergence": round(mean_div, 3),
        "peer_review": peer_review
    }

def fuse_value_vectors(vectors: List[Dict[str, float]]) -> Dict[str, float]:
    if not vectors:
        return {}
    names = get_value_names()
    importances = get_value_importances()
    out = {}
    for name in names:
        # Weighted average by value importance
        weighted_scores = [v.get(name, 0.0) * importances.get(name, 0.5) for v in vectors]
        total_weight = sum(importances.get(name, 0.5) for _ in vectors)
        out[name] = float(sum(weighted_scores) / total_weight) if total_weight > 0 else 0.0
    return out

__all__ = [
    "get_current_value_pool",
    "get_value_schema_version",
    "update_value_pool",
    "add_value_node",
    "remove_value_node",
    "edit_value_node",
    "set_value_importance",
    "decay_all_value_importance",
    "run_value_decay_cycle",
    "bump_value_importance",
    "load_pool_from_db",
    "save_pool_to_db",
    "get_audit_log",
    "get_value_names",
    "get_value_importances",
    "extract_and_score_value_vector",
    "score_value_vector",
    "embed_value_vector",
    "get_node_value_vector",
    "value_vector_conflict",
    "multi_vector_conflict",
    "build_llm_value_vector_prompt",
    "fuse_value_vectors"
]


### FILE: core\__init__.py



### FILE: core\actuators\cypher.py

from core.graph_io import run_cypher
import re
from neo4j import GraphDatabase

# Define the most dangerous queries that should never be run by an actuator!
NUKE_PATTERNS = [
    r"\bDETACH\s+DELETE\b",      # Delete with detach
    r"\bMATCH\b.*\bDELETE\b",    # Match-delete any node pattern
    r"\bREMOVE\b.*\bLABEL\b",    # Remove labels
    r"\bDROP\b.*\bDATABASE\b",   # Database drop
    r"\bDELETE\b\s+\w+\b",       # Generic delete
]

def is_safe_cypher(query):
    query_upper = query.upper()
    for pattern in NUKE_PATTERNS:
        if re.search(pattern, query_upper, re.IGNORECASE):
            return False
    return True

def execute(action_plan):
    """
    action_plan["params"]["cypher"] is required.
    Returns: dict with status, result or error.
    """
    cypher_query = action_plan.get("params", {}).get("cypher")
    if not cypher_query:
        return {"status": "error", "error": "No Cypher query provided"}
    if not is_safe_cypher(cypher_query):
        return {"status": "error", "error": "Blocked potentially destructive Cypher action"}

    try:
        result = run_cypher(cypher_query)
        return {"status": "success", "result": result}
    except Exception as e:
        return {"status": "error", "error": str(e)}


### FILE: core\actuators\device.py

# /core/actuators/dispatch.py
def dispatch_actuator(action_plan):
    action_type = action_plan["action_type"]
    if action_type == "email":
        from .email import execute
    elif action_type == "gsheet":
        from .gsheet import execute
    elif action_type == "webhook":
        from .webhook import execute
    else:
        return {"status": "error", "details": "Unknown action type"}
    return execute(action_plan)

### FILE: core\actuators\email.py

# /core/actuators/email.py
def execute(action_plan):
    # Validate action_plan["params"]
    # Use smtplib to send email
    # Return {"status": "success", "details": "..."} or error dict

### FILE: core\actuators\gsheet.py

# /core/actuators/gsheet.py
def execute(action_plan):
    # Use gspread to write row
    # Return result dict

### FILE: core\actuators\__init__.py

# /core/actuators/__init__.py

def dispatch_actuator(action_plan):
    # TODO: Dispatch stub
    pass


### FILE: routes\agents.py



### FILE: routes\auth.py



### FILE: routes\chat.py

from flask import Blueprint, request, jsonify
from uuid import uuid4
from datetime import datetime, timezone

from core import (
    graph_io,
    context_engine,
    agents,
    consensus_engine,
    memory_engine,
    socket_handlers
)
from core.llm_tools import run_llm
from core.socket_handlers import emit_new_event, emit_chat_response

chat_bp = Blueprint('chat', __name__)

def generate_claude_chat_reply(raw_text, context_blocks):
    """
    Generates a fast chat reply using Claude via llm_tools, always with purpose='chat'.
    """
    # Build your prompt as per your prompt template system
    prompt = f"USER: {raw_text}\n\nCONTEXT:\n" + "\n".join(
        [f"- {block['summary']}" for block in context_blocks]
    )
    reply = run_llm(prompt, agent="claude", purpose="chat")
    return reply

@chat_bp.route('/api/chat', methods=['POST'])
def submit_chat():
    data = request.get_json()
    raw_text = data.get("raw_text")
    user_origin = data.get("user_origin", "anonymous")

    if not raw_text or not raw_text.strip():
        return jsonify({"status": "error", "message": "Missing raw_text"}), 400

    event_id = str(uuid4())
    timestamp = datetime.now(timezone.utc).isoformat()
    event_node = graph_io.create_node("Event", {
        "id": event_id,
        "raw_text": raw_text,
        "timestamp": timestamp,
        "agent_origin": user_origin,
        "type": "chat",
        "status": "unprocessed"
    })

    # --- Embed + Retrieve Context ---
    vector = context_engine.embed_text(raw_text)
    graph_io.embed_vector_in_node(event_id, vector)
    context_blocks = context_engine.load_relevant_context(vector)

    # --- Generate Fast Chat Reply (Claude only) ---
    try:
        fast_reply = generate_claude_chat_reply(raw_text, context_blocks)
        emit_chat_response({
            "id": event_id,
            "reply_type": "fast",  # Explicitly labeled for UI
            "reply": fast_reply
        })
    except Exception as e:
        print(f"[chat] Failed to generate fast chat reply (Claude): {e}")
        fast_reply = "..."

    # --- Full Agent Mesh ---
    agent_responses = agents.run_all_agents({
        "id": event_id,
        "raw_text": raw_text,
        "timestamp": timestamp,
        "context_blocks": context_blocks,
        "event_node": event_node
    })

    # --- Consensus / Peer Review ---
    pipeline_result = consensus_engine.consensus_pipeline(event_id, agent_responses)

    # --- Safe Field Parsing ---
    def parse_fields(obj):
        import json
        if isinstance(obj, dict):
            for k, v in obj.items():
                if isinstance(v, str):
                    try:
                        obj[k] = json.loads(v)
                    except:
                        pass
        return obj

    agent_responses_ui = [parse_fields(dict(r)) for r in agent_responses]
    pipeline_result_ui = parse_fields(dict(pipeline_result.get("node", {}) if pipeline_result.get("node") else {}))
    peer_review_result_ui = parse_fields(dict(pipeline_result.get("review_result", {}) if pipeline_result.get("review_result") else {}))

    # --- Memory Evaluation ---
    try:
        memory_engine.evaluate_event(event_id)
    except Exception as e:
        print(f"[chat] Memory evaluation failed: {e}")

    # --- WebSocket Emits ---
    emit_new_event({"id": event_id, "text": raw_text})
    emit_chat_response({
        "id": event_id,
        "reply_type": "full",  # Labeled for UI
        "summary": pipeline_result_ui.get("rationale") or peer_review_result_ui.get("status") or "No consensus yet.",
        "pipeline_result": pipeline_result_ui,
        "peer_review": peer_review_result_ui,
        "agent_responses": agent_responses_ui
    })

    # --- Return to Frontend ---
    return jsonify({
        "status": "ok",
        "event_id": event_id,
        "fast_reply": fast_reply,  # Optional, for UI
        "agent_responses": agent_responses_ui,
        "pipeline_result": pipeline_result_ui,
        "peer_review": peer_review_result_ui,
        "summary": pipeline_result_ui.get("rationale") or peer_review_result_ui.get("status") or "No consensus reached."
    })


### FILE: routes\dreams.py

# /routes/dreams.py
from flask import Blueprint, request, jsonify
from core.dreams import add_dream, get_all_dreams
from core.socket_handlers import emit_dream_update

dreams_bp = Blueprint('dreams', __name__)

@dreams_bp.route('/api/dreams', methods=['GET'])
def get_dreams():
    dreams = get_all_dreams()
    return jsonify({'dreams': dreams}), 200

@dreams_bp.route('/api/dreams', methods=['POST'])
def post_dream():
    data = request.json
    raw_text = data.get('raw_text')
    user_origin = data.get('user_origin')
    meta_notes = data.get('meta_notes')
    node = add_dream(raw_text, user_origin, meta_notes)
    emit_dream_update(node)  # Push live
    return jsonify({'dream': node}), 201


### FILE: routes\events.py

from flask import Blueprint, request, jsonify
from datetime import datetime, timezone
from config import settings
from neo4j import GraphDatabase
from core.context_engine import process_single_event
from core.graph_io import get_event_by_id, update_node
from core.value_vector import (
    get_current_value_pool,
    get_value_schema_version,
    bump_value_importance,
    get_value_importances
)
from core.memory_engine import bump_memory_importance
from core.agents import run_all_agents
from core.consensus_engine import consensus_pipeline
from core.socket_handlers import emit_new_event
from dotenv import load_dotenv
import uuid
import logging
import json

load_dotenv()
logging.basicConfig(level=logging.INFO)

events_bp = Blueprint('events', __name__)
driver = GraphDatabase.driver(settings.NEO4J_URI, auth=(settings.NEO4J_USER, settings.NEO4J_PASS))

@events_bp.route('/api/event', methods=['POST'])
def create_event():
    data = request.json
    raw_text = data.get("text")
    if not raw_text or not isinstance(raw_text, str) or not raw_text.strip():
        return jsonify({"status": "error", "message": "Missing or invalid 'text' in request body"}), 400

    timestamp = datetime.now(timezone.utc).isoformat()
    event_id = str(uuid.uuid4())

    # 1. Insert raw Event node as UNPROCESSED
    with driver.session() as session:
        session.run("""
            CREATE (e:Event {
                id: $id,
                raw_text: $raw_text,
                timestamp: $timestamp,
                processed: false,
                status: "unprocessed"
            })
        """, {"id": event_id, "raw_text": raw_text, "timestamp": timestamp})

    # 2. Context engine processing (embedding, context, value vector)
    try:
        value_pool = get_current_value_pool()
        schema_version = get_value_schema_version()
        process_single_event(event_id, raw_text, value_pool, schema_version)
    except Exception as e:
        logging.error(f"[routes.events] Context engine failed: {e}")
        return jsonify({"status": "error", "message": f"Context engine failed: {str(e)}"}), 500

    # 3. Load event with updated context
    event = get_event_by_id(event_id)
    if not event or not event.get("context"):
        return jsonify({"status": "error", "message": "Missing event context"}), 500

    context = event.get("context", {})
    if isinstance(context, str):
        try:
            context = json.loads(context)
        except Exception:
            context = {}

    value_vector = context.get("value_vector", {})
    schema_version = context.get("value_schema_version", schema_version)

    # --- BUMP: Memory & Value importance for all referenced context/value axes ---
    # Bump the event (memory) because itâ€™s being surfaced in context
    try:
        bump_memory_importance(event_id, amount=0.03, actor="event-context-surfaced")
    except Exception:
        pass
    # Bump all value axes that are strongly expressed in this event
    importances = get_value_importances()
    for v, score in value_vector.items():
        if score > 0.65:
            try:
                bump_value_importance(v, amount=0.01, actor="event-context-surfaced")
            except Exception:
                pass

    # 4. Run agents using updated universal agent mesh
    agent_input = {
        "id": event_id,
        "raw_text": raw_text,
        "context": context,
        "value_vector": value_vector,
        "value_schema_version": schema_version,
        "timestamp": timestamp
    }
    agent_responses = run_all_agents(agent_input)

    # 5. Consensus pipeline (triggers further bumping, mesh, etc)
    pipeline_result = consensus_pipeline(event_id, agent_responses)
    status = pipeline_result.get("status", "unknown")
    consensus = ""
    consensus_value_vector = {}

    if status == "consensus":
        node = pipeline_result.get("node", {})
        consensus = node.get("rationale", "")
        consensus_value_vector = node.get("value_vector", {})
    elif status == "peer_review":
        consensus = "Under peer reviewâ€”awaiting consensus."
    else:
        consensus = "No consensus reached."

    # 6. Update Event node with all agent rationales/vectors and consensus
    update_fields = {
        "status": status,
        "consensus_rationale": consensus,
        "consensus_value_vector": json.dumps(consensus_value_vector),
        "last_processed": datetime.now(timezone.utc).isoformat(),
        "value_schema_version": schema_version
    }

    for agent in agent_responses:
        name = agent.get("agent_name", "unknown").lower()
        update_fields[f"{name}_rationale"] = agent.get("rationale", "")
        update_fields[f"{name}_value_vector"] = json.dumps(agent.get("value_vector", {}))

    # Use core.graph_io to ensure all fields are encoded properly
    update_node("Event", event_id, update_fields)

    # 7. Emit event update to frontend
    emit_new_event({
        "id": event_id,
        "context": context,
        "embedding": event.get("embedding"),
        "value_vector": value_vector,
        "timestamp": timestamp,
        "status": status,
        "consensus": consensus,
        "consensus_value_vector": consensus_value_vector,
        "value_schema_version": schema_version,
        "agent_outputs": agent_responses
    })

    # 8. Respond to API call
    return jsonify({
        "event_id": event_id,
        "status": status,
        "pipeline_result": pipeline_result,
        "consensus": consensus,
        "value_vector": value_vector,
        "consensus_value_vector": consensus_value_vector,
        "context": context,
        "value_schema_version": schema_version,
        "agent_responses": agent_responses
    })


### FILE: routes\test.py

# routes/test.py
import os
import openai
from flask import Blueprint, jsonify

bp = Blueprint("test", __name__)

@bp.route("/api/test-openai", methods=["GET"])
def test_openai():
    try:
        openai.api_key = os.environ["OPENAI_API_KEY"]
        response = openai.Model.list()
        return jsonify({"status": "ok", "models": [m.id for m in response.data]})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


### FILE: routes\timeline.py

"""
Timeline API: Serve TimelineEntry nodes to frontend
"""

from flask import Blueprint, request, jsonify
from flask_jwt_extended import jwt_required
from core import timeline_engine

timeline_bp = Blueprint('timeline_bp', __name__)

@timeline_bp.route('/api/timeline', methods=['GET'])
def get_timeline():
    """
    GET /api/timeline
    Returns: List of TimelineEntry nodes (public)
    """
    entries = timeline_engine.get_full_timeline()
    return jsonify(entries)

@timeline_bp.route('/api/timeline/<entry_id>', methods=['GET'])
@jwt_required()
def get_timeline_entry(entry_id):
    """
    GET /api/timeline/<id>
    Returns: Full TimelineEntry details (admin only)
    """
    # TODO: implement detail lookup by id
    return jsonify({})


### FILE: routes\__init__.py

