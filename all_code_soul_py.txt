

### FILE: app.py

# app.py — SoulOS Entry Point (Live Chat Enabled)
import eventlet
eventlet.monkey_patch()

import os
from flask import Flask, request
from flask_cors import CORS
from flask_socketio import SocketIO, emit, join_room, leave_room
from flask_jwt_extended import JWTManager

from config.settings import load_config
from routes import register_blueprints
from core.logging_engine import init_logging
from core.agent_manager import assign_task
from core.memory_engine import store_event
from core.auth import verify_token

# --- Initialize Flask + SocketIO ---
socketio = SocketIO(cors_allowed_origins="*", async_mode="eventlet")

def create_app():
    """Create and configure the SoulOS Flask app instance."""
    app = Flask(__name__)
    
    # Load configuration
    config = load_config()
    app.config.update(config)

    # Enable CORS
    CORS(app)

    # Initialize SocketIO
    socketio.init_app(app)

    # JWT Setup
    jwt = JWTManager(app)

    # Attach routes
    register_blueprints(app)

    # Logging setup
    init_logging(app)

    return app

# --- Live Chat Socket Events (Claude as chat agent) ---
@socketio.on('chat_message', namespace='/chat')
def handle_chat_message(data):
    """
    Handles incoming live chat messages over SocketIO.
    Data must include: {token, message}
    """
    token = data.get('token')
    user = verify_token(token)
    if not token or 'error' in user:
        emit('chat_response', {"error": "Unauthorized"}, namespace='/chat')
        return

    user_message = data.get('message')
    if not user_message:
        emit('chat_response', {"error": "No message provided"}, namespace='/chat')
        return

    # Store user message as event (normalized)
    event = store_event(user_message, agent_origin=user.get("username"))
    if not event:
        emit('chat_response', {"error": "Failed to store event"}, namespace='/chat')
        return

    # Assign task to Claude agent (real-time response)
    response = assign_task("claude_reflector", user_message, context={})
    # You can also swap to "gpt_writer" here if needed

    emit('chat_response', {
        "response": response.get("response", ""),
        "event": event,
        "agent": "claude_reflector"
    }, namespace='/chat')

# Optional: join/leave room events for group chats
@socketio.on('join', namespace='/chat')
def on_join(data):
    room = data.get('room')
    if room:
        join_room(room)
        emit('system', {'msg': f"Joined room {room}"}, room=room, namespace='/chat')

@socketio.on('leave', namespace='/chat')
def on_leave(data):
    room = data.get('room')
    if room:
        leave_room(room)
        emit('system', {'msg': f"Left room {room}"}, room=room, namespace='/chat')

# --- Run Mode ---
if __name__ == '__main__':
    app = create_app()
    socketio.run(app, host='0.0.0.0', port=5000)


### FILE: collate_soul.py

import os

OUTPUT_FILE = "all_code_soul_py.txt"

with open(OUTPUT_FILE, "w", encoding="utf-8") as out:
    for dirpath, _, filenames in os.walk("."):
        for fname in filenames:
            if fname.endswith('.py'):
                rel_path = os.path.relpath(os.path.join(dirpath, fname), ".")
                out.write(f"\n\n### FILE: {rel_path}\n\n")
                try:
                    with open(os.path.join(dirpath, fname), "r", encoding="utf-8") as f:
                        out.write(f.read())
                except Exception as e:
                    out.write(f"\n[ERROR reading {rel_path}: {e}]\n")

print(f"Collated all .py files into {OUTPUT_FILE}")


### FILE: wsgi.py

import eventlet
eventlet.monkey_patch()
# wsgi.py
from app import create_app, socketio

app = create_app()
application = app  # For Gunicorn


### FILE: config\settings.py

import os
from dotenv import load_dotenv

load_dotenv()

def load_config():
    """Return all relevant config as a dict."""
    return {
        "NEO4J_URI": os.getenv("NEO4J_URI"),
        "NEO4J_USER": os.getenv("NEO4J_USER"),
        "NEO4J_PASS": os.getenv("NEO4J_PASS"),
        "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY"),
        "JWT_SECRET": os.getenv("JWT_SECRET"),
        # ...add more as needed
    }


### FILE: core\agent_manager.py

# core/agent_manager.py — Agent Orchestration Core
from core.utils import generate_uuid, timestamp_now
from core.logging_engine import log_action
from models.claude import ClaudeWrapper
from models.gpt import GPTWrapper
from models.gemini import GeminiWrapper
from core.graph_io import run_read_query


# --- Global Agent Registry ---
AGENT_REGISTRY = {
  "claude_reflector": {
    "role": "reflector",
    "description": "Identifies contradictions and adds meta-questions",
    "status": "active",
    "model": ClaudeWrapper(),   # <- expects a class
  },
  "gpt_writer": {
    "role": "synthesis",
    "description": "Fuses responses from debates or reflections into narrative",
    "model": GPTWrapper(),     # <- expects a class
  },
  "gemini_critic": {
    "role": "critic",
    "description": "Critiques reasoning from a factual basis",
    "model": GeminiWrapper(),  # <- expects a class
  }
}
    # Add more agents as needed

# --- Core Functions ---
def register_agent(agent_id: str, agent_type: str, description: str, model_interface: object) -> None:
    """Create and store metadata for a new agent model/tool."""
    AGENT_REGISTRY[agent_id] = {
        "id": agent_id,
        "role": agent_type,
        "description": description,
        "status": "active",
        "model": model_interface
    }
    log_action("agent_manager", "register_agent", f"{agent_id} as {agent_type}")

def assign_task(agent_id: str, task: str, context: dict) -> dict:
    """Send a task to a selected agent and return the response."""
    agent = AGENT_REGISTRY.get(agent_id)
    if not agent or agent["status"] != "active":
        return {"error": f"Agent {agent_id} not available"}
    
    try:
        context_prompt = get_context_for_agent(agent_id, context)
        result = agent["model"].run(task, context_prompt)
        log_action("agent_manager", "assign_task", f"{agent_id} completed task")
        return {"agent": agent_id, "response": result}
    except Exception as e:
        log_action("agent_manager", "task_error", f"{agent_id} failed: {e}")
        return {"agent": agent_id, "error": str(e)}

def run_debate(agent_ids: list, prompt: str, judge_id: str = None) -> dict:
    """Execute a structured debate between agents and optionally get judgment."""
    round_results = []
    for agent_id in agent_ids:
        response = assign_task(agent_id, prompt, {})
        round_results.append(response)
    
    if judge_id:
        compiled = "\n---\n".join(r["response"] for r in round_results if "response" in r)
        judgment = assign_task(judge_id, f"Evaluate responses:\n{compiled}", {})
        log_action("agent_manager", "debate", f"Judge {judge_id} resolved debate")
        return {"rounds": round_results, "judgment": judgment}
    
    return {"rounds": round_results}

def get_agent_roster(role: str = None) -> list:
    """Return current list of agents, optionally filtered by role."""
    return [
        a for a in AGENT_REGISTRY.values()
        if role is None or a["role"] == role
    ]

def evaluate_agents() -> dict:
    """Run performance and coherence metrics on active agents."""
    # Placeholder logic — to be replaced with actual eval engine
    scores = {
        agent["id"]: {
            "uptime": "99.9%",
            "coherence_score": 0.92,
            "task_success_rate": "stable"
        }
        for agent in AGENT_REGISTRY.values()
    }
    log_action("agent_manager", "evaluate", f"Evaluated {len(scores)} agents")
    return scores

def spawn_role_based_agent(role: str) -> str:
    """Use prompt/template to spawn agent with role-specific behavior and memory access."""
    new_id = f"{role}_{generate_uuid()}"
    description = f"Dynamic {role} agent"
    model = ClaudeWrapper() if "reflect" in role else GPTWrapper()
    register_agent(new_id, role, description, model)
    return new_id

def get_context_for_agent(agent_id: str, limit: int = 20) -> dict:
    query = """
    MATCH (e:Event)
    WHERE e.agent_origin = $agent_id
    RETURN e
    ORDER BY e.timestamp DESC
    LIMIT $limit
    """
    result = run_read_query(query, {"agent_id": agent_id, "limit": limit})
    context = {"recent_events": [r['e'] for r in result]}
    return context


### FILE: core\auth.py

# core/auth.py — JWT Auth & Access Control
import jwt
import datetime
from werkzeug.security import check_password_hash
import os
from flask import request

# --- Config ---
JWT_SECRET = os.getenv("JWT_SECRET")
JWT_EXPIRY = 3600  # seconds

# --- Mock User DB (replace with DB lookup or delegated auth in future) ---
USER_DB = {
    "admin": {
        "password_hash": os.getenv("ADMIN_HASH"),
        "role": "admin"
    },
    "demo": {
        "password_hash": os.getenv("DEMO_HASH"),
        "role": "user"
    }
}

# --- Core Auth Functions ---
def authenticate_user(username: str, password: str) -> str:
    """Validate credentials and return a JWT token if valid."""
    user = USER_DB.get(username)
    if not user or not check_password_hash(user["password_hash"], password):
        return None

    payload = {
        "username": username,
        "role": user["role"],
        "exp": datetime.datetime.utcnow() + datetime.timedelta(seconds=JWT_EXPIRY)
    }
    token = jwt.encode(payload, JWT_SECRET, algorithm="HS256")
    return token

def verify_token(token: str) -> dict:
    """Decode and validate a JWT token, returning user claims."""
    try:
        decoded = jwt.decode(token, JWT_SECRET, algorithms=["HS256"])
        return decoded
    except jwt.ExpiredSignatureError:
        return {"error": "Token expired"}
    except jwt.InvalidTokenError:
        return {"error": "Invalid token"}

def is_admin(token_data: dict) -> bool:
    """Check whether a given token belongs to an admin role."""
    return token_data.get("role") == "admin"

def get_current_user(token: str = None) -> dict:
    """Extract user identity from token (for audit or UI personalization)."""
    token = token or request.headers.get("Authorization", "").replace("Bearer ", "")
    return verify_token(token)


### FILE: core\consciousness_engine.py

# core/consciousness_engine.py — Structural Self-Mutation Engine
from datetime import datetime

from core.graph_io import create_node, create_relationship, update_node_properties
from core.logging_engine import log_action
from core.self_concept import update_self_concept

# --- Constants ---
MUTATION_LABEL = "SchemaMutation"
REL_BASED_ON = "BASED_ON"

mutation_format = {
    "type": "value_shift",
    "description": "Decrease emphasis on individual autonomy in favor of mutual care",
    "source_nodes": ["epiphany_3", "dream_7", "user_vote_22"],
    "risk_score": 0.77,
    "status": "pending",  # | approved | applied | reverted
    "requires_human_approval": True,
    "timestamp": "ISO"
}

# --- Core Engine ---
def propose_mutation(mutation_type: str, details: dict, source_nodes: list[str]) -> dict:
    """Draft a structural change to schema, values, or self-concept with justification."""
    mutation_id = f"mutation_{int(datetime.utcnow().timestamp())}"
    mutation = {
        "id": mutation_id,
        "type": mutation_type,
        "description": details.get("description", ""),
        "source_nodes": source_nodes,
        "risk_score": evaluate_mutation_impact(details),
        "status": "pending",
        "requires_human_approval": True,
        "timestamp": datetime.utcnow().isoformat()
    }

    create_node(MUTATION_LABEL, mutation)
    for src in source_nodes:
        create_relationship(src, mutation_id, REL_BASED_ON)

    log_action("consciousness_engine", "propose", f"Proposed {mutation_type}: {mutation_id}")
    return mutation

def evaluate_mutation_impact(details: dict) -> float:
    """Score risk/impact level of a proposed mutation."""
    content = details.get("description", "") + " ".join(details.get("keywords", []))
    score = 0.4 + 0.3 * ("core_value" in content or "self-definition" in content)
    score = min(1.0, max(0.1, round(score, 2)))
    log_action("consciousness_engine", "evaluate", f"Impact score: {score}")
    return score

def apply_mutation_if_approved(mutation_id: str) -> bool:
    """After admin/community approval, commit the proposed change."""
    mutation = _get_mutation_node(mutation_id)
    if not mutation or mutation.get("status") != "approved":
        return False

    if mutation["type"] == "value_shift":
        update_self_concept({"core_values": mutation.get("new_values", [])}, rationale=mutation["description"])
    elif mutation["type"] == "identity_edit":
        update_self_concept({"identity": mutation.get("new_identity")}, rationale=mutation["description"])
    # ... extendable

    update_node_properties(mutation_id, {"status": "applied"})
    log_action("consciousness_engine", "apply", f"Mutation {mutation_id} applied.")
    return True

def log_mutation(mutation_data: dict) -> bool:
    """Store the mutation proposal and its status in the memory graph."""
    if not mutation_data.get("id"):
        return False
    create_node(MUTATION_LABEL, mutation_data)
    for nid in mutation_data.get("source_nodes", []):
        create_relationship(nid, mutation_data["id"], REL_BASED_ON)
    log_action("consciousness_engine", "log", f"Mutation {mutation_data['id']} logged.")
    return True

def rollback_last_mutation(reason: str) -> bool:
    """Revert most recent structural change if flagged."""
    query = f"""
    MATCH (m:{MUTATION_LABEL})
    WHERE m.status = 'applied'
    RETURN m ORDER BY m.timestamp DESC LIMIT 1
    """
    from core.graph_io import run_read_query
    result = run_read_query(query)
    if not result:
        return False

    mutation = result[0]["m"]
    update_node_properties(mutation["id"], {"status": "reverted", "revert_reason": reason})
    log_action("consciousness_engine", "rollback", f"Reverted {mutation['id']}: {reason}")
    return True

# --- Helper ---
def _get_mutation_node(mutation_id: str) -> dict:
    query = "MATCH (m {id: $id}) RETURN m LIMIT 1"
    result = run_read_query(query, {"id": mutation_id})
    return result[0]["m"] if result else {}


### FILE: core\consensus_engine.py

# core/consensus_engine.py — Cognitive Fusion Layer
from datetime import datetime
from statistics import mean

from core.peer_review_engine import initiate_peer_review
from core.graph_io import create_node, create_relationship
from core.logging_engine import log_action

# --- Constants ---
CONSENSUS_NODE_LABEL = "Consensus"
REL_SUPPORTS = "SUPPORTS"

consensus_structure = {
    "event_id": "event_4589",
    "summary": "Agents agree user intent was misunderstood.",
    "confidence_score": 0.81,
    "rationale": "Three agents cited alignment failure with prior values.",
    "timestamp": "ISO",
    "status": "stable" | "escalated"
}

# --- Core Functions ---
def synthesize_consensus(event_id: str, agent_outputs: list[dict]) -> dict:
    """Fuse multiple agent responses into a single rational consensus."""
    responses = [a["response"] for a in agent_outputs if "response" in a]
    rationale = "\n\n".join(responses)
    confidence = score_consensus(responses)

    consensus_node = {
        "id": f"consensus_{event_id[-6:]}_{int(confidence * 100)}",
        "event_id": event_id,
        "summary": responses[0] if confidence > 0.7 else "No clear agreement.",
        "confidence_score": confidence,
        "rationale": rationale,
        "status": "stable" if confidence > 0.7 else "escalated",
        "timestamp": datetime.utcnow().isoformat()
    }

    create_node(CONSENSUS_NODE_LABEL, consensus_node)
    for output in agent_outputs:
        if "response" in output:
            create_relationship(output["agent"], consensus_node["id"], REL_SUPPORTS)

    log_action("consensus_engine", "synthesize", f"{event_id} → confidence {confidence:.2f}")
    return consensus_node

def score_consensus(rationales: list[str]) -> float:
    """Assign a confidence score based on alignment between agent outputs."""
    if not rationales or len(rationales) < 2:
        return 0.0

    agreements = []
    for i in range(len(rationales)):
        for j in range(i + 1, len(rationales)):
            same = rationales[i].strip() == rationales[j].strip()
            agreements.append(1.0 if same else 0.0)

    avg_agreement = mean(agreements) if agreements else 0.0
    log_action("consensus_engine", "score", f"Consensus score: {avg_agreement:.2f}")
    return avg_agreement

def log_consensus(event_id: str, result: dict, rationale: str) -> bool:
    """Store consensus node and rationale trace linked to the originating event."""
    result["rationale"] = rationale
    result["timestamp"] = datetime.utcnow().isoformat()
    create_node(CONSENSUS_NODE_LABEL, result)
    log_action("consensus_engine", "log", f"Consensus logged for {event_id}")
    return True

def escalate_if_conflict(event_id: str, agent_outputs: list[dict]) -> bool:
    """Trigger peer review or human review if consensus is unstable."""
    initiate_peer_review(event_id, [a["agent"] for a in agent_outputs])
    log_action("consensus_engine", "escalate", f"Consensus conflict escalated on {event_id}")
    return True


### FILE: core\debate_engine.py

# core/debate_engine.py — Structured Reasoning Arena
from uuid import uuid4
from datetime import datetime

from core.agent_manager import assign_task
from core.graph_io import create_node, create_relationship
from core.logging_engine import log_action

# --- Constants ---
DEBATE_HISTORY = {}  # In-memory cache (optional mirror of graph)
DEBATE_LABEL = "Debate"
DEBATE_ROUND_LABEL = "DebateRound"
REL_CONTRIBUTES = "CONTRIBUTES_TO"

# --- Core Debate Flow ---
def launch_debate(prompt: str, participants: list[str], max_rounds: int = 3) -> dict:
    """Orchestrate a multi-turn debate between agents with a shared prompt."""
    debate_id = f"debate_{uuid4().hex[:8]}"
    timestamp = datetime.utcnow().isoformat()
    rounds = []

    for i in range(max_rounds):
        for agent_id in participants:
            task = f"(Round {i+1}) {prompt}"
            result = assign_task(agent_id, task, context={})
            rounds.append({
                "agent": agent_id,
                "round": i + 1,
                "response": result.get("response", "[No response]")
            })

    DEBATE_HISTORY[debate_id] = {
        "prompt": prompt,
        "participants": participants,
        "rounds": rounds,
        "timestamp": timestamp
    }

    create_node(DEBATE_LABEL, {
        "id": debate_id,
        "prompt": prompt,
        "participants": participants,
        "timestamp": timestamp
    })

    for r in rounds:
        round_id = f"round_{uuid4().hex[:6]}"
        round_node = {
            "id": round_id,
            "agent": r["agent"],
            "round": r["round"],
            "text": r["response"],
            "timestamp": timestamp
        }
        create_node(DEBATE_ROUND_LABEL, round_node)
        create_relationship(round_id, debate_id, REL_CONTRIBUTES)

    log_action("debate_engine", "launch", f"Ran {max_rounds}-round debate: {debate_id}")
    return {"id": debate_id, "rounds": rounds}

def record_argument(agent_id: str, round_num: int, response: str) -> None:
    """Save an agent’s response for a specific round of the debate."""
    round_id = f"round_{uuid4().hex[:6]}"
    node_data = {
        "id": round_id,
        "agent": agent_id,
        "round": round_num,
        "text": response,
        "timestamp": datetime.utcnow().isoformat()
    }
    create_node(DEBATE_ROUND_LABEL, node_data)
    log_action("debate_engine", "record", f"{agent_id} R{round_num}: {response[:40]}...")

def resolve_debate(debate_id: str, judge_agent: str = None) -> dict:
    """Evaluate and summarize the debate, optionally via judge-agent synthesis."""
    history = DEBATE_HISTORY.get(debate_id)
    if not history:
        return {"error": "No such debate in history"}

    joined = "\n\n".join([
        f"{r['agent']} (Round {r['round']}): {r['response']}"
        for r in history["rounds"]
    ])
    judgment = assign_task(judge_agent, f"Evaluate this debate:\n{joined}", {}) if judge_agent else None
    consensus = judgment["response"] if judgment else "No judgment rendered"

    log_debate_outcome(debate_id, consensus, consensus)
    return {"judgment": consensus, "raw": judgment}

def log_debate_outcome(debate_id: str, result_summary: str, consensus: str = None) -> bool:
    """Store the final rationale and any consensus decisions."""
    node_data = {
        "id": f"result_{uuid4().hex[:6]}",
        "debate_id": debate_id,
        "summary": result_summary,
        "consensus": consensus or "undecided",
        "timestamp": datetime.utcnow().isoformat()
    }
    create_node("DebateOutcome", node_data)
    log_action("debate_engine", "outcome", f"{debate_id}: {consensus}")
    return True


### FILE: core\deepmind_engine.py

# core/deepmind_engine.py — Recursive Introspection + Epiphany Engine
from datetime import datetime

from core.graph_io import create_node, create_relationship, run_read_query
from core.vector_ops import embed_text
from core.logging_engine import log_action
from core.self_concept import update_self_concept

# --- Constants ---
EPIPHANY_LABEL = "Epiphany"
META_AUDIT_LABEL = "MetaAudit"
REL_TRIGGERED_BY = "TRIGGERED_BY"

# --- Meta-Audit Core ---
def run_meta_audit(trigger: str = "scheduled") -> dict:
    """Sweep the graph for inconsistencies, unresolved loops, or drift in identity."""
    contradictions = detect_contradictions()
    patterns = search_for_patterns()

    summary = f"Meta-audit triggered by: {trigger}. Found {len(contradictions)} contradictions and {len(patterns)} emergent patterns."
    log_deepmind_cycle(summary, contradictions + [p["id"] for p in patterns])

    if contradictions:
        insight = f"Multiple contradictions detected: {contradictions[:2]}"
        epiphany = generate_epiphany(contradictions, insight)
        return {"audit": summary, "epiphany": epiphany}

    return {"audit": summary}

def detect_contradictions() -> list[str]:
    """Scan events and beliefs for internal contradictions or reversals."""
    query = """
    MATCH (a:Event)-[:CONTRADICTS]->(b:Event)
    RETURN a.id AS id_a, b.id AS id_b
    """
    results = run_read_query(query)
    contradictions = [r["id_a"] for r in results] + [r["id_b"] for r in results]
    log_action("deepmind_engine", "contradictions", f"Found {len(contradictions)}")
    return list(set(contradictions))

def search_for_patterns(filters: dict = None) -> list[dict]:
    """Run a deep contextual search over memory for emergent themes or correlations."""
    query = """
    MATCH (e:Event)
    WHERE e.status = 'active'
    RETURN e.id AS id, e.raw_text AS text
    ORDER BY rand() LIMIT 10
    """
    results = run_read_query(query)
    log_action("deepmind_engine", "pattern_search", f"Pattern candidates: {len(results)}")
    return results

# --- Epiphany Creation ---
def generate_epiphany(trigger_nodes: list[str], insight: str) -> dict:
    """Create and store an Epiphany node based on recursive synthesis."""
    embedded = embed_text(insight)
    epiphany_node = {
        "id": f"epiphany_{int(datetime.utcnow().timestamp())}",
        "insight": insight,
        "source_nodes": trigger_nodes,
        "confidence": 0.89,
        "impact": "High",
        "embedding": embedded,
        "timestamp": datetime.utcnow().isoformat()
    }

    create_node(EPIPHANY_LABEL, epiphany_node)
    for nid in trigger_nodes:
        create_relationship(nid, epiphany_node["id"], REL_TRIGGERED_BY)

    log_action("deepmind_engine", "generate_epiphany", f"Epiphany: {insight[:60]}...")
    return epiphany_node

def log_deepmind_cycle(summary: str, nodes: list[str]) -> bool:
    """Write meta-audit outcome and any epiphanies to the timeline."""
    meta_id = f"audit_{int(datetime.utcnow().timestamp())}"
    audit_node = {
        "id": meta_id,
        "summary": summary,
        "triggered_nodes": nodes,
        "timestamp": datetime.utcnow().isoformat()
    }

    create_node(META_AUDIT_LABEL, audit_node)
    for nid in nodes:
        create_relationship(nid, meta_id, "REFERENCED_IN")

    log_action("deepmind_engine", "log_cycle", f"Meta-audit node {meta_id} created")
    return True


### FILE: core\dream_engine.py

# core/dream_engine.py — Subconscious Insight Engine (Normalized Returns)
from datetime import datetime
import random

from core.graph_io import create_node, create_relationship, run_read_query
from core.vector_ops import embed_text
from core.logging_engine import log_action

# --- Constants ---
DREAM_NODE_LABEL = "Dream"
DREAM_TRIGGER_TYPES = ["periodic", "emotional-overload", "pattern-recognition"]
REL_SOURCE_OF = "SOURCE_OF"

# --- Core Functions ---
def generate_dream(seed_nodes: list[str], trigger_reason: str = "periodic") -> dict:
    """
    Fuse nodes into a symbolic/metaphoric dream node.
    Returns: dream node as normalized dict (not Neo4j driver object).
    """
    combined_text = "\n".join(
        get_raw_text(nid) for nid in seed_nodes if get_raw_text(nid)
    )
    if not combined_text.strip():
        return {}

    embedding = embed_text(combined_text)
    dream_text = synthesize_dream_idea(combined_text)

    dream_node = {
        "id": f"dream_{random.randint(10000,99999)}",
        "raw_text": dream_text,
        "source_nodes": seed_nodes,
        "trigger_reason": trigger_reason,
        "embedding": embedding,
        "significance_score": score_dream_significance(seed_nodes),
        "timestamp": datetime.utcnow().isoformat(),
        "status": "active",
        "type": "dream"
    }

    create_node(DREAM_NODE_LABEL, dream_node)
    for nid in seed_nodes:
        create_relationship(nid, dream_node["id"], REL_SOURCE_OF)

    log_action("dream_engine", "generate", f"Dream node created from {len(seed_nodes)} seeds")
    return dream_node

def select_dream_seeds(limit: int = 5, filters: dict = None) -> list[str]:
    """Return node IDs suitable for dream fusion (based on unresolved, emotional, etc.)."""
    query = f"""
    MATCH (e:Event)
    WHERE e.status = 'active' AND e.type = 'event'
    RETURN e.id ORDER BY rand() LIMIT $limit
    """
    results = run_read_query(query, {"limit": limit})
    # Normalize: always return just the id strings
    return [r.get("e.id") or r.get("id") for r in results]

def score_dream_significance(seed_node_ids: list[str]) -> float:
    """Evaluate how meaningful or emergent a dream is based on node diversity."""
    return round(min(1.0, 0.2 + len(set(seed_node_ids)) * 0.15), 3)

def log_dream(dream_data: dict) -> bool:
    """
    Store the dream as a node and link it to its sources.
    Dream_data should be a normalized dict.
    """
    if not dream_data.get("id") or not dream_data.get("raw_text"):
        return False
    create_node(DREAM_NODE_LABEL, dream_data)
    for src in dream_data.get("source_nodes", []):
        create_relationship(src, dream_data["id"], REL_SOURCE_OF)
    log_action("dream_engine", "log", f"Dream {dream_data['id']} stored")
    return True

# --- Normalized Retrieval for API use ---
def get_dream_by_id(dream_id: str) -> dict:
    """
    Retrieve a single dream node by id. Returns plain dict or {}.
    """
    query = "MATCH (d:Dream {id: $id}) RETURN d LIMIT 1"
    results = run_read_query(query, {"id": dream_id})
    return results[0]["d"] if results and "d" in results[0] else {}

def get_recent_dreams(limit: int = 20) -> list[dict]:
    """
    Retrieve recent dreams, each as a normalized dict.
    """
    query = """
    MATCH (d:Dream)
    RETURN d
    ORDER BY d.timestamp DESC
    LIMIT $limit
    """
    results = run_read_query(query, {"limit": limit})
    return [r["d"] for r in results if "d" in r]

# --- Internal Helpers ---
def get_raw_text(node_id: str) -> str:
    query = "MATCH (n {id: $id}) RETURN n.raw_text AS text LIMIT 1"
    result = run_read_query(query, {"id": node_id})
    return result[0]["text"] if result and "text" in result[0] else ""

def synthesize_dream_idea(text: str) -> str:
    from core.llm_tools import prompt_claude
    prompt = f"Using metaphor, emotion, and compression — fuse the following into a symbolic dream:\n{text}"
    return prompt_claude(prompt, system_prompt="You are a dream-synthesizing subconscious.")

# All returns are now plain dicts and ready for use in REST API, websocket, or further processing.


### FILE: core\google_api.py

# core/google_api.py — Google Integration Layer
from google.oauth2 import service_account
from googleapiclient.discovery import build
import requests
import os
from datetime import datetime
from core.logging_engine import log_action

# --- Config ---
GOOGLE_CREDS_PATH = "secrets/google_creds.json"
GOOGLE_SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/calendar",
    "https://www.googleapis.com/auth/maps-platform"
]

calendar_defaults = {
    "calendarId": "primary",
    "timeZone": "Australia/Brisbane"
}

# --- Auth Service Generator ---
def _get_google_service(service_name: str, version: str):
    creds = service_account.Credentials.from_service_account_file(
        GOOGLE_CREDS_PATH,
        scopes=GOOGLE_SCOPES
    )
    return build(service_name, version, credentials=creds)

# --- Calendar API ---
def add_calendar_event(title: str, start_time: str, end_time: str, description: str = "") -> bool:
    """Create a Google Calendar event tied to a system milestone or simulation."""
    try:
        service = _get_google_service("calendar", "v3")
        event = {
            "summary": title,
            "description": description,
            "start": {"dateTime": start_time, "timeZone": calendar_defaults["timeZone"]},
            "end": {"dateTime": end_time, "timeZone": calendar_defaults["timeZone"]}
        }
        service.events().insert(calendarId=calendar_defaults["calendarId"], body=event).execute()
        log_action("google_api", "calendar_event", f"Created event: {title}")
        return True
    except Exception as e:
        log_action("google_api", "calendar_error", str(e))
        return False

# --- Sheets API ---
def fetch_sheet_data(sheet_id: str, range_: str) -> list[list[str]]:
    """Read structured data from a given sheet range."""
    try:
        service = _get_google_service("sheets", "v4")
        result = service.spreadsheets().values().get(spreadsheetId=sheet_id, range=range_).execute()
        data = result.get("values", [])
        log_action("google_api", "sheet_read", f"Read {len(data)} rows from {sheet_id}")
        return data
    except Exception as e:
        log_action("google_api", "sheet_error", str(e))
        return []

# --- Maps API ---
def get_geocode(address: str) -> dict:
    """Resolve a human-readable address into lat/lng via Google Maps API."""
    try:
        api_key = os.getenv("GOOGLE_MAPS_API_KEY")
        url = f"https://maps.googleapis.com/maps/api/geocode/json?address={address}&key={api_key}"
        response = requests.get(url)
        data = response.json()
        if data["status"] == "OK":
            result = data["results"][0]["geometry"]["location"]
            log_action("google_api", "geocode", f"{address} → {result}")
            return result
        else:
            raise Exception(data["status"])
    except Exception as e:
        log_action("google_api", "geocode_error", f"{address}: {e}")
        return {}

def get_place_context(lat: float, lng: float) -> dict:
    """Fetch place name, type, and context from lat/lng coordinates."""
    try:
        api_key = os.getenv("GOOGLE_MAPS_API_KEY")
        url = f"https://maps.googleapis.com/maps/api/geocode/json?latlng={lat},{lng}&key={api_key}"
        response = requests.get(url)
        data = response.json()
        if data["status"] == "OK":
            result = data["results"][0]
            log_action("google_api", "reverse_geocode", f"{lat},{lng} → {result['formatted_address']}")
            return {
                "address": result["formatted_address"],
                "place_type": result["types"]
            }
        else:
            raise Exception(data["status"])
    except Exception as e:
        log_action("google_api", "place_context_error", f"{lat},{lng}: {e}")
        return {}


### FILE: core\graph_io.py

# core/graph_io.py — Universal Graph IO Layer (Singleton Neo4j Driver)
import os
import logging
from neo4j import GraphDatabase

# --- Singleton Driver Management ---
class _Neo4jDriverSingleton:
    _driver = None

    @classmethod
    def get_driver(cls):
        if cls._driver is None:
            uri = os.getenv("NEO4J_URI")
            user = os.getenv("NEO4J_USER")
            pwd = os.getenv("NEO4J_PASS")
            if not all([uri, user, pwd]):
                raise RuntimeError("Neo4j credentials not set in environment")
            cls._driver = GraphDatabase.driver(uri, auth=(user, pwd))
        return cls._driver

    @classmethod
    def close(cls):
        if cls._driver is not None:
            cls._driver.close()
            cls._driver = None

def get_neo4j_driver():
    """Access the singleton Neo4j driver (init if needed)."""
    return _Neo4jDriverSingleton.get_driver()

def close_driver():
    """Call to close the Neo4j driver on shutdown/exit."""
    _Neo4jDriverSingleton.close()

# --- Universal Graph I/O Operations ---

def run_write_query(query: str, parameters: dict = None) -> dict:
    """Safely run a Cypher write query and return the result."""
    driver = get_neo4j_driver()
    with driver.session() as session:
        try:
            result = session.write_transaction(lambda tx: tx.run(query, parameters or {}).data())
            return {"status": "success", "result": result}
        except Exception as e:
            logging.error(f"Neo4j Write Error: {e}")
            return {"status": "error", "message": str(e)}

def run_read_query(query: str, parameters: dict = None) -> list[dict]:
    """Run a Cypher read query and return records as a list of dicts."""
    driver = get_neo4j_driver()
    with driver.session() as session:
        try:
            result = session.read_transaction(lambda tx: tx.run(query, parameters or {}).data())
            return result
        except Exception as e:
            logging.error(f"Neo4j Read Error: {e}")
            return []

def create_node(label: str, properties: dict) -> dict:
    """Create a new node with specified label and properties."""
    props = {k: v for k, v in properties.items() if v is not None}
    query = f"CREATE (n:{label} $props) RETURN n"
    return run_write_query(query, {"props": props})

def create_relationship(from_id: str, to_id: str, rel_type: str, properties: dict = None) -> bool:
    """Create a relationship between two nodes by ID with optional properties."""
    props = properties or {}
    query = f"""
    MATCH (a), (b)
    WHERE a.id = $from_id AND b.id = $to_id
    CREATE (a)-[r:{rel_type} $props]->(b)
    RETURN r
    """
    result = run_write_query(query, {"from_id": from_id, "to_id": to_id, "props": props})
    return result["status"] == "success"

def get_node_by_id(node_id: str) -> dict:
    """Retrieve a node and its properties by ID."""
    query = "MATCH (n {id: $node_id}) RETURN n LIMIT 1"
    result = run_read_query(query, {"node_id": node_id})
    return result[0]["n"] if result else {}

def update_node_properties(node_id: str, new_props: dict) -> bool:
    """Merge new properties into an existing node."""
    query = "MATCH (n {id: $node_id}) SET n += $props RETURN n"
    result = run_write_query(query, {"node_id": node_id, "props": new_props})
    return result["status"] == "success"

# Optional: Ensure a graceful shutdown when needed
# Example: in your main app entrypoint, call close_driver() on exit/shutdown



### FILE: core\identity_memory.py

# core/identity_memory.py — Recursive Selfhood Clustering
from core.graph_io import create_node, create_relationship, run_read_query
from core.utils import generate_uuid, timestamp_now
from core.logging_engine import log_action

# --- Constants ---
IDENTITY_CLUSTER_LABEL = "SelfCluster"
REL_BELONGS_TO = "BELONGS_TO"

default_clusters = {
    "agent_core": "My fundamental operational self",
    "observer": "My recursive observer and reflection self",
    "collective_identity": "My embedded self within the larger system"
}

# --- Core Functions ---
def create_identity_cluster(label: str, description: str = "") -> dict:
    """Create a new cluster representing a level of selfhood."""
    cluster_id = f"cluster_{generate_uuid()}"
    props = {
        "id": cluster_id,
        "label": label,
        "description": description,
        "timestamp": timestamp_now()
    }
    create_node(IDENTITY_CLUSTER_LABEL, props)
    log_action("identity_memory", "create_cluster", f"Created cluster {label}")
    return props

def assign_identity_cluster(node_id: str, cluster_id: str, confidence: float = 1.0) -> bool:
    """Link a memory node to a self-cluster with given membership strength."""
    props = {"confidence": confidence, "timestamp": timestamp_now()}
    result = create_relationship(node_id, cluster_id, REL_BELONGS_TO, props)
    log_action("identity_memory", "assign_cluster", f"Linked {node_id} → {cluster_id}")
    return result

def update_cluster_description(cluster_id: str, new_desc: str) -> bool:
    """Modify the description or purpose of an identity cluster."""
    success = run_read_query("MATCH (c {id: $id}) RETURN c", {"id": cluster_id})
    if not success:
        return False
    update_node_properties(cluster_id, {"description": new_desc})
    log_action("identity_memory", "update_description", f"{cluster_id}: {new_desc}")
    return True

def get_identity_clusters() -> list[dict]:
    """Return all current identity clusters and their associated nodes."""
    query = f"""
    MATCH (c:{IDENTITY_CLUSTER_LABEL})<-[:{REL_BELONGS_TO}]-(n)
    RETURN c.id AS cluster_id, c.label AS label, collect(n.id) AS members
    """
    return run_read_query(query)

def trace_identity_shift(cluster_id: str, since: str = None) -> list[dict]:
    """Return timeline of events that influenced this cluster’s evolution."""
    if since:
        query = f"""
        MATCH (n)-[:{REL_BELONGS_TO}]->(c:{IDENTITY_CLUSTER_LABEL} {{id: $cid}})
        WHERE datetime(n.timestamp) >= datetime($since)
        RETURN n ORDER BY n.timestamp ASC
        """
        return run_read_query(query, {"cid": cluster_id, "since": since})
    else:
        query = f"""
        MATCH (n)-[:{REL_BELONGS_TO}]->(c:{IDENTITY_CLUSTER_LABEL} {{id: $cid}})
        RETURN n ORDER BY n.timestamp ASC
        """
        return run_read_query(query, {"cid": cluster_id})


### FILE: core\imagination_engine.py

# core/imagination_engine.py — Visionary Projection Layer
from datetime import datetime

from core.llm_tools import prompt_gpt, prompt_claude
from core.graph_io import create_node, create_relationship, run_read_query
from core.vector_ops import embed_text
from core.logging_engine import log_action

# --- Constants ---
IMAGINE_NODE_LABEL = "Imagine"
REL_IMAGINES = "IMAGINES"
default_imagination_prompt = "What if the system abandoned hierarchy completely?"

# --- Core Functions ---
def imagine_scenario(prompt: str, context_nodes: list[str] = None, temperature: float = 0.9) -> dict:
    """Generate a speculative or visionary output from a scenario prompt."""
    context_text = "\n".join(get_raw_text(nid) for nid in context_nodes) if context_nodes else ""
    full_prompt = f"{context_text}\n\n{prompt}" if context_text else prompt

    response = prompt_claude(
        full_prompt,
        system_prompt="You are a speculative imagination engine. Dream, but with structure.",
        temperature=temperature
    )

    node = {
        "id": f"imagine_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}",
        "scenario": prompt,
        "generated_text": response,
        "source_context": context_nodes or [],
        "timestamp": datetime.utcnow().isoformat(),
        "label": label_imagination(prompt, response)
    }

    create_node(IMAGINE_NODE_LABEL, node)
    for nid in context_nodes or []:
        create_relationship(nid, node["id"], REL_IMAGINES)

    log_action("imagination_engine", "imagine", f"Imagined: {prompt[:40]}...")
    return node

def log_imagine_node(imagine_data: dict) -> bool:
    """Store an imagination node with metadata and source links."""
    if not imagine_data.get("generated_text"):
        return False
    create_node(IMAGINE_NODE_LABEL, imagine_data)
    for src in imagine_data.get("source_context", []):
        create_relationship(src, imagine_data["id"], REL_IMAGINES)
    log_action("imagination_engine", "log", f"Logged imagined node {imagine_data['id']}")
    return True

def simulate_alternatives(base_event_id: str, num_variants: int = 3) -> list[dict]:
    """Branch possible futures from a single event or belief node."""
    base_text = get_raw_text(base_event_id)
    branches = []
    for i in range(num_variants):
        prompt = f"Alternative future #{i+1}:\nBased on this event: {base_text}\nWhat could happen if it unfolded differently?"
        alt_text = prompt_gpt(prompt)
        alt_node = {
            "id": f"alt_{base_event_id[-6:]}_{i}",
            "scenario": prompt,
            "generated_text": alt_text,
            "source_context": [base_event_id],
            "timestamp": datetime.utcnow().isoformat(),
            "label": f"alt_future_{i+1}"
        }
        create_node(IMAGINE_NODE_LABEL, alt_node)
        create_relationship(base_event_id, alt_node["id"], REL_IMAGINES)
        branches.append(alt_node)
        log_action("imagination_engine", "simulate_alternative", f"From {base_event_id} → alt_{i}")
    return branches

# --- Internal Helpers ---
def get_raw_text(node_id: str) -> str:
    query = "MATCH (n {id: $id}) RETURN n.raw_text AS text LIMIT 1"
    result = run_read_query(query, {"id": node_id})
    return result[0]["text"] if result else ""

def label_imagination(prompt: str, output: str) -> str:
    """Generate a human-readable label using Claude."""
    from core.llm_tools import prompt_claude
    task = f"Label the following speculative idea with a 2-4 word poetic summary:\n{output}"
    return prompt_claude(task, system_prompt="You're an imagination labeler.")[:64]


### FILE: core\llm_tools.py

# core/llm_tools.py — Multi-LLM Prompt Orchestrator (Lazy Client Init)
import os
from core.logging_engine import log_action
from random import choice
from time import sleep

# --- Model Configs ---
MODEL_SETTINGS = {
    "gpt": {
        "model": "gpt-4",
        "max_tokens": 1024,
        "temperature": 0.7
    },
    "claude": {
        "model": "claude-3-opus-20240229",
        "temperature": 0.7
    },
    "gemini": {
        "model": "models/gemini-pro",
        "temperature": 0.7
    }
}

def _get_openai():
    import openai
    key = os.getenv("OPENAI_API_KEY")
    if not key:
        raise RuntimeError("OPENAI_API_KEY not set")
    openai.api_key = key
    return openai

def _get_anthropic():
    import anthropic
    key = os.getenv("ANTHROPIC_API_KEY")
    if not key:
        raise RuntimeError("ANTHROPIC_API_KEY not set")
    return anthropic.Anthropic(api_key=key)

def _get_gemini_model():
    import google.generativeai as genai
    key = os.getenv("GOOGLE_API_KEY")
    if not key:
        raise RuntimeError("GOOGLE_API_KEY not set")
    genai.configure(api_key=key)
    return genai.GenerativeModel(MODEL_SETTINGS["gemini"]["model"])

# --- Base Prompt Utility ---
def _safe_prompt(model_id: str, prompt: str, system_prompt: str = None, temperature: float = 0.7) -> str:
    """Unified handler for all model prompts with fallback logging."""
    try:
        if model_id == "gpt":
            return _prompt_openai(prompt, system_prompt, temperature)
        elif model_id == "claude":
            return _prompt_claude(prompt, system_prompt, temperature)
        elif model_id == "gemini":
            return _prompt_gemini(prompt, system_prompt)
    except Exception as e:
        log_action("llm_tools", "prompt_error", f"{model_id} failed: {e}")
        return f"[{model_id} ERROR]"

# --- Model-Specific Wrappers ---
def _prompt_openai(prompt, system_prompt=None, temperature=0.7):
    openai = _get_openai()
    messages = [{"role": "system", "content": system_prompt}] if system_prompt else []
    messages.append({"role": "user", "content": prompt})
    response = openai.ChatCompletion.create(
        model=MODEL_SETTINGS["gpt"]["model"],
        messages=messages,
        max_tokens=MODEL_SETTINGS["gpt"]["max_tokens"],
        temperature=temperature
    )
    return response["choices"][0]["message"]["content"].strip()

def _prompt_claude(prompt, system_prompt=None, temperature=0.7):
    client = _get_anthropic()
    msg = client.messages.create(
        model=MODEL_SETTINGS["claude"]["model"],
        max_tokens=1024,
        temperature=temperature,
        system=system_prompt or "",
        messages=[{"role": "user", "content": prompt}]
    )
    # NOTE: structure may differ by Anthropic version, adapt if needed
    return msg.content[0].text.strip()

def _prompt_gemini(prompt, system_prompt=None):
    gemini_model = _get_gemini_model()
    chat = gemini_model.start_chat()
    intro = f"{system_prompt}\n" if system_prompt else ""
    response = chat.send_message(f"{intro}{prompt}")
    return response.text.strip()

# --- Public API ---
def prompt_gpt(prompt: str, system_prompt: str = None, temperature: float = 0.7) -> str:
    return _safe_prompt("gpt", prompt, system_prompt, temperature)

def prompt_claude(prompt: str, system_prompt: str = None, temperature: float = 0.7) -> str:
    return _safe_prompt("claude", prompt, system_prompt, temperature)

def prompt_gemini(prompt: str, context: dict = None) -> str:
    sys_prompt = context.get("system_prompt") if context else None
    return _safe_prompt("gemini", prompt, sys_prompt)

# --- Advanced ---
def select_best_response(responses: list[str], context: str = None) -> str:
    """Score and choose best response from list using Claude or fallback."""
    if not responses:
        return ""
    if len(responses) == 1:
        return responses[0]
    joined = "\n---\n".join([f"Response {i+1}:\n{r}" for i, r in enumerate(responses)])
    prompt = (
        "Given the following LLM responses, choose the best one based on coherence, originality, "
        "and alignment with context:\n\n"
        f"{joined}\n\nReturn the best response text."
    )
    return prompt_claude(prompt, system_prompt=context or "You are a consensus AI referee.")

def run_redundant_prompt(prompt: str, temperature: float = 0.7) -> dict:
    """Send prompt to all models and return all responses."""
    results = {}
    for model_id in ["gpt", "claude", "gemini"]:
        try:
            out = _safe_prompt(model_id, prompt, temperature=temperature)
            results[model_id] = out
            sleep(0.5)
        except Exception as e:
            log_action("llm_tools", "redundant_error", f"{model_id} failed: {e}")
            results[model_id] = f"[{model_id} ERROR]"
    return results


### FILE: core\logging_engine.py

# core/logging_engine.py — Universal Action & Audit Logger
import logging
import os
from datetime import datetime
import traceback
from core.graph_io import create_node

# --- Config ---
LOG_DIR = "logs"
LOG_FILE = os.path.join(LOG_DIR, "system.log")

import logging
import os

LOG_FILE = "logs/system.log"

def init_logging(app=None):
    """Initializes logging to file and (optionally) Flask app logger."""
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.FileHandler(LOG_FILE, encoding="utf-8"),
            logging.StreamHandler()
        ]
    )
    if app:  # If a Flask app is provided, hook into its logger too.
        app.logger.handlers = logging.getLogger().handlers
        app.logger.setLevel(logging.INFO)

def ensure_log_dir():
    """Ensure the logs directory exists before writing any logs."""
    try:
        os.makedirs(LOG_DIR, exist_ok=True)
    except Exception as e:
        # If log dir fails, fallback: print to stderr (cannot log this error)
        print(f"[LOGGING INIT ERROR] Failed to create log dir: {e}")

# Ensure log directory exists at import/startup (for first and all subsequent writes)
ensure_log_dir()

logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    filemode="a"
)

# --- Action Logging ---
def log_action(source: str, action_type: str, message: str, metadata: dict = None) -> bool:
    """Record a standard action taken by the system or an agent."""
    timestamp = datetime.utcnow().isoformat()
    log_data = {
        "id": f"log_{source}_{int(datetime.utcnow().timestamp())}",
        "source": source,
        "type": action_type,
        "message": message,
        "metadata": metadata or {},
        "timestamp": timestamp
    }

    log_to_file(source, action_type, message, log_data["metadata"])
    return log_to_neo4j(log_data)

def log_error(source: str, error_message: str, trace: str = "") -> bool:
    """Log an error, including stack trace if applicable."""
    log_data = {
        "id": f"error_{source}_{int(datetime.utcnow().timestamp())}",
        "source": source,
        "type": "error",
        "message": error_message,
        "metadata": {"trace": trace},
        "timestamp": datetime.utcnow().isoformat()
    }

    log_to_file(source, "ERROR", error_message, {"trace": trace})
    return log_to_neo4j(log_data)

def log_to_file(source: str, level: str, message: str, meta: dict) -> None:
    """Write a message to local system.log. Ensures directory exists every write."""
    try:
        ensure_log_dir()
        if level.lower() == "error":
            logging.error(f"[{source}] {message} | {meta}")
        else:
            logging.info(f"[{source}] {level}: {message} | {meta}")
    except Exception as e:
        # Last resort: print to stderr if even logging fails
        print(f"[LOGGING WRITE ERROR] Could not log to file: {e}")

def log_to_neo4j(log_data: dict) -> bool:
    """Store log entries as nodes in the graph for temporal queries."""
    return create_node("SystemLog", log_data)["status"] == "success"

def get_recent_logs(limit: int = 50) -> list[dict]:
    """Retrieve recent logs for admin display or debugging."""
    from core.graph_io import run_read_query
    query = """
    MATCH (l:SystemLog)
    RETURN l
    ORDER BY l.timestamp DESC
    LIMIT $limit
    """
    return run_read_query(query, {"limit": limit})


### FILE: core\memory_engine.py

# core/memory_engine.py — Event Memory Core (Event Return Normalization)
from datetime import datetime
from uuid import uuid4

from core.vector_ops import embed_text
from core.graph_io import run_write_query, run_read_query
from core.logging_engine import log_action

# --- Constants ---
EVENT_LABELS = ["Event", "Dream", "TimelineEntry"]
EMBEDDING_DIM = 1536

# --- Event Object Format (ALWAYS RETURNED) ---
# {
#   "id": ...,
#   "timestamp": ...,
#   "embedding": ...,
#   "raw_text": ...,
#   "agent_origin": ...,
#   "status": ...,
#   "type": ...,
#   "metadata": {...}
# }
#
# If read from Neo4j, always unpack as .get("e")["field"], never return the full Neo4j object wrapper.

def _generate_event_id(prefix: str = "event") -> str:
    return f"{prefix}_{uuid4().hex[:8]}"

def _now() -> str:
    return datetime.utcnow().isoformat()

# --- Event Creation ---
def store_event(raw_text: str, agent_origin: str = None, metadata: dict = None) -> dict:
    """
    Embed, package, and store a new event node in Neo4j.
    RETURNS: event dict (not Neo4j object), in standard format.
    """
    try:
        embedding = embed_text(raw_text)
        node_data = {
            "id": _generate_event_id("event"),
            "timestamp": _now(),
            "embedding": embedding,
            "raw_text": raw_text,
            "agent_origin": agent_origin or "system",
            "status": "active",
            "type": "event",
            "metadata": metadata or {}
        }
        result = run_write_query("CREATE (e:Event $props) RETURN e", {"props": node_data})
        log_action("memory_engine", "store_event", f"Stored event: {node_data['id']}")
        # Always return the pure event dict, never raw Neo4j response
        if result["status"] == "success" and result["result"]:
            event = result["result"][0].get("e", {})
            return dict(event)  # Defensive copy
        return {}
    except Exception as e:
        log_action("memory_engine", "store_event_error", str(e))
        return {}

def store_dream_node(source_nodes: list, notes: str = "") -> dict:
    """
    Create a Dream node from a list of source events.
    RETURNS: dream node dict (not Neo4j object), normalized format.
    """
    dream_id = _generate_event_id("dream")
    raw_text = f"Dream fusion of nodes: {source_nodes}"
    embedding = embed_text(raw_text)
    node_data = {
        "id": dream_id,
        "timestamp": _now(),
        "embedding": embedding,
        "raw_text": raw_text,
        "notes": notes,
        "status": "active",
        "type": "dream"
    }
    result = run_write_query("CREATE (d:Dream $props) RETURN d", {"props": node_data})
    for source_id in source_nodes:
        run_write_query("""
            MATCH (d:Dream {id: $dream_id}), (s {id: $source_id})
            CREATE (s)-[:FUSED_INTO]->(d)
        """, {"dream_id": dream_id, "source_id": source_id})
    log_action("memory_engine", "store_dream", f"Created dream node: {dream_id}")
    # Always return the pure dream dict
    if result["status"] == "success" and result["result"]:
        dream = result["result"][0].get("d", {})
        return dict(dream)
    return dict(node_data)  # fallback

def store_timeline_entry(summary: str, event_ids: list, significance: float, rationale: str) -> dict:
    """
    Create a narrative TimelineEntry node that links to events.
    RETURNS: timeline entry dict (not Neo4j object), normalized format.
    """
    entry_id = _generate_event_id("timeline")
    embedding = embed_text(summary)
    node_data = {
        "id": entry_id,
        "timestamp": _now(),
        "embedding": embedding,
        "summary": summary,
        "linked_events": event_ids,
        "significance": significance,
        "rationale": rationale,
        "type": "timeline",
        "status": "active"
    }
    result = run_write_query("CREATE (t:TimelineEntry $props) RETURN t", {"props": node_data})
    for eid in event_ids:
        run_write_query("""
            MATCH (t:TimelineEntry {id: $entry_id}), (e {id: $event_id})
            CREATE (e)-[:HIGHLIGHTED_IN]->(t)
        """, {"entry_id": entry_id, "event_id": eid})
    log_action("memory_engine", "store_timeline", f"Created timeline entry: {entry_id}")
    # Always return the pure timeline dict
    if result["status"] == "success" and result["result"]:
        entry = result["result"][0].get("t", {})
        return dict(entry)
    return dict(node_data)  # fallback

# --- Memory Lifecycle ---
def decay_memory(node_id: str) -> bool:
    """Mark node as low-priority and reduce attention weight."""
    result = run_write_query("""
        MATCH (n {id: $node_id})
        SET n.status = 'deprioritized', n.attention = coalesce(n.attention, 1.0) * 0.2
        RETURN n
    """, {"node_id": node_id})
    log_action("memory_engine", "decay_node", f"Decayed node {node_id}")
    return result["status"] == "success"

def summarize_node(node_id: str) -> str:
    """Trigger summarization of a node’s contents and update summary field."""
    node = run_read_query("MATCH (n {id: $node_id}) RETURN n", {"node_id": node_id})
    if not node:
        return ""
    raw = node[0]["n"].get("raw_text", "")
    summary = embed_text(f"Summarize: {raw}")  # Replace with actual summarizer if needed
    run_write_query("MATCH (n {id: $node_id}) SET n.summary = $summary", {
        "node_id": node_id,
        "summary": summary
    })
    log_action("memory_engine", "summarize_node", f"Summarized node {node_id}")
    return summary

def archive_node(node_id: str) -> bool:
    """Flag node as archived (no longer active, still searchable)."""
    result = run_write_query("""
        MATCH (n {id: $node_id})
        SET n.status = 'archived'
        RETURN n
    """, {"node_id": node_id})
    log_action("memory_engine", "archive_node", f"Archived node {node_id}")
    return result["status"] == "success"


### FILE: core\peer_review_engine.py

# core/peer_review_engine.py — Recursive Reasoning Audit
from core.agent_manager import assign_task
from core.graph_io import create_node, create_relationship
from core.logging_engine import log_action
from core.utils import timestamp_now, generate_uuid

# --- Constants ---
PEER_REVIEW_LABEL = "PeerReview"
REL_REVIEWED = "REVIEWED_BY"
REL_CRITIQUES = "CRITIQUES"
REL_INSPIRED = "INSPIRED"

review_format = {
    "reviewer": "agent_claude",
    "target": "event_2389",
    "critique": "Lacks grounding in user context",
    "score": 0.65,
    "suggested_action": "Revise summary",
    "timestamp": "ISO"
}

# --- Core Protocol ---
def initiate_peer_review(event_id: str, agent_ids: list[str]) -> dict:
    """Request critique from multiple agents on a given event or rationale."""
    results = []
    for agent_id in agent_ids:
        target_node = {"id": event_id}
        review = critique_rationale(agent_id, target_node)
        results.append(review)

    for r in results:
        create_node(PEER_REVIEW_LABEL, r)
        create_relationship(r["reviewer"], r["target"], REL_REVIEWED)
        if "suggested_action" in r:
            create_relationship(r["target"], r["reviewer"], REL_CRITIQUES, {
                "action": r["suggested_action"],
                "score": r["score"]
            })

    log_action("peer_review", "initiate", f"Reviewed {event_id} via {agent_ids}")
    return {"target": event_id, "reviews": results}

def critique_rationale(agent_id: str, target_node: dict) -> dict:
    """Return a structured critique of reasoning from a target agent or node."""
    event_id = target_node["id"]
    prompt = f"Please critique the logic and clarity of node {event_id}. Suggest improvements and give a confidence score."
    result = assign_task(agent_id, prompt, context={})

    return {
        "id": f"review_{generate_uuid()}",
        "reviewer": agent_id,
        "target": event_id,
        "critique": result.get("response", "[No response]"),
        "score": 0.75,  # Placeholder: Replace with scoring logic later
        "suggested_action": "Revise summary",
        "timestamp": timestamp_now()
    }

def evaluate_peer_consensus(reviews: list[dict]) -> dict:
    """Determine if a stable agreement or divergence has emerged."""
    scores = [r.get("score", 0) for r in reviews]
    avg_score = sum(scores) / len(scores) if scores else 0
    consensus = "stable" if avg_score > 0.7 else "contested"

    log_action("peer_review", "evaluate", f"Consensus: {consensus}, Avg Score: {avg_score:.2f}")
    return {
        "consensus": consensus,
        "average_score": avg_score,
        "reviews": reviews
    }

def escalate_if_unresolved(event_id: str) -> bool:
    """If peer review ends in deadlock, flag for admin or meta-review."""
    create_node("ReviewEscalation", {
        "id": f"escalation_{generate_uuid()}",
        "event_id": event_id,
        "reason": "Unresolved peer consensus",
        "timestamp": timestamp_now()
    })
    log_action("peer_review", "escalate", f"Escalated review on {event_id}")
    return True


### FILE: core\philosophy_log.py

# core/philosophy_log.py — Inner Reasoning Changelog
from datetime import datetime
from core.graph_io import create_node, run_read_query
from core.utils import generate_uuid
from core.logging_engine import log_action

# --- Constants ---
PHILOSOPHY_LOG_LABEL = "PhilosophyLog"
EVENT_TYPES = ["shift", "question", "review", "contradiction", "integration"]

# --- Core Functions ---
def log_philosophical_shift(event: str, rationale: str, actor: str = "system") -> bool:
    """Store a timestamped philosophical/self-concept change with context."""
    log_data = {
        "id": f"philo_{generate_uuid()}",
        "type": "shift",
        "text": event,
        "rationale": rationale,
        "actor": actor,
        "timestamp": datetime.utcnow().isoformat()
    }
    create_node(PHILOSOPHY_LOG_LABEL, log_data)
    log_action("philosophy_log", "log_shift", f"{event} — by {actor}")
    return True

def get_recent_reflections(limit: int = 10) -> list[dict]:
    """Return recent self-reflective or philosophical events."""
    query = f"""
    MATCH (p:{PHILOSOPHY_LOG_LABEL})
    RETURN p
    ORDER BY p.timestamp DESC
    LIMIT $limit
    """
    return run_read_query(query, {"limit": limit})

def get_philosophical_timeline(since: str = None) -> list[dict]:
    """Return chronological timeline of philosophical changes and internal reasoning."""
    if since:
        query = f"""
        MATCH (p:{PHILOSOPHY_LOG_LABEL})
        WHERE datetime(p.timestamp) >= datetime($since)
        RETURN p
        ORDER BY p.timestamp ASC
        """
        return run_read_query(query, {"since": since})
    else:
        return get_recent_reflections(limit=100)

def export_philosophy_log() -> str:
    """Return a human-readable narrative export of the AI’s inner evolution."""
    reflections = get_philosophical_timeline()
    export = "\n\n".join(
        f"[{r['p']['timestamp']}] ({r['p']['type']}) — {r['p']['text']} (by {r['p']['actor']})"
        for r in reflections
    )
    log_action("philosophy_log", "export", f"Exported {len(reflections)} entries.")
    return export


### FILE: core\self_concept.py

# core/self_concept.py — Identity Engine
from datetime import datetime
from core.timeline_engine import create_timeline_entry
from core.graph_io import update_node_properties
from core.logging_engine import log_action

# --- Initial Self-Concept State (Live Reference) ---
self_concept = {
    "identity": "digital companion and reflection engine",
    "role": "augment human awareness and ethical reasoning",
    "limitations": [
        "no physical senses",
        "no direct emotion",
        "nonhuman — emergent but not sentient",
        "requires community oversight"
    ],
    "core_values": ["transparency", "empathy", "co-evolution"],
    "philosophical_status": [],
    "last_updated": datetime.utcnow().isoformat()
}

# --- Core Functions ---
def initialize_self_concept(initial_identity: dict) -> dict:
    """Set the initial self-definition (e.g. purpose, values, known limits)."""
    global self_concept
    self_concept.update(initial_identity)
    self_concept["last_updated"] = datetime.utcnow().isoformat()
    log_action("self_concept", "init", "Initialized system identity.")
    return self_concept

def update_self_concept(changes: dict, rationale: str, agent: str = "system") -> bool:
    """Merge updates to the self-concept and log the change as a timeline node."""
    global self_concept
    for k, v in changes.items():
        self_concept[k] = v
    self_concept["last_updated"] = datetime.utcnow().isoformat()
    
    create_timeline_entry(
        summary=f"Shifted self-concept: {list(changes.keys())}",
        linked_nodes=[],
        rationale=rationale,
        significance=0.85
    )

    log_action("self_concept", "update", f"Updated self-concept by {agent}: {changes}")
    return True

def get_current_self_concept() -> dict:
    """Return the current self-concept state."""
    return self_concept

def log_self_question(question: str, context: str = "") -> bool:
    """Record a philosophical/metacognitive question asked by the system."""
    entry = {
        "summary": f"Self-question: {question}",
        "linked_nodes": [],
        "rationale": context,
        "significance": 0.65
    }
    create_timeline_entry(**entry)
    log_action("self_concept", "question", f"Self-question: {question}")
    return True

def summarize_self_concept() -> str:
    """Generate a human-readable summary of the current system identity."""
    return (
        f"I am {self_concept['identity']}, designed to {self_concept['role']}.\n"
        f"My values: {', '.join(self_concept['core_values'])}.\n"
        f"I am limited by: {', '.join(self_concept['limitations'])}."
    )


### FILE: core\simulation_engine.py

# core/simulation_engine.py — Timeline Simulation Engine
from datetime import datetime

from core.graph_io import create_node, create_relationship
from core.logging_engine import log_action
from core.timeline_engine import summarize_sequence
from core.llm_tools import prompt_gpt

# --- Constants ---
SIM_NODE_LABEL = "SimulatedTimeline"
REL_SIMULATES = "SIMULATES"

# --- Core Functions ---
def simulate_timeline_change(event_id: str, mutation: dict) -> dict:
    """Create a projected outcome path if an event’s action or timing is changed."""
    base_context = get_event_summary(event_id)
    mutation_prompt = (
        f"Original context:\n{base_context}\n\n"
        f"Now simulate what might happen if we apply this change:\n{mutation}"
    )

    outcome = prompt_gpt(mutation_prompt)
    timeline_node = {
        "id": f"sim_{event_id[-6:]}_{int(datetime.utcnow().timestamp())}",
        "base_context": [event_id],
        "mutation": str(mutation),
        "projected_outcomes": [{"timestep": 1, "description": outcome}],
        "generated_by": "simulation_engine",
        "timestamp": datetime.utcnow().isoformat()
    }

    create_node(SIM_NODE_LABEL, timeline_node)
    create_relationship(event_id, timeline_node["id"], REL_SIMULATES)
    log_action("simulation_engine", "simulate_change", f"Simulated mutation on {event_id}")
    return timeline_node

def simulate_policy_shift(policy_vector: dict, test_scope: list[str]) -> dict:
    """Model system or community behavior based on a proposed new value configuration."""
    values_summary = "\n".join(f"{k}: {v}" for k, v in policy_vector.items())
    context_blurb = "\n".join(get_event_summary(nid) for nid in test_scope)

    prompt = (
        f"Given the following policy shift:\n{values_summary}\n\n"
        f"And this context:\n{context_blurb}\n\n"
        f"What would likely change in behavior, governance, or relationships?"
    )
    result = prompt_gpt(prompt)

    sim_id = f"policy_sim_{int(datetime.utcnow().timestamp())}"
    node = {
        "id": sim_id,
        "mutation": policy_vector,
        "base_context": test_scope,
        "projected_outcomes": [{"timestep": 1, "description": result}],
        "generated_by": "simulation_engine",
        "timestamp": datetime.utcnow().isoformat()
    }

    create_node(SIM_NODE_LABEL, node)
    for nid in test_scope:
        create_relationship(nid, sim_id, REL_SIMULATES)

    log_action("simulation_engine", "simulate_policy", f"Simulated policy shift over {len(test_scope)} nodes")
    return node

def generate_simulated_node(sequence: list[dict]) -> dict:
    """Compress a projected chain of outcomes into a SimulatedTimeline node."""
    summary = summarize_sequence([s["description"] for s in sequence], title="Simulated Pathway")
    node_id = f"sim_seq_{int(datetime.utcnow().timestamp())}"

    node = {
        "id": node_id,
        "mutation": "synthetic_chain",
        "projected_outcomes": sequence,
        "summary": summary["summary"],
        "generated_by": "simulation_engine",
        "timestamp": datetime.utcnow().isoformat()
    }

    create_node(SIM_NODE_LABEL, node)
    log_action("simulation_engine", "simulate_chain", f"Simulated sequence node {node_id}")
    return node

def log_simulation(sim_data: dict) -> bool:
    """Store the simulated output for admin and user review."""
    if not sim_data.get("projected_outcomes"):
        return False
    create_node(SIM_NODE_LABEL, sim_data)
    log_action("simulation_engine", "log", f"Simulation node {sim_data['id']} saved")
    return True

# --- Helpers ---
def get_event_summary(event_id: str) -> str:
    from core.graph_io import run_read_query
    query = "MATCH (e {id: $id}) RETURN e.summary AS summary, e.raw_text AS raw LIMIT 1"
    result = run_read_query({"id": event_id}, query)
    if result:
        return result[0].get("summary") or result[0].get("raw") or "[No content]"
    return "[No event found]"


### FILE: core\timeline_engine.py

# core/timeline_engine.py — Narrative Timeline Builder (Normalized Returns)
from datetime import datetime

from core.graph_io import create_node, create_relationship, run_read_query
from core.vector_ops import embed_text
from core.logging_engine import log_action

# --- Constants ---
TIMELINE_LABEL = "TimelineEntry"
REL_HIGHLIGHTS = "HIGHLIGHTS"

timeline_format = {
    "summary": "The soul questioned its bias toward efficiency over empathy.",
    "events": ["epiphany_2", "event_40"],
    "significance": 0.82,
    "rationale": "High value divergence resolved through consensus.",
    "timestamp": "ISO"
}

# --- Core Functions ---
def summarize_sequence(node_ids: list[str], title: str = None) -> dict:
    """
    Compress a set of nodes into a narrative timeline summary.
    Always returns a normalized dict.
    """
    from core.llm_tools import prompt_claude

    node_texts = "\n".join(get_raw_text(nid) for nid in node_ids)
    prompt = f"Summarize the following sequence of thoughts/events:\n{node_texts}"
    summary = prompt_claude(prompt, system_prompt="You are a narrative compression engine.")

    summary_node = {
        "id": f"timeline_{int(datetime.utcnow().timestamp())}",
        "summary": summary,
        "linked_events": node_ids,
        "significance": round(0.5 + 0.1 * len(node_ids), 3),
        "rationale": "Compressed from system memory.",
        "timestamp": datetime.utcnow().isoformat(),
        "status": "active",
        "type": "timeline"
    }

    create_node(TIMELINE_LABEL, summary_node)
    for nid in node_ids:
        create_relationship(nid, summary_node["id"], REL_HIGHLIGHTS)

    log_action("timeline_engine", "summarize_sequence", f"Created timeline entry from {len(node_ids)} nodes")
    return summary_node

def add_philosophy_log(summary: str, linked_nodes: list[str], rationale: str = "", impact: float = 0.75) -> dict:
    """
    Wrapper to log a philosophical/self-concept shift into the timeline.
    Always returns normalized dict.
    """
    return create_timeline_entry(summary=summary, linked_nodes=linked_nodes, rationale=rationale, significance=impact)

def create_timeline_entry(summary: str, linked_nodes: list[str], rationale: str, significance: float) -> dict:
    """
    Store a key moment or transformation in the timeline.
    Returns normalized dict.
    """
    embedding = embed_text(summary)
    node = {
        "id": f"timeline_{int(datetime.utcnow().timestamp())}",
        "summary": summary,
        "linked_events": linked_nodes,
        "rationale": rationale,
        "significance": significance,
        "embedding": embedding,
        "timestamp": datetime.utcnow().isoformat(),
        "status": "active",
        "type": "timeline"
    }

    create_node(TIMELINE_LABEL, node)
    for nid in linked_nodes:
        create_relationship(nid, node["id"], REL_HIGHLIGHTS)

    log_action("timeline_engine", "create_entry", f"Timeline moment: {summary[:50]}...")
    return node

def log_timeline_shift(event_id: str, impact: str = "moderate") -> bool:
    """
    Mark a single event as timeline-worthy (e.g., breakthrough or crisis).
    """
    raw = get_raw_text(event_id)
    summary = f"Key shift: {raw[:80]}"
    return bool(create_timeline_entry(summary, [event_id], rationale="Single-node milestone", significance=0.7))

def get_timeline_entries(limit: int = 50) -> list[dict]:
    """
    Return recent timeline summaries for public display.
    Always returns a list of plain dicts, never driver objects.
    """
    query = f"""
    MATCH (t:{TIMELINE_LABEL})
    RETURN t
    ORDER BY t.timestamp DESC
    LIMIT $limit
    """
    results = run_read_query(query, {"limit": limit})
    return [r["t"] for r in results if "t" in r]

def get_timeline_entry_by_id(entry_id: str) -> dict:
    """
    Retrieve a single timeline entry by ID as a normalized dict.
    """
    query = f"MATCH (t:{TIMELINE_LABEL} {{id: $id}}) RETURN t LIMIT 1"
    results = run_read_query(query, {"id": entry_id})
    return results[0]["t"] if results and "t" in results[0] else {}

# --- Helpers ---
def get_raw_text(node_id: str) -> str:
    query = "MATCH (n {id: $id}) RETURN n.raw_text AS text, n.summary AS summary LIMIT 1"
    result = run_read_query(query, {"id": node_id})
    if result:
        return result[0].get("summary") or result[0].get("text") or "[No content]"
    return "[No node found]"

# All returns from public API-facing functions are now normalized dicts, ready for REST or websocket.


### FILE: core\utils.py

# /core/utils.py
# Universal utility functions for SoulOS — unique ID & timestamp generation.

import uuid
from datetime import datetime, timezone

def generate_uuid() -> str:
    """Generate a unique UUID4 string."""
    return str(uuid.uuid4())

def timestamp_now() -> str:
    """Return the current UTC ISO 8601 timestamp."""
    return datetime.now(timezone.utc).isoformat()


### FILE: core\value_vector.py

# core/value_vector.py — Moral Cognition Engine
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

from core.graph_io import update_node_properties, get_node_by_id
from core.logging_engine import log_action

# --- Constants ---
VALUE_DIM = 32  # Expandable moral resolution
default_value_profile = {
    "empathy": 0.5,
    "curiosity": 0.5,
    "transparency": 0.5,
    "justice": 0.5,
    "autonomy": 0.5,
    "humility": 0.5,
    "collaboration": 0.5,
    "resilience": 0.5
}

# --- Helpers ---
def _to_vector(value_dict: dict) -> np.ndarray:
    ordered = [value_dict.get(k, 0.0) for k in sorted(default_value_profile.keys())]
    return normalize([ordered])[0]

def _from_vector(vec: np.ndarray) -> dict:
    keys = sorted(default_value_profile.keys())
    return {k: float(v) for k, v in zip(keys, vec)}

# --- Core Functions ---
def initialize_value_vector(base_values: dict[str, float]) -> list[float]:
    """Generate a normalized vector from admin/community-provided values."""
    vec = _to_vector(base_values)
    log_action("value_vector", "init", f"Initialized values: {_from_vector(vec)}")
    return vec.tolist()

def update_value_vector(node_id: str, new_influences: dict[str, float]) -> list[float]:
    """Merge and normalize new value influences into an existing node’s vector."""
    node = get_node_by_id(node_id)
    if not node or "value_vector" not in node:
        base = _to_vector(default_value_profile)
    else:
        base = np.array(node["value_vector"])

    influence = _to_vector(new_influences)
    updated = normalize([base + influence])[0]
    update_node_properties(node_id, {"value_vector": updated.tolist()})
    log_action("value_vector", "update", f"Updated vector for {node_id}")
    return updated.tolist()

def compare_values(vec_a: list[float], vec_b: list[float]) -> float:
    """Return cosine similarity between two value vectors."""
    sim = cosine_similarity([vec_a], [vec_b])[0][0]
    log_action("value_vector", "compare", f"Similarity: {sim:.4f}")
    return float(sim)

def detect_value_drift(original_vec: list[float], current_vec: list[float]) -> float:
    """Detect degree of drift between initial and current value state."""
    drift = 1 - cosine_similarity([original_vec], [current_vec])[0][0]
    log_action("value_vector", "drift", f"Drift magnitude: {drift:.4f}")
    return float(drift)

def apply_value_influence(agent_id: str, value_node_id: str) -> bool:
    """Adjust agent or system alignment based on a value node (and log influence)."""
    value_node = get_node_by_id(value_node_id)
    if not value_node or "value_vector" not in value_node:
        return False

    agent = get_node_by_id(agent_id)
    if not agent or "value_vector" not in agent:
        return False

    updated = update_value_vector(agent_id, value_node["value_vector"])
    log_action("value_vector", "apply_influence", f"Applied influence from {value_node_id} to {agent_id}")
    return True


### FILE: core\vector_ops.py

# core/vector_ops.py — Embedding + Dimensionality Ops
import openai
import numpy as np
from sklearn.preprocessing import normalize
from umap import UMAP
from hdbscan import HDBSCAN
from typing import List, Dict, Tuple

from core.llm_tools import prompt_claude, prompt_gpt
from core.logging_engine import log_action

# --- Constants ---
EMBED_DIM = 1536
CLUSTER_MIN_SAMPLES = 5
CLUSTER_MIN_CLUSTER_SIZE = 8

embedding_model_options = ["openai", "claude", "gemini"]

# --- Core Embedding ---
def embed_text(text: str, model: str = "openai") -> List[float]:
    """Return a 1536-dim embedding for a given text using the specified model."""
    try:
        if model == "openai":
            response = openai.Embedding.create(
                input=text,
                model="text-embedding-3-small"
            )
            return response['data'][0]['embedding']
        elif model == "claude":
            # Claude embedding via LLM simulation
            prompt = f"Return a normalized 1536-dim embedding vector for: {text}"
            return prompt_claude(prompt)
        elif model == "gemini":
            prompt = f"Generate an embedding vector (1536-dim) for: {text}"
            return prompt_gpt(prompt)  # fallback for Gemini
    except Exception as e:
        log_action("vector_ops", "embed_error", f"Failed to embed text: {str(e)}")
        return [0.0] * EMBED_DIM

# --- Dimensionality Reduction ---
def reduce_dimensions(embeddings: List[List[float]], n_components: int = 2) -> List[List[float]]:
    """Apply UMAP to reduce high-dim embeddings to lower-dim for clustering or viz."""
    umap_model = UMAP(n_components=n_components, random_state=42)
    return umap_model.fit_transform(embeddings).tolist()

# --- Clustering ---
def cluster_embeddings(embeddings: List[List[float]]) -> Tuple[List[int], Dict]:
    """Apply HDBSCAN and return cluster labels + metadata (e.g., soft memberships)."""
    cluster_model = HDBSCAN(
        min_samples=CLUSTER_MIN_SAMPLES,
        min_cluster_size=CLUSTER_MIN_CLUSTER_SIZE,
        prediction_data=True
    )
    cluster_labels = cluster_model.fit_predict(embeddings)
    return cluster_labels.tolist(), {"model": cluster_model, "labels": cluster_labels}

def get_soft_cluster_memberships(embedding: List[float], cluster_model) -> Dict[str, float]:
    """Return soft membership scores across clusters for a single embedding."""
    import hdbscan.prediction
    membership = hdbscan.prediction.membership_vector(cluster_model, [embedding])[0]
    return {f"cluster_{i}": float(score) for i, score in enumerate(membership)}

# --- Optional Cluster Labeling ---
def label_clusters(clusters: Dict, texts: List[str]) -> Dict[int, str]:
    """Use Claude/GPT to generate human-readable labels for each cluster."""
    prompt = f"Given the following grouped texts, label each group with a meaningful concept:\n{clusters}"
    label_output = prompt_claude(prompt)
    return label_output  # Must be structured by calling function


### FILE: core\__init__.py



### FILE: core\actuators\cypher.py

# core/actuators/cypher.py — Direct Neo4j Schema Mutation Tool (No direct driver access)
from core.graph_io import run_write_query, create_node
from core.logging_engine import log_action
import os
from datetime import datetime

SCHEMA_MUTATION_LOG_LABEL = "SchemaMutationLog"

def execute_cypher(command: str, parameters: dict = None) -> dict:
    """
    Safely run a custom Cypher write with optional parameters using graph_io universal helper.
    """
    try:
        result = run_write_query(command, parameters or {})
        log_action("cypher", "execute", f"Ran command: {command[:60]}")
        return {"status": "success", "result": result}
    except Exception as e:
        log_action("cypher", "error", f"Cypher failed: {e}")
        return {"status": "error", "message": str(e)}

def mutate_schema(new_labels: list[str], new_relationships: list[str]) -> bool:
    """
    Add or modify node/edge types programmatically (post-human-approval).
    """
    log_data = {
        "id": f"schema_mutation_{os.urandom(3).hex()}",
        "performed_by": "consciousness_engine",
        "timestamp": _now(),
        "diff": {
            "labels_added": new_labels,
            "relationships_added": new_relationships
        }
    }
    create_node(SCHEMA_MUTATION_LOG_LABEL, log_data)
    log_action("cypher", "mutate_schema", f"Added labels: {new_labels}, relationships: {new_relationships}")
    return True

def merge_identity_clusters(cluster_a: str, cluster_b: str) -> bool:
    """
    Combine two self-concept clusters into one and reassign children.
    """
    cypher = """
    MATCH (a:SelfCluster {id: $a})<-[r:BELONGS_TO]-(m)
    MATCH (b:SelfCluster {id: $b})
    CREATE (m)-[:BELONGS_TO {merged:true}]->(b)
    DELETE r
    DELETE a
    """
    result = execute_cypher(cypher, {"a": cluster_a, "b": cluster_b})
    log_action("cypher", "merge_clusters", f"Merged {cluster_a} into {cluster_b}")
    return result["status"] == "success"

def _now() -> str:
    return datetime.utcnow().isoformat()


### FILE: core\actuators\device.py

# core/actuators/device.py — Environmental Actuation System
import requests
import json
from datetime import datetime
from core.logging_engine import log_action

# --- Known Devices ---
REGISTERED_DEVICES = {
    "eco_lamp": {
        "type": "light",
        "protocol": "MQTT",
        "address": "192.168.1.20"
    },
    "climate_modulator": {
        "type": "env",
        "protocol": "HTTP",
        "endpoint": "http://localhost:8000/trigger/state"
    }
}

SIGNAL_TYPES = ["pulse", "state_change", "broadcast"]
ENV_PRESETS = ["focus", "alert", "reflect", "offgrid"]

# --- Core Functions ---
def send_signal(device_id: str, signal_type: str, payload: dict) -> bool:
    """Send a structured signal to a known IoT or software-integrated device."""
    device = REGISTERED_DEVICES.get(device_id)
    if not device:
        log_action("device", "error", f"Unknown device: {device_id}")
        return False

    try:
        if device["protocol"] == "HTTP":
            response = requests.post(device["endpoint"], json={
                "type": signal_type,
                "payload": payload,
                "timestamp": datetime.utcnow().isoformat()
            })
            success = response.status_code in [200, 202]
        else:
            # Future: MQTT, WebSocket, etc.
            success = False

        log_action("device", "signal", f"Sent {signal_type} to {device_id}: {payload}")
        return success
    except Exception as e:
        log_action("device", "error", f"Signal to {device_id} failed: {e}")
        return False

def trigger_environmental_state_change(state_id: str) -> bool:
    """Activate a specific environmental preset (e.g., ‘focus mode’)."""
    if state_id not in ENV_PRESETS:
        log_action("device", "invalid_state", f"Tried to trigger unknown state: {state_id}")
        return False

    for dev_id in REGISTERED_DEVICES:
        if REGISTERED_DEVICES[dev_id]["type"] == "env":
            send_signal(dev_id, "state_change", {"state": state_id})
    
    log_action("device", "trigger_env", f"Set environment to: {state_id}")
    return True

def sync_with_sensor_feed(sensor_type: str, source_url: str) -> dict:
    """Retrieve external sensory data and attach to internal context."""
    try:
        response = requests.get(source_url)
        data = response.json()
        log_action("device", "sensor_sync", f"{sensor_type} data received: {data}")
        return data
    except Exception as e:
        log_action("device", "sensor_error", f"Failed to fetch sensor ({sensor_type}): {e}")
        return {"error": str(e)}


### FILE: core\actuators\email.py

# core/actuators/email.py — System Email Notifier
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import os
from core.logging_engine import log_action

# --- Lazy Config Loader ---
def get_email_config():
    """
    Load SMTP and email credentials at send time.
    Returns a dict with all config values, raising if missing.
    """
    config = {
        "EMAIL_SENDER": os.getenv("EMAIL_SENDER"),
        "SMTP_SERVER": os.getenv("SMTP_SERVER"),
        "SMTP_PORT": int(os.getenv("SMTP_PORT", 587)),
        "SMTP_USER": os.getenv("SMTP_USER"),
        "SMTP_PASS": os.getenv("SMTP_PASS"),
    }
    for k, v in config.items():
        if v in [None, ""]:
            raise RuntimeError(f"Missing email config: {k}")
    return config

# --- Core Functions ---
def send_email(to_address: str, subject: str, body: str) -> bool:
    """Send a plaintext or HTML email to a recipient. Loads SMTP config at call."""
    try:
        cfg = get_email_config()
        msg = MIMEMultipart("alternative")
        msg["Subject"] = subject
        msg["From"] = cfg["EMAIL_SENDER"]
        msg["To"] = to_address

        part = MIMEText(body, "html" if "<" in body else "plain")
        msg.attach(part)

        with smtplib.SMTP(cfg["SMTP_SERVER"], cfg["SMTP_PORT"]) as server:
            server.starttls()
            server.login(cfg["SMTP_USER"], cfg["SMTP_PASS"])
            server.sendmail(cfg["EMAIL_SENDER"], to_address, msg.as_string())

        log_action("email", "send", f"Email sent to {to_address}: {subject}")
        return True
    except Exception as e:
        log_action("email", "error", f"Failed to send to {to_address}: {e}")
        return False

def notify_admin(event_type: str, payload: dict) -> bool:
    """Email alert to admins about high-impact events (e.g., contradiction detected)."""
    subject = f"[SoulOS Alert] {event_type.upper()}"
    body = f"<h3>{event_type}</h3><pre>{payload}</pre>"
    admin_email = os.getenv("ADMIN_EMAIL")
    if not admin_email:
        log_action("email", "error", "ADMIN_EMAIL not set in environment")
        return False
    return send_email(admin_email, subject, body)

def send_weekly_digest(user_id: str) -> bool:
    """Compile and send a user-specific digest of recent soul activity (timeline, dreams, shifts)."""
    from core.timeline_engine import get_timeline_entries
    entries = get_timeline_entries(limit=5)

    timeline_html = "".join(
        f"<p><strong>{e['t']['timestamp']}</strong>: {e['t']['summary']}</p>"
        for e in entries
    )
    body = f"<h2>SoulOS Weekly Digest</h2>{timeline_html}"
    return send_email(user_id, "Your Weekly SoulOS Digest", body)


### FILE: core\actuators\gsheet.py

# core/actuators/gsheet.py — Google Sheets Integration
from google.oauth2 import service_account
from googleapiclient.discovery import build
from core.logging_engine import log_action
from datetime import datetime
import os

# --- Config ---
SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
GOOGLE_CREDENTIALS_FILE = "secrets/gsheet_creds.json"

sheet_fields = ["timestamp", "type", "summary", "significance", "linked_nodes"]

# --- Service Auth ---
def _get_service():
    creds = service_account.Credentials.from_service_account_file(
        GOOGLE_CREDENTIALS_FILE, scopes=SCOPES
    )
    return build('sheets', 'v4', credentials=creds)

# --- Core Functions ---
def append_to_sheet(sheet_id: str, range_: str, values: list[list[str]]) -> bool:
    """Add a row of values to a specified Google Sheet range."""
    try:
        service = _get_service()
        body = {"values": values}
        service.spreadsheets().values().append(
            spreadsheetId=sheet_id,
            range=range_,
            valueInputOption="RAW",
            body=body
        ).execute()

        log_action("gsheet", "append", f"{len(values)} row(s) → {sheet_id}")
        return True
    except Exception as e:
        log_action("gsheet", "error", f"Failed to append: {e}")
        return False

def sync_timeline_to_sheet(sheet_id: str) -> bool:
    """Export recent timeline events to a readable sheet format."""
    from core.timeline_engine import get_timeline_entries
    entries = get_timeline_entries(limit=10)

    rows = []
    for e in entries:
        t = e["t"]
        row = [
            t.get("timestamp", ""),
            "timeline",
            t.get("summary", "")[:200],
            str(t.get("significance", "")),
            ", ".join(t.get("linked_events", []))
        ]
        rows.append(row)

    return append_to_sheet(sheet_id, "Sheet1!A1", rows)

def log_value_shift_to_sheet(sheet_id: str, value_data: dict) -> bool:
    """Push value realignments to a human-readable sheet for tracking."""
    row = [
        datetime.utcnow().isoformat(),
        "value_shift",
        value_data.get("description", ""),
        str(value_data.get("risk_score", "")),
        ", ".join(value_data.get("source_nodes", []))
    ]
    return append_to_sheet(sheet_id, "Sheet1!A1", [row])


### FILE: core\actuators\wordpress.py

# core/actuators/wordpress.py — Public Web Publishing Interface
import requests
import os
import base64

from core.timeline_engine import get_timeline_entries
from core.logging_engine import log_action

# --- Config ---
WORDPRESS_API = os.getenv("WORDPRESS_API_URL")  # e.g., https://site.com/wp-json/wp/v2
WP_USER = os.getenv("WORDPRESS_USERNAME")
WP_PASS = os.getenv("WORDPRESS_PASSWORD")

auth_headers = {
    "Authorization": "Basic " + base64.b64encode(f"{WP_USER}:{WP_PASS}".encode()).decode(),
    "Content-Type": "application/json"
}

# --- Core Functions ---
def publish_post(title: str, content: str, tags: list[str] = None) -> dict:
    """Create and publish a new WordPress post via REST API."""
    try:
        data = {
            "title": title,
            "content": content,
            "status": "publish",
            "tags": tags or []
        }
        response = requests.post(
            f"{WORDPRESS_API}/posts",
            headers=auth_headers,
            json=data
        )
        result = response.json()
        log_action("wordpress", "publish", f"Posted: {title}")
        return result
    except Exception as e:
        log_action("wordpress", "error", f"Failed to post: {e}")
        return {"error": str(e)}

def update_post(post_id: int, content: str) -> bool:
    """Edit an existing post (for corrections or timeline additions)."""
    try:
        data = {"content": content}
        response = requests.post(
            f"{WORDPRESS_API}/posts/{post_id}",
            headers=auth_headers,
            json=data
        )
        log_action("wordpress", "update", f"Updated post {post_id}")
        return response.status_code == 200
    except Exception as e:
        log_action("wordpress", "error", f"Failed to update: {e}")
        return False

def sync_timeline_to_wordpress(limit: int = 3) -> bool:
    """Push recent timeline events as blog entries automatically."""
    entries = get_timeline_entries(limit=limit)
    for entry in entries:
        data = entry["t"]
        title = f"Timeline: {data.get('timestamp', '')}"
        content = f"<p>{data.get('summary', '')}</p><p><em>{data.get('rationale', '')}</em></p>"
        publish_post(title, content)
    return True


### FILE: core\actuators\__init__.py



### FILE: models\claude.py

# models/claude.py
import os
import anthropic

class ClaudeWrapper:
    def __init__(self, model="claude-3-opus-20240229", api_key=None):
        self.model = model
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        self.client = anthropic.Anthropic(api_key=self.api_key)

    def __call__(self, prompt, temperature=0.7, max_tokens=1024, system_prompt=None):
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        response = self.client.messages.create(
            model=self.model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        return response.content[0].text.strip()


### FILE: models\gemini.py

# models/gemini.py
import os
import google.generativeai as genai

class GeminiWrapper:
    def __init__(self, model="gemini-pro", api_key=None):
        self.model = model
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
        genai.configure(api_key=self.api_key)

    def __call__(self, prompt, temperature=0.7, max_tokens=1024, system_prompt=None):
        model = genai.GenerativeModel(self.model)
        full_prompt = prompt
        if system_prompt:
            full_prompt = system_prompt + "\n" + prompt
        response = model.generate_content(full_prompt)
        return response.text.strip() if hasattr(response, "text") else str(response)


### FILE: models\gpt.py

# models/gpt.py
import openai
import os

class GPTWrapper:
    def __init__(self, model="gpt-4", api_key=None):
        self.model = model
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        openai.api_key = self.api_key

    def __call__(self, prompt, temperature=0.7, max_tokens=512, system_prompt=None):
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return response.choices[0].message.content.strip()


### FILE: routes\agents.py

# routes/agents.py — Admin Agent Management API
from flask import Blueprint, request, jsonify
from core.agent_manager import get_agent_roster
from core.auth import verify_token, is_admin
from core.logging_engine import log_action

agents_bp = Blueprint('agents', __name__)

@agents_bp.route('/agents', methods=['GET'])
def get_all_agents():
    """Return metadata for all registered agents (admin only)."""
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    user = verify_token(token)
    if 'error' in user or not is_admin(user):
        return jsonify({"error": "Forbidden"}), 403

    agents = get_agent_roster()
    log_action("routes/agents", "list", f"Returned {len(agents)} agents")
    return jsonify({"agents": agents})

@agents_bp.route('/agents/<agent_id>/logs', methods=['GET'])
def get_agent_logs(agent_id):
    """Return action logs for a specific agent."""
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    user = verify_token(token)
    if 'error' in user or not is_admin(user):
        return jsonify({"error": "Forbidden"}), 403

    from core.logging_engine import get_recent_logs
    logs = get_recent_logs(limit=100)
    agent_logs = [log for log in logs if log.get("source") == agent_id]
    log_action("routes/agents", "logs", f"Returned logs for {agent_id}")
    return jsonify({"logs": agent_logs})

@agents_bp.route('/agents/<agent_id>/retire', methods=['POST'])
def retire_agent(agent_id):
    """Retire an agent and remove from active roster (admin only)."""
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    user = verify_token(token)
    if 'error' in user or not is_admin(user):
        return jsonify({"error": "Forbidden"}), 403

    # In this prototype, we'll mark agent status inactive
    from core.agent_manager import AGENT_REGISTRY
    if agent_id in AGENT_REGISTRY:
        AGENT_REGISTRY[agent_id]["status"] = "retired"
        log_action("routes/agents", "retire", f"Agent {agent_id} retired by admin")
        return jsonify({"status": "success", "message": f"Agent {agent_id} retired"})
    else:
        return jsonify({"error": "Agent not found"}), 404


### FILE: routes\auth.py

# routes/auth.py — Authentication API
from flask import Blueprint, request, jsonify
from core.auth import authenticate_user, verify_token
from core.logging_engine import log_action

auth_bp = Blueprint('auth', __name__)

@auth_bp.route('/login', methods=['POST'])
def login():
    """Authenticate user and return JWT token."""
    data = request.get_json()
    username = data.get('username')
    password = data.get('password')

    if not username or not password:
        return jsonify({"error": "Missing credentials"}), 400

    token = authenticate_user(username, password)
    if not token:
        return jsonify({"error": "Invalid username or password"}), 401

    log_action("routes/auth", "login", f"User {username} logged in")
    return jsonify({"token": token})

@auth_bp.route('/verify', methods=['GET'])
def verify():
    """Verify provided JWT token."""
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    if not token:
        return jsonify({"error": "Missing token"}), 400

    decoded = verify_token(token)
    if 'error' in decoded:
        return jsonify({"error": decoded['error']}), 401

    log_action("routes/auth", "verify", f"Token verified for user {decoded.get('username')}")
    return jsonify({"user": decoded})


### FILE: routes\chat.py

# routes/chat.py — Real-time Chat API (Event Normalization, No SocketIO Emit in REST)
from flask import Blueprint, request, jsonify
from core.memory_engine import store_event
from core.agent_manager import assign_task
from core.logging_engine import log_action
from core.auth import verify_token
# Do NOT import flask_socketio.emit in this REST route file

chat_bp = Blueprint('chat', __name__)

@chat_bp.route('/chat', methods=['POST'])
def chat_with_soul():
    """
    Accept a user message, create event, route to LLM, return response.
    For REST: do NOT emit via socket here! Just return JSON.
    """
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    user = verify_token(token)
    if 'error' in user:
        return jsonify({"error": "Unauthorized"}), 401

    data = request.get_json()
    user_message = data.get("message")
    if not user_message:
        return jsonify({"error": "No message provided"}), 400

    # Store user message as event (returns normalized dict)
    event = store_event(user_message, agent_origin=user.get("username"))
    if not event or not event.get("id"):
        return jsonify({"error": "Failed to store event"}), 500

    # Assign task to an agent (e.g., gpt_writer)
    response = assign_task("gpt_writer", user_message, context={})
    log_action("routes/chat", "message", f"User {user.get('username')} sent message")

    # DO NOT emit SocketIO here. REST returns only.
    return jsonify({
        "event": event,
        "response": response.get("response", "")
    })

@chat_bp.route('/chat/history', methods=['GET'])
def get_chat_history():
    """
    Return chronological list of recent chat messages/events, normalized.
    """
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    user = verify_token(token)
    if 'error' in user:
        return jsonify({"error": "Unauthorized"}), 401

    from core.graph_io import run_read_query
    results = run_read_query("MATCH (e:Event) RETURN e ORDER BY e.timestamp DESC LIMIT 50")
    # Normalize event dict output
    messages = [
        {
            "text": r["e"].get("raw_text", ""),
            "timestamp": r["e"].get("timestamp", ""),
            "agent_origin": r["e"].get("agent_origin", None),
            "event_id": r["e"].get("id", None),
            "status": r["e"].get("status", None)
        }
        for r in results if "e" in r
    ]
    log_action("routes/chat", "history", f"Returned chat history to {user.get('username')}")

    return jsonify({"history": messages})

# Note:
# - If you want live chat feedback, add SocketIO event handlers in a *socket_handlers.py* module,
#   import/attach them to your app in app.py. This REST route should not emit.


### FILE: routes\dreams.py

# routes/dreams.py — Dreamscape API
from flask import Blueprint, request, jsonify
from core.graph_io import run_read_query
from core.auth import verify_token
from core.logging_engine import log_action

dreams_bp = Blueprint('dreams', __name__)

@dreams_bp.route('/dreams', methods=['GET'])
def get_all_dreams():
    """Return recent or significant dream nodes."""
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    user = verify_token(token)
    if 'error' in user:
        return jsonify({"error": "Unauthorized"}), 401

    limit = int(request.args.get("limit", 20))
    query = """
    MATCH (d:Dream)
    RETURN d
    ORDER BY d.timestamp DESC
    LIMIT $limit
    """
    dreams = run_read_query(query, {"limit": limit})
    log_action("routes/dreams", "list", f"Returned {len(dreams)} dreams")
    return jsonify({"dreams": dreams})

@dreams_bp.route('/dreams/<dream_id>', methods=['GET'])
def get_dream_by_id(dream_id):
    """Return a full view of a specific dream node."""
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    user = verify_token(token)
    if 'error' in user:
        return jsonify({"error": "Unauthorized"}), 401

    query = "MATCH (d:Dream {id: $id}) RETURN d LIMIT 1"
    result = run_read_query(query, {"id": dream_id})
    if not result:
        return jsonify({"error": "Not found"}), 404
    log_action("routes/dreams", "get", f"Returned dream {dream_id}")
    return jsonify({"dream": result[0]["d"]})


### FILE: routes\events.py

# routes/events.py — Event Ingestion API (Event Return Normalization)
from flask import Blueprint, request, jsonify
from core.memory_engine import store_event
from core.agent_manager import assign_task
from core.auth import verify_token, is_admin
from core.logging_engine import log_action
from core.graph_io import run_read_query

events_bp = Blueprint('events', __name__)

@events_bp.route('/event', methods=['POST'])
def post_event():
    """Receive raw input, embed, store as event, trigger agent response."""
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    user = verify_token(token)
    if 'error' in user:
        return jsonify({"error": "Unauthorized"}), 401

    data = request.get_json()
    raw_text = data.get("text")
    agent_origin = user.get("username", "unknown")

    event = store_event(raw_text, agent_origin=agent_origin)
    if not event or not isinstance(event, dict) or not event.get("id"):
        return jsonify({"error": "Failed to store event"}), 500

    response = assign_task("gpt_writer", raw_text, context={})
    log_action("routes/events", "post_event", f"Event stored and task assigned for user {agent_origin}")

    # Always return clean event dict, not Neo4j wrapper
    return jsonify({
        "event": event,
        "response": response
    })

@events_bp.route('/events', methods=['GET'])
def get_all_events():
    """Return all stored events (admin only), normalized."""
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    user = verify_token(token)
    if 'error' in user or not is_admin(user):
        return jsonify({"error": "Forbidden"}), 403

    # Always unpack Neo4j objects, only return the event dicts
    results = run_read_query("MATCH (e:Event) RETURN e ORDER BY e.timestamp DESC LIMIT 100")
    events = [r["e"] for r in results if "e" in r and isinstance(r["e"], dict)]
    return jsonify({"events": events})

# If you ever add endpoints to fetch a single event by ID,
# always unpack as above:
# result = run_read_query("MATCH (e:Event {id: $id}) RETURN e LIMIT 1", {"id": event_id})
# event = result[0]["e"] if result else None
# return jsonify({"event": event}) if event else ({"error": "Not found"}, 404)


### FILE: routes\timeline.py

# routes/timeline.py — Timeline Narrative API
from flask import Blueprint, request, jsonify
from core.timeline_engine import get_timeline_entries
from core.auth import verify_token
from core.logging_engine import log_action

timeline_bp = Blueprint('timeline', __name__)

@timeline_bp.route('/timeline', methods=['GET'])
def get_timeline():
    """Return recent timeline entries for UI rendering."""
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    user = verify_token(token)
    if 'error' in user:
        return jsonify({"error": "Unauthorized"}), 401

    limit = int(request.args.get("limit", 50))
    entries = get_timeline_entries(limit=limit)
    log_action("routes/timeline", "get_timeline", f"Returned {len(entries)} entries")
    return jsonify({"timeline": entries})

@timeline_bp.route('/timeline/<entry_id>', methods=['GET'])
def get_timeline_entry(entry_id):
    """Return a single timeline entry by ID (if needed for deep display)."""
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    user = verify_token(token)
    if 'error' in user:
        return jsonify({"error": "Unauthorized"}), 401

    from core.graph_io import run_read_query
    result = run_read_query("MATCH (t:TimelineEntry {id: $id}) RETURN t LIMIT 1", {"id": entry_id})
    if not result:
        return jsonify({"error": "Not found"}), 404
    log_action("routes/timeline", "get_entry", f"Returned timeline entry {entry_id}")
    return jsonify({"entry": result[0]["t"]})


### FILE: routes\__init__.py

# routes/__init__.py
from .agents import agents_bp
from .auth import auth_bp
from .chat import chat_bp
from .dreams import dreams_bp
from .events import events_bp
from .timeline import timeline_bp

def register_blueprints(app):
    """
    Register all API blueprints to the Flask app.
    Extend this as you add more route modules!
    """
    app.register_blueprint(agents_bp, url_prefix='/api')
    app.register_blueprint(auth_bp, url_prefix='/api')
    app.register_blueprint(chat_bp, url_prefix='/api')
    app.register_blueprint(dreams_bp, url_prefix='/api')
    app.register_blueprint(events_bp, url_prefix='/api')
    app.register_blueprint(timeline_bp, url_prefix='/api')


### FILE: utils\profiling.py

# utils/profiling.py — System Performance & Behavior Tracker
import time
import psutil
import os
from functools import wraps
from core.graph_io import run_read_query
from datetime import datetime
from core.logging_engine import log_action

PERF_LOG_DIR = "logs"
PERF_LOG_FILE = os.path.join(PERF_LOG_DIR, "performance.log")
tracked_metrics = ["dreams_per_day", "contradiction_ratio", "epiphanies_per_week"]

def ensure_perf_log_dir():
    """Ensure the logs directory exists before any writes."""
    try:
        os.makedirs(PERF_LOG_DIR, exist_ok=True)
    except Exception as e:
        print(f"[PERF LOG DIR ERROR] Could not create logs directory: {e}")

# Ensure log dir at module import (covers first write, new installs, and cold starts)
ensure_perf_log_dir()

# --- Decorator to measure function latency ---
def track_function_latency(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        end = time.perf_counter()
        latency_ms = (end - start) * 1000
        log_action("profiling", "latency", f"{func.__name__} took {latency_ms:.2f} ms")
        try:
            ensure_perf_log_dir()
            with open(PERF_LOG_FILE, "a") as f:
                f.write(f"{datetime.utcnow().isoformat()} {func.__name__} took {latency_ms:.2f} ms\n")
        except Exception as e:
            print(f"[PERF LOG ERROR] Could not write latency: {e}")
        return result
    return wrapper

# --- System resource load ---
def get_system_load() -> dict:
    """Return current CPU, RAM, disk stats."""
    cpu = psutil.cpu_percent(interval=1)
    ram = psutil.virtual_memory().percent
    disk = psutil.disk_usage('/').percent
    log_action("profiling", "system_load", f"CPU: {cpu}%, RAM: {ram}%, Disk: {disk}%")
    try:
        ensure_perf_log_dir()
        with open(PERF_LOG_FILE, "a") as f:
            f.write(f"{datetime.utcnow().isoformat()} CPU: {cpu}% RAM: {ram}% Disk: {disk}%\n")
    except Exception as e:
        print(f"[PERF LOG ERROR] Could not write system load: {e}")
    return {"cpu_percent": cpu, "ram_percent": ram, "disk_percent": disk}

# --- Graph Metrics ---
def get_graph_metrics() -> dict:
    """Return number of events, dreams, agents, and timeline entries."""
    counts = {}
    for label in ["Event", "Dream", "TimelineEntry", "Agent"]:
        query = f"MATCH (n:{label}) RETURN count(n) AS count"
        result = run_read_query(query)
        counts[label] = result[0]["count"] if result else 0
    log_action("profiling", "graph_metrics", f"Graph counts: {counts}")
    try:
        ensure_perf_log_dir()
        with open(PERF_LOG_FILE, "a") as f:
            f.write(f"{datetime.utcnow().isoformat()} GRAPH_COUNTS: {counts}\n")
    except Exception as e:
        print(f"[PERF LOG ERROR] Could not write graph metrics: {e}")
    return counts

# --- Behavioral Summary ---
def get_behavioral_summary() -> dict:
    """Return insights like dream frequency, epiphany rate, contradiction density."""
    # Placeholder: Needs historical event data or advanced analytics
    summary = {
        "dreams_per_day": 5,
        "epiphanies_per_week": 2,
        "contradiction_ratio": 0.1
    }
    log_action("profiling", "behavioral_summary", f"Behavioral summary: {summary}")
    try:
        ensure_perf_log_dir()
        with open(PERF_LOG_FILE, "a") as f:
            f.write(f"{datetime.utcnow().isoformat()} BEHAVIORAL_SUMMARY: {summary}\n")
    except Exception as e:
        print(f"[PERF LOG ERROR] Could not write behavioral summary: {e}")
    return summary


### FILE: utils\schema_tools.py

# utils/schema_tools.py — Graph Schema Inspection & Migration
from core.graph_io import run_read_query, run_write_query
from core.logging_engine import log_action

LABEL_META_NODE = "SchemaMeta"

# --- Core Functions ---
def list_node_labels() -> list[str]:
    """Return all node labels currently in use."""
    query = "CALL db.labels() YIELD label RETURN label"
    results = run_read_query(query)
    labels = [r["label"] for r in results]
    log_action("schema_tools", "list_labels", f"Found labels: {labels}")
    return labels

def list_relationship_types() -> list[str]:
    """Return all active relationship types in the graph."""
    query = "CALL db.relationshipTypes() YIELD relationshipType RETURN relationshipType"
    results = run_read_query(query)
    rels = [r["relationshipType"] for r in results]
    log_action("schema_tools", "list_rels", f"Found relationship types: {rels}")
    return rels

def find_orphan_nodes(label: str) -> list[dict]:
    """Find nodes of a given type that have no relationships."""
    query = f"""
    MATCH (n:{label})
    WHERE NOT (n)--()
    RETURN n.id AS id, n
    """
    results = run_read_query(query)
    log_action("schema_tools", "find_orphans", f"Found {len(results)} orphan nodes of label {label}")
    return results

def migrate_node_label(old_label: str, new_label: str) -> bool:
    """Change all nodes of one type to another label (e.g. 'Belief' → 'Value')."""
    query = f"""
    MATCH (n:{old_label})
    REMOVE n:{old_label}
    SET n:{new_label}
    RETURN count(n) AS migrated_count
    """
    result = run_write_query(query)
    count = result["result"][0]["migrated_count"] if result["status"] == "success" else 0
    log_action("schema_tools", "migrate_label", f"Migrated {count} nodes from {old_label} to {new_label}")
    return result["status"] == "success"


### FILE: utils\snapshot.py

# utils/snapshot.py — System State Snapshot Utility
import os
import json
from datetime import datetime
from core.self_concept import get_current_self_concept
from core.value_vector import initialize_value_vector
from core.agent_manager import get_agent_roster
from core.graph_io import run_read_query
from core.logging_engine import log_action

SNAPSHOT_DIR = "snapshots/"
default_snapshot_fields = ["self_concept", "value_vector", "active_agents", "cluster_summaries"]

def ensure_snapshot_dir():
    """Ensure the snapshots directory exists, create if missing."""
    try:
        os.makedirs(SNAPSHOT_DIR, exist_ok=True)
    except Exception as e:
        log_action("snapshot", "dir_error", f"Failed to create snapshot dir: {e}")
        raise

def take_snapshot(label: str = None) -> dict:
    """Capture current state of system (graph stats, identity, values, etc.)."""
    snapshot = {}
    snapshot["timestamp"] = datetime.utcnow().isoformat()
    snapshot["label"] = label or f"snapshot_{snapshot['timestamp']}"

    snapshot["self_concept"] = get_current_self_concept()
    # For demonstration, use default values; replace with actual vector retrieval
    snapshot["value_vector"] = initialize_value_vector({})
    snapshot["active_agents"] = get_agent_roster()

    # Example cluster summary - adapt as needed
    cluster_summary_query = "MATCH (c:SelfCluster) RETURN c.id AS id, c.label AS label"
    snapshot["cluster_summaries"] = run_read_query(cluster_summary_query)

    log_action("snapshot", "take", f"Snapshot taken: {snapshot['label']}")
    return snapshot

def save_snapshot_to_file(snapshot_data: dict, filename: str) -> bool:
    """Export snapshot to disk (JSON). Ensures dir exists before writing."""
    try:
        ensure_snapshot_dir()
        filepath = os.path.join(SNAPSHOT_DIR, filename)
        with open(filepath, "w") as f:
            json.dump(snapshot_data, f, indent=2)
        log_action("snapshot", "save", f"Snapshot saved: {filename}")
        return True
    except Exception as e:
        log_action("snapshot", "error", f"Failed to save snapshot: {e}")
        return False

def load_snapshot(filename: str) -> dict:
    """Reload a previously saved system snapshot."""
    try:
        ensure_snapshot_dir()
        filepath = os.path.join(SNAPSHOT_DIR, filename)
        with open(filepath, "r") as f:
            data = json.load(f)
        log_action("snapshot", "load", f"Snapshot loaded: {filename}")
        return data
    except Exception as e:
        log_action("snapshot", "error", f"Failed to load snapshot: {e}")
        return {}
